Bitcoin is broken. And not just superficially so, but fundamentally, at the core protocol level. We're not talking about a simple buffer overflow here, or even a badly designed API that can be easily patched; instead, the problem is intrinsic to the entire way Bitcoin works. All other cryptocurrencies and schemes based on the same Bitcoin idea, including Litecoin, Namecoin, and any of the other few dozen Bitcoin-inspired currencies, are broken as well.

Specifically, in a paper we placed on arXiv, Ittay Eyal and I outline an attack by which a minority group of miners can obtain revenues in excess of their fair share, and grow in number until they reach a majority. When this point is reached, the Bitcoin value-proposition collapses: the currency comes under the control of a single entity; it is no longer decentralized; the controlling entity can determine who participates in mining and which transactions are committed, and can even roll back transactions at will. This snowball scenario does not require an ill-intentioned Bond-style villain to launch; it can take place as the collaborative result of people trying to earn a bit more money for their mining efforts.

Conventional wisdom has long asserted that Bitcoin is secure against groups of colluding miners as long as the majority of the miners are honest (by honest, we mean that they dutifully obey the protocol as proscribed by pseudonymous Nakamoto). Our work shows that this assertion is wrong. We show that, at the moment, any group of nodes employing our attack will succeed in earning an income above their fair share. We also show a new bound that invalidates the honest majority claim: under the best of circumstances, at least 2/3rds of the participating nodes have to be honest to protect against our attack. But achieving this 2/3 bound is going to be difficult in practice. We outline a practical fix to the protocol that is easy to deploy and will guard against the attack as long as 3/4ths of the miners are honest.

We need the Bitcoin community's help in deploying this fix so that the Bitcoin ecosystem can be made more robust, at least against attackers whose mining power is below the 25% threshold. Even with our fix deployed, however, there is a problem: there are mining pools at the moment that command more than 25% of the mining power, and, in the past, there have been mining pools that commanded more than 33% of the mining power. We need the Bitcoin community's awareness and concerted effort to ensure that no mining pool reaches these thresholds. The mere possibility that the system can get into a vulnerable state will be an impediment to greater adoption of Bitcoin.

Those of you who want a precise and full explanation of the attack can cut straight to the research paper, though it may be a bit terse and dry. In the rest of this blog entry, we will outline the attack for the non-hard-core practitioner, such that by the end of the blog entry, anyone should understand the intuition behind our attack, be equipped to earn higher revenues through mining, and possess the tools required to usurp the currency. To get to this point, we need a little bit of background on how Bitcoin works. If you're familiar with Bitcoin mining, you can skip to the next section that describes how the attack works. If you are a non-techie Bitcoin user, you can skip straight to the Implications section.

The Blockchain
Cash for gold.
The key idea behind Bitcoin's success is a decentralized protocol for maintaining a global ledger, called a blockchain. The blockchain records transactions between Bitcoin addresses, tracking the movement of every Bitcoin as it changes hands. This tracking ensures that no one can double-spend a coin, as the ledger makes it all too apparent whether a user sent out more Bitcoins from his account than he earned. The particular way in which Bitcoin tracking is performed makes sure that the record is also immutable; once a Bitcoin transaction is committed and buried in the blockchain, it is difficult for an attacker to reverse the transaction, so that a merchant can ship goods in good conscience, assured that the transaction will later not be reversed.

This protocol works through a process called mining. In essence, the ledger is organized into a single, ordered sequence of blocks, each of which records a set of transactions. Each block contains a crypto-puzzle, a computationally difficult challenge akin to a CAPTCHA. Miners organize themselves into a loosely-organized, distributed network, and they all concurrently try to add a new block to the ledger. To do this, they need to discover the solution to a crypto-puzzle, formed by the contents of the ledger until the point where the new block is being added. Solving a crypto-puzzle is hard work; a computer has to plug in many different values and see if they solve the crypto-puzzle posed by the new block. The puzzles are such that a home computer working alone will take many years to solve a crypto-puzzle. Some people use GPUs to speed up this process, while others have invested in custom ASICs designed to solve Bitcoin crypto-puzzles.

Of course, this process is not free, as the process of solving these crypto-puzzles consumes power and requires cooling. For the currency to be viable, the miners need to be compensated for their efforts. Bitcoin miners are compensated through two mechanisms: they collect the transaction fees from the transactions recorded in the new block they contributed to the block chain, and they also collect a lump sum fee. This lump sum fee creates new Bitcoins, according to a time-varying formula. Hence, "mining" is similar to digging for gold -- every now and then, a miner is rewarded with a nugget. The difficulty of crypto-puzzles are automatically adjusted such that a new block is added to the ledger approximately every 10 minutes, which ensures a predictable coin generation rate for the system, which stems inflation and makes the currency supply more predictable than it would be otherwise.

The nice thing about having crypto-puzzles that are so difficult is that it is not practical for an attacker to modify the ledger. Someone who wants to, say, buy something from a Bitcoin merchant, get the goods shipped, and then later change that block to erase the transfer of money to the merchant, faces a very difficult task: they need to find alternative solutions to cryptopuzzles for that block and every subsequent block. What makes this difficult is that the main bulk of the miners will be working hard on adding new blocks at the tail end of the ledger, so an attacker, with limited resources, cannot hope to find alternative solutions for all the past blocks and catch up to the rest of the miners.

Miners today organize themselves into groups known as pools. A pool will typically consist of a set of cooperating nodes that share their revenues whenever they find blocks. Mining pools are kind of like the shared tip jar at a restaurant: on occasion, a miner will hit the potluck, discover a good solution to a cryptopuzzle, and rake in some revenues, kind of like a waiter who lands a big table that runs a large tab. Since this occurs relatively infrequently from the point of view of any given miner, sharing the proceeds enables the miners to have more predictability in their lives.

The Attack
Cash for gold.
The honest Bitcoin protocol assumes that all miners engage in a benign strategy where they quickly and truthfully share every block they have discovered. Until now, everyone assumed that this was the dominant strategy; no other strategy was known that could result in higher revenues for miners.

Our work shows that there is an alternative strategy, called Selfish-Mine, that enables a mining pool to make additional money at the risk of hurting the system. In Selfish-Mining, miners keep their block discoveries private to their own pool, and judiciously reveal them to the rest of the honest miners so as to force the honest miners to waste their resources on blocks that are ultimately not part of the blockchain.

Here's how this works in practice. Selfish miners start out just like regular miners, working on finding a new block that goes at the end of the blockchain. On occasion, like every other miner, they will discover a block and get ahead of the rest of the honest miners. Whereas an honest miner would immediately publicize this new block and cause the rest of the honest miners to shift their effort to the newly established end of the chain, a selfish miner keeps this block private.

From here, two things can happen. The selfish miners may get lucky again, and increase their lead by finding another block. They will now be ahead of the honest crowd by two blocks. They keep their new discovery secret as well, and work on extending their lead. Eventually, the honest miners close the gap. Just before the gap is closed, the selfish pool publishes its longer chain. The result is that all the honest miners' work is discarded, and the selfish miners enjoy the revenue from their previously secret chain.

The analysis of revenues gets technical from here, and the only way to do it justice is to follow along the algorithm and state machine provided in our paper. But the outcome is that the selfish mining pool, on the whole, nullifies the work performed by the honest pool through their revelations.

The success of the attack, and the amount of excess revenue it yields, depends on the size of the selfish mining pool. It will not be successful if the pool is below a threshold size. But this threshold is non-existent in the current implementation -- selfish mining is immediately profitable. Our proposed fix raises the threshold to 25% if universally adopted. And, while there may be other fixes, no fix can raise it above 33%. So, at least 2/3rds of the Bitcoin miners have to be honest. All three of these findings are a far cry from the 50% previously (and falsely) believed to protect the currency.

The Implications
Cash for gold.
The selfish mining strategy has significant implications for the Bitcoin system:

The members of a selfish mining pool will earn more revenue than honest participants: This means that rational, self-interested miners, who typically invest significant amounts of money in their rigs, will want to join selfish miners instead of follow the honest strategy.

Once launched and successful, selfish mining pools will grow in size: There are no mechanisms in place to exert any kind of pressure to break up a selfish mining pool.

Selfish mining is harmful to the Bitcoin community: Selfish miners bring down revenues for everyone. The fact that a selfish mining attack can be launched, and a selfish pool can grow in size until it controls the currency, is a deterrent to people, like the Winklevii, who are drawn to the decentralized nature of Bitcoin.

This attack is practical right now with any size mining pool: Anyone can launch this attack successfully right now, and make revenues in excess of what they would otherwise make.

Under the best theoretical conditions, Bitcoin requires at least 2/3rds of the miners to be honest: It was previously believed that the Bitcoin ecosystem was safe as long as a majority were honest. Our analysis shows that this is wrong. If a selfish-mining pool were to command 1/3rd (33%) of mining power, it'll always be in a position to make excess revenues over honest miners.

We propose a practical fix that will protect against selfish mining as long as pools command below 25% of the mining power: The fix is simple to apply. It would be a good idea for the Bitcoin community to adopt it.

There are mining pools in existence that can conceivably launch successful selfish mining attacks: At the moment, any mining pool can launch a successful mining attack. With our proposed fix, only pools above 25% can launch the attack, but there exists a pool of this size right now. And there have even been pools that commanded more than 33% of the mining power in the past.

FAQ
Cash for gold.
Some frequently asked questions:

What happens when a selfish mining group is formed?
Once a group of selfish miners appear on the horizon, rational miners will preferentially join that mining group to obtain a share of their higher revenues. And their revenues will increase with increasing group size. This creates a dynamic where the attackers can quickly acquire majority mining power, at which point the decentralized nature of the Bitcoin currency collapses, as the attackers get to control all transactions.

When a single pool controls the currency, does the value of a Bitcoin go to $0?
No. It all depends on how the controlling group runs the currency. But the decentralization, which in our view is so critical to Bitcoin's adoption, is lost. It would not be at all healthy for the Bitcoin ecosystem.

Does this affect X, where X is another cryptocurrency?
Probably. It affects every currency system that is inspired by Bitcoin's blockchain. That includes Litecoin, PPcoin, Novacoin, Namecoin, Primecoin, Terracoin, Worldcoin, and a host of other currencies that share the same global ledger concept.

What's the core discovery here?
We're the first to discover that the Bitcoin protocol is not incentive-compatible. The protocol can be gamed by people with selfish interests. And once the system veers away from the happy mode where everyone is honest, there is no force that opposes the growth of really large pools that command control of the currency.

Is selfish mining happening now?
We cannot know for sure, but we suspect not. Ours is the first work to publicly investigate an alternative mining strategy.

What's with these two separate thresholds? Do 2/3rds of the nodes have to be honest? Or 3/4ths? Why is there a gap between the two?
At the moment, the threshold is non-existent. With our proposed fix, which is practical and easy to deploy, it gets raised to 25%; i.e. 3/4ths of the network must be honest. Perhaps someone can propose a fix that raises this threshold further, but we have shown that they cannot raise it above 2/3rds.

Would we be able to tell a selfish mining pool from any other pool?
Not easily. A selfish mining pool can hide behind throwaway addresses to mask its identity. And while the timing of block revelations does look different for selfish miners, it's difficult to tell who was genuinely first, as near-concurrent revelations will arrive in different orders at hosts.

Is there a danger associated with making this attack public?
The only way to protect the system against selfish mining attacks is to get everyone to change their implementations. So the only way we can protect the system is by publicizing the potential attack. We have chosen not to launch the attack ourselves, because we care about the long-term viability of the currency.

Can Bitcoin remain a viable currency?
Probably. We have shown that as long as selfish miners are below a certain threshold, they will not succeed. And while this threshold does not exist yet (i.e. selfish mining will immediately yield benefits for any sized pool), we have a proposed fix that raises the threshold to 25%.
Addendum, November 14, 2013
Followup post on frequently-asked questions

Followup post on the novelty of the selfish mining attack

Followup post on altruism among capitalists

Followup post on response to feedback on selfish mining

Addendum, February 28, 2014
Haldane says that there are four stages of acceptance to new ideas:

This is worthless nonsense.
This is an interesting, but perverse, point of view.
This is true, but quite unimportant.
I always said so.
The comments below, coming from "Bitcoin entrepreneurs", organized into a brigade, exhibit all four of these stages at the same time. I have preserved them as they are (except for deleting profanity-laced comments), to shame the community. In some cases, the commenters edited their own comments when proven wrong, so reading the comment chains may seem non-sensical at times as a result.

- See more at: http://hackingdistributed.com/2013/11/04/bitcoin-is-broken/#sthash.kHUWtmJG.dpuf

Ballmer is leaving Microsoft, and there are lots of developers complaining about how "stack ranking," the practice of ranking employees on a rigid curve, "killed Microsoft." Now, let's first observe that Microsoft is nowhere near dead; it has transformed itself into an enterprise company and has an immense revenue stream, though it no longer commands the market leader position in all aspects of computing. This attempt to pin the blame for Microsoft's loss of leadership on its necessarily-flawed-like-every-other employee recognition process is silly -- stack ranking is the least of Microsoft's problems.

Steve Ballmer.
Memento Mori. Remember thou art mortal.

Microsoft is not the first empire to falter. A pretty big empire before it had a "stack ranking" system that was far more unjust and far more damaging to morale. I am referring, of course, to The Roman Empire, SPQR, and their practice of punishing underperforming legions by "decimating" them. Many people have heard of the term, but few know that the word comes from the Roman practice of lining up an underperforming legion and literally killing off every tenth soldier by having his 9 other colleagues beat him to death. Emperor Ballmerius was nice enough, at least, to rank his soldiers before getting rid of the lower few percent. Imagine what a random culling does for morale. Yet few historians in their right minds would point to decimation as a factor in the fall of the Roman Empire.

For no empire falls because of any single reason. The decline of the Byzantine Empire took approximately half a standard solar millenium, the Ottoman Empire 400 standard solar years, and Microsoft's took 15 Internet years, equivalent to 150 solar years or so. A lot happens in those kinds of time frames. There are so many disparate reasons for any fall that the ones people pick out simply betray their internal anxieties and prejudices.

Solomon the Magnificent.
Solomon the Magnificent loved expensive tiaras. He also loved to stack-rank the royal court and strangle the low rankers. He expanded his empire across three continents, owned the Mediterranean, and no one complained about how he distributed year end bonuses.

Ask anyone about the Byzantines, and they'll tell you all about intrigue and internal wrangling among competing royal contenders. These guys gave us the distributed systems term "Byzantine" for failures so complex that they defy any kind of reasoning. For instance, one of the Cantacuzens wishing to ascend to the Byzantian throne sold off a critical castle in Europe, and not just to any buyer, but to the very hordes pounding at the gates, trying to establish a foothold on the European side. Yet even this kind of treason is merely a small tactical blunder in the grand scheme of things; Byzantium was doomed to fall one way or another.

Similarly, many old-school conservative male historians have claimed that the fall of the Ottoman Empire was precipitated by the "increased involvement of the harem women in imperial politics." So many, in fact, that I would have lost track of them, except I kept their names in a folder called "sexist idiots to never read again." What one picks out from a list of many dozen options simply shows what story they want to tell, what their own internal world revolves around.

In reality, all of these empires lost their dominance due to fundamental shifts: changes in trade routes, shifts in alliances that barred their access to lucrative colonies, or global shifts in people's behavior that turned once dominant powers into bit players. Empires fail to follow through these shifts because they lack leaders with vision and upper-level managers who can execute.

Let's catalog some fundamental shifts that left Microsoft behind.

Missed Opportunities
The Road Ahead
Romans are famous for the straight roads they paved, peasants be damned. The Road Ahead can be tortuous if you're on someone else's path.

Microsoft missed the Internet. When Gates published his book "The Road Ahead," the first web browser had been out for a few years and everyone could see that it was pointing to the next big thing on the road ahead. Yet the book made no mention of the Internet and paid lip service to wide-area cross-domain network connectivity; these ideas had to be retrofitted into the second edition after the fact.

Microsoft missed control of the server side. Their ecosystem was built on platform tie-down on the client-side with the help of klunky APIs. Throughout the 90's, every thinking programmer forced to develop code for Windows knew that there was something inherently wrong with passing 9 additional NULL arguments to every call, or looking things up in always out-of-date knowledgebase articles that somehow went on for pages without providing a single insight. The resulting systems were neither fast to run nor easy to manage. People do not address total cost of ownership issues with incremental changes -- they fix them by switching to a new platform wholesale. And on the server side, people switched to free and open source Unix, which provided clean APIs designed by and for discerning programmers.

Microsoft missed virtualization. Despite having purchased half a dozen virtualization startups, Microsoft was not able to establish a credible presence in this space, and allowed VMWare and others to creep in under their OS. In retrospect, this blunder was of little consequence, as few people use Windows in settings where virtualization is required, but it was a chink in the armor.

Microsoft missed ad-supported web services. Not "Web Services," a strange buzzword dressed in angle brackets and levels of indirection that I never was able to understand, but regular old services on the web. They copied the portal craze with MSN, but MSN failed like all other portals because no one wants a horoscope or news about Hollywood nobodies when they're trying to get to information. Hotmail was quickly outclassed by Gmail, which offered 10X more storage and higher limits across the board. Perhaps Microsoft lacked the cash to buy some extra hard drives, but their 10Q filings show that what they lacked was vision and the will to fight -- after all, Hotmail wasn't really integrated with Windows, and therefore an orphan. My Microsoft friends tell me that Bing is ok, except the results look terrible every time I use it by accident, and there is a real trust issue that makes discerning users not want to use it. They let their lead in messaging services get usurped by socially-connected messaging a la Twitter. Buying Skype and weakening its security properties was an awful move, one that presaged the Snowden revelations that came soon afterwards, and in any case, all of this happened when Internet telephony was causing the cost of phone calls to go to $0. They had no play in the Internet video space, unless we count Live, a service that fell far short of its name. And most importantly, Microsoft could not monetize these services the way Google could, due to its market position and existing shrink-wrapped-software-oriented business model. Perhaps ad-supported business models were not compatible with Microsoft's DNA, perhaps customers would be put off by Microsoft's history of playing hardball, but perhaps it would have been possible to spin off a unit that was independent and distinct, and not hampered by the baggage associated with the Microsoft name.

Microsoft missed the cloud. And they did this on the analytics side despite having invented Dryad/LINQ, a much more elegant model compared to that of Hadoop. Canceling Dryad/LINQ for Hadoop was a decision to trail the existing market, instead of advancing it with a technically superior system. On the infrastructure side, I don't quite see why Microsoft, or any other American company, should succeed in the long run in a game of providing a standard API with reduced personnel costs.

Microsoft missed social. And good thing, too, as I can't imagine having to maintain one more "friends list." But they could have had their own play in this space. From what I see every time I click on some LinkedIn or Foursquare email, social networks get built through very aggressive friend linking practices in the post-Orkut, post-MySpace, post-Facebook world. Microsoft had the right hooks in place to aggressively mine such connections. Back during the reign of Gatesius and before the Buzz lawsuit, they certainly would not have blinked before mining people's Outlook contacts to build their own network.

Microsoft missed mobile. There was ferocious competition within Microsoft to determine the operating system for mobile systems. The initial winner, Windows CE, was chosen because it was essentially Windows in miniature. Yet carrying the klunky Windows experience onto mobile devices was destined not to be a winning move, as evidenced by how Blackberry, then the iPhone, then Android managed to eat into this space. Surface is an excellent product, except it just doesn't have the developer ecosystem to prop it up, and power laws imply that the third player on the scene gets less than 1/8th of the action.

Give Unto Caesar
Caesar
He deserves what's his.

Let's be quite clear: Microsoft has had some amazing successes as well. Windows was a huge success story up through XP. Office is still the main market leader and still far ahead of all web-based competition. Kinect is a huge success of technical innovation. Surface is an extremely well-engineered device. .NET and C# are clear winners, showing how "embrace and extend" can be done elegantly, without having to corrupt and spoil that which was being embraced. And there is no way anyone can do justice to the amount of groundbreaking work that has come out of Microsoft Research.

It was only a decade ago that people were trying to emulate every aspect of life at Microsoft, including the dynamic between program managers, developers, and STEs/SDETs (a social arrangement that I always found unbalanced and questionable), their release cycles and nomenclature (whose implementation often leads to an early clamp down on critical fixes, and leads to software that is broken until Service Pack 2), and of course, the dreaded stack ranking. One could implement stack ranking in a way that does not distribute low scores across all groups uniformly, but instead recognizes top performing groups. Even though I never grade on a curve, I have taken countless courses that were curved, and never once felt the urge to sabotage my fellow students nor felt that the resulting environment was toxic. In fact, Microsoft prospered quite well with stack ranking firmly in place. Many other companies, including Google, also stack-rank. The Microsoft CEO could stack-rank, inverse-stack-rank, randomly-decimate, or personally tuck every developer to sleep each night muttering sweet songs about monopolies, and it wouldn't change much about the failures I listed above. There will undoubtedly be many current and and ex-Microsofties who will loudly complain about stack ranking, but pinning the entire blame on this one aspect of employee recognition just doesn't make sense. There were fundamental failures of vision and failures to execute, starting at the top and reaching down, not the other way around. There will be many new battles to come, and many new domains to fight over. Some fresh blood is desperately called for, and Ballmer's step down is just the beginning of a much needed transformation.

Related
The new Microsoft CEO will most likely get rid of stack ranking, the same way emperors initially dole out some gold to the foot soldiers to win their support. This will provide a natural experiment to see if the change makes any difference to the bottom line.

- See more at: http://hackingdistributed.com/2013/08/24/stack-ranking-did-not-kill-microsoft/#sthash.TfUQV5j5.dpuf

To understand how something will end, we have to understand how it began.

Around 2003, someone very very high up in the intelligence community visited Cornell. You might think you know people high up in the intelligence community; this person was a click higher. Naturally, I signed up to chat with him. I remember that he walked into my office with the aura of Brigadier Gen. Jack D. Ripper having sent the entire wing in on Wing Attack Plan R. Underneath the bravado though, there was a strong dash of Gen. Buck Turgidson, and in any case, he had no military background and would have been too plump to make PFC if he enrolled. The protruding belly betrayed years spent managing physicists, whose research seems to run on beer as much as it does on taxpayer money that is spent on gadgets that promise to "unleash the secrets of the cosmos and the inner workings of God," with the latter added purely to win Republican support in Congress, but ends up refining a constant by an inconsequential amount.

Jack D. Ripper
It's not that the US surveillance industry is afraid of the public, but they do deny them their essence existence.

Once he decisively took a seat in my office, he leaned back, and I could imagine him taking a puff from a non-existing cigar, Ripper-style. This person knew how to pause. He then said "So. You're a systems guy. You must know about systems." I braced for a question from left field, probably involving flouridation and the purity of our precious bodily fluids. "What if I had a graph. A really, really large graph. Billions of nodes. Trillions of edges. Let's say every node on the graph was a person. Edges between people described phone calls, interactions, stuff like that." He paused for dramatic effect, as he mentally took another puff from his non-existent cigar. "How would you find bin Laden?"

The man was literally looking for bin Laden, in my office.

And he was using Big Data to do so, long before it was a trendy buzzword.

Big Brother discovers Big Data
It has been no secret whatsoever that the intelligence community in the US has been collecting and collating data at a massive scale. And any techie will tell you that you can't operate at "Google-scale" while remaining compliant with all those pesky laws and regulations that restrict the government from performing domestic surveillance. This data is inherently tainted and inseparably intertwined. Anyone who pretends otherwise has a Total Information Awareness program to sell under a disguise.

So, I knew it, you knew it, and everyone else knew it long before Snowden revealed awfully designed PowerPoint slides that confirmed what we all knew. And thank God for that, so we can now discuss where we are and how to proceed from here.

But to have a rational discussion, let's first dispense with the false indignation and cheap righteousness that seems to run freely in DC these days.

We Were All Snowden Once
And we failed to take a stand. We consumed reports about "terrorist chatter" without asking how those reports were compiled. We knew that Atta and friends were not hanging out on IRC in the #terror channel, chattering away. In actual fact, they were using messages left in GMail Drafts folders to evade detection, and to get at this information, you either use human intelligence (HUMINT) or signals intelligence (SIGINT). The news reports clearly implied that we were hearing the distilled result of massive SIGINT, and the nation ate this stuff up. So if any Congressperson is going to get indignant now and get upset about what was happening, let me remind them:

Scent jars of the Stasi
Das Parfum seemed like a creepy novel until the scent jars of the Stasi were revealed.

We knew it because we were warned. There was no shortage of people who spoke out while the PATRIOT act was being passed that the ensuing culture of surveillance was going to erode values we hold near and dear. Many people urged caution, and noted that we needed to define that which we hold sacred, so we can uphold those principles even as we fight terror. Kind of the way Norway did when tackling the threat Breivik posed to their open society.

But we threw that all away. It was the result of a decades-long process of wussification, enabled by lack of leadership. John Wayne was long dead and gone, the Marlboro Man had died of emphysema, Kurt Cobain had offed himself, and Superman was on a ventilator. There was no one who seemed to possess a pulse and a sense of American principles; in fact, one of the top people in charge lacked both. I cannot overemphasize the damage this overreach did to our identity as a nation. Even from where I occasionally sit, on the admissions committee at an Ivy League university, its impact is pretty clear: the number and quality of our overseas applicants dropped as we squandered our moral high ground.

We knew it because we told them to do it. Massive data collection is something that the intelligence community is explicitly tasked to do. They would be remiss in their duties if they did not actually attempt to collect everything they can collect. This community consists of hard-working, reasonable individuals who have been handed an impossible, ill-defined mandate; namely, protect US interests here and abroad against all possible threats. And the mandate comes from a public that has grown exceedingly soft, one that is all too ready to compromise its core principles. It is no surprise that they will encounter gray areas, and in fact, it is not even a surprise that they will overzealously venture onto not so gray areas.

And we actually need the NSA to perform signals intelligence. Shortly after the Snowden leaks, there were calls for ceasing all SIGINT or defunding the NSA. Anyone who advocates such an extreme position is either not living here or hopelessly naive.

But the boundaries of such data collection have to comply with our values and have to be arrived at through a collective debate. "The Johnsons do it" is no reason to compromise that which defines us, even, or especially if, the Johnsons are Chinese or French. The same establishment that is currently using the French as justification for surveillance were calling them all kinds of derogatory surrendering simian names just a short while ago.

Tigers have stripes and massive data collection of all kinds is what this community does with tax money. This is why checks and balances are necessary.

We knew it because it's in their DNA. Massive data collection is nothing new. It was only a few years ago that the Stasi archives were opened up to reveal "Geruchsproben," carefully catalogued jars containing the smell of their citizens. The Stasi had field agents collect smell samples, sometimes by having the citizens sit on special chairs, or sometimes by breaking into people's homes and literally stealing their underwear.

Sharon Stone
She has no reason to fear the underwear-stealing Stasi.

The obvious reaction is to get outraged and demand to know "under what conditions would the smell of an individual be of any use to an intelligence officer?" As with most obvious questions, this question, its answer ("to sic the hounds"), and the ensuing debate are all a waste of time. One can already imagine the headlines. Wired will discuss, at length, the mechanics of tracking people by scent, with a special highlight on a nose designed out of commercial-off-the-shelf components by the MIT Media Lab that outperforms a hound in carefully controlled laboratory experiments, as long as that laboratory is squarely inside the MIT Media Lab. DailyKos will ring out with indignation, while the National Review will ask why anyone would use perfume if they didn't have something to hide. When the topic is thoroughly exhausted, when it is so universally accepted that the act of speaking up carries absolutely no political risk, mainstream writers like Thomas Friedman will jump into the fray. The only sane voice in all of this will be a short letter by a NYT reader from Kalamazoo, Michigan, pointing out to no one in particular on page C6 that "to sic a hound on a person's scent trail, you need a starting point for the scent, for the hound cannot perform investigative analysis to locate someone, and if you've got a starting point, why do you need the scent sample?"

By the way, speaking of DNA: You can be sure that there are massive DNA databases in cold storage, and it'll take a Snowden Junior for that other discussion to come to light. We know that the intelligence community collected DNA data at the risk of forever tainting vaccination efforts worldwide. How surprised would anyone be if busboys in trendy DC bars near Embassy Row earn an income on the side, swiping saliva from the used utensils of foreign emissaries, or if worldwide bone-marrow registries have been compromised to compile aggregate DNA data for different ethnic groups.

Once again, this industry collects every kind of information when it's left unchecked. All the soul searching and righteous indignation from inside the beltway is cheap, after-the-fact posturing.

For the right question is: what will happen now that the Snowden leaks are public? Having re-directed the emotional angst that this topic attracts, we can now dispassionately analyze how we will find the balance point between national security interests and privacy concerns. But to make any prediction, we need a framework.

A Framework for Analysis
There has been a lot of discussion on how the Snowden saga will end. I am referring of course to the actual part of this story that has societal consequences, not to the human interest story around Snowden that the media wants to play up. While the questions that relate to Snowden-the-man are interesting, e.g. "what motivated him to give up his life in Hawaii?", "did he time his leak to undermine the Sino-US talks?", and most importantly, "what is it like being stuck in an airport for life?", they are essentially of very little consequence to the rest of us.

Instead, I want to offer a simple framework for analyzing Internet policy issues, unabashedly cribbed from the Internet visionary David Clark, but amended with a quantitative model that can tell us what to expect. So, let's do some soft science on a topic that involves technology, policy and society.

Three Forces
All online policy emerges as a result of three competing forces. Think of them as vectors whose net sum determines what happens online at the end of the day. Once we sort out the strength and direction of each force, it becomes really trivial to figure out what will happen next.

Here are the forces, their direction, and their magnitude:

Military/Political:
This force vector consists of the military, intelligence and political establishment, whose aims are to keep online social movements in check. Its ostensible goal is to control all online interactions. The force vector points in a direction much like mainstream media today: left unchecked, this force will reduce the number of voices, limit how they can be expressed, and eliminate all discourse except for a handful of corporate-approved messages. This is the right arm of Big Brother, with swole fingers from the 5am morning beltway jog followed by crossfit that makes typing out nuanced computer policy difficult.

It's this force vector that tried to ban encrypted communications for decades and tried to make it illegal to export three-liner perl scripts for encryption. It's the same forces that advocated, and still continue to advocate, a "driver's license for the Internet." Everyone who has been on an online forum and engaged in a meaningless online fight has felt like wanting to hunt down whoever is hiding behind that pseudonym. When I ran an online forum where political discussions took place, the main request I received from the older folks was that I "demand users to fax me their identity cards." While most young people have deeply understood that this is neither possible nor desirable, the people who are in a position to make Internet policy proposals have yet to learn this lesson.

So it stands to reason that some of the more aspiring members of this community will want to collect everything everyone does online, keep it in a big datacenter in the middle of the country, and run queries against it to see who's up to what.

And these folks have an enormous budget. Unopposed, they're unstoppable, for what kind of a president can overrule this community, and still count on them to feed him useful analysis later on? The only thing tampering the destruction that this force could unleash is that it historically has not been technically savvy, but this is, evidently, changing.
Commerce:
This force vector consists simply of the collective economic interests of companies that fund elections. And it points in the direction of making the Internet a "pay-for-play" environment, of maximizing revenue extraction from Internet users. It has no ethics or higher goals; it is axiomatic, in our current times, that companies are motivated solely by greed and answerable solely to their own shareholders, regardless of the fact that they rely on countless resources from their surrounding society. This single-minded drive for profit maximization actually makes it easier to analyze this force.

The commerce vector typically operates synergistically with the military/political vector, where one herds the online populace to a few corporate-owned and operated choke-holds, and the other extracts profits. But the commerce force does break ranks with the M/P force, to create more communication channels instead of fewer venues, to create more interactions among users instead of suppressing them, and so forth. And as powerful as the military/political force vector is, and as many dollars as they command, the commerce force vector commands two to three orders of magnitude more. So they handily beat the Military/Political forces every time they point in different directions.

What happened with encryption makes the relative magnitudes of the forces very clear. For decades, it was US policy to prohibit the export of cryptographic algorithms. And the cool geeky t-shirts with those perl three-liners did absolutely nothing to change policy -- the geeks were too few in number, and "freedom to share cryptographic algorithms" was not exactly a cause the lay public could rally behind. But the moment the US computer industry decided that, for its own competitiveness, it needed strong encryption on the Internet, the politicians suddenly discovered that the First Amendment applies to crypto just as surely as it applies to everything else. The transformation was overnight, and it brought us good things, like SSL and online commerce and a host of other developments that make the world a much more interesting, and better, place.
Public:
This force vector consists simply of the collective human interests of the people who use the network. It is by far the most powerful force, but has a number of shortcomings: it is slow to awaken, not technically sophisticated, and easy to derail and divide into factions over trivial concerns. But once the giant is awake, absolutely nothing can stand in its path.

What makes the public stand up and take a stance? No one knows. The Arab Spring was precipitated by a street salesman whose cart was taken away by the police, who got so depressed that he decided to put himself on fire, and before we knew it, dictators across many continents were spinning up their chopper blades. The Turkish uprising was precipitated by a couple of trees in a park. Second wave of Brazilian uprisings were over a 10 cent hike. This makes this force terrifying, because when the giant shows signs of awakening, when his eyelids flutter and he's asking questions trying to get his bearings, it's too late.
Vector Sum
I propose a simple technique to decide which of these forces will reign supreme based on simple high-school math with a dash of historical analysis.

On issues of Internet governance, I propose the use of dollars as a common, universal unit of strength for measuring societal forces.

sum up the amount of spending that the M/P force field is going to need
to achieve its goals. There are slightly different multipliers for "new funding to be appropriated" versus "already allocated funding," as laying people off meets more resistance than new pork-barrel spending, but a unit multiplier is sufficient for first-cut analysis. Add to this the cost of catastrophes that the policies could prevent, multiplied by their likelihood.
sum up the amount of revenue the commercial sector stands to gain or lose
under different policy regimes.
sum up the amount of economic activity by which the proposed policies
affect citizenry.
Doing all this quantitatively is difficult, but engineering is all about back of the envelope calculations.

In this case, the first number is simply the sum that the intelligence community spends on eavesdropping at large scale, combined with very tiny likelihoods for events that have modest costs. Contrary to the $30M claim in the Snowden slide deck, the cost of the datacenters and the analysis personnel will likely be in the single to very low double digit billions.

The second sum is the dollar amount that the cloud providers would lose due to direct loss of revenue from antsy customers, especially foreign ones, as well as the indirect costs of losing dominance in their field.

Finally, the last sum is the value people place on activities that cannot take place in a surveillance society. It is almost impossible to estimate this, for we cannot know, say, the dollar figure a dissident would place on being able to blog unfettered, or perhaps, the dollar value Andrew Weiner would place on keeping Carlos Danger's emails private. But a good proxy for this metric is simply the amount of money people plaintively spend on underground activities, a small percentage of which would be curtailed in a surveillance state. Given that our underground economy is roughly 1-2 trillion dollars, even tiny percentages have impact.

So my rough guesstimate is that the forces are aligned in the ratio 1:1:3, with an alliance of the public and commercial interests that overpowers the M/P establishment in favor of transparency and online privacy guarantees.

Let's sanity check: measuring by the highly scientific metric of column inches in newspapers that I have personally seen (yes, I'm fully aware of the bias and write this with a tongue firmly in cheek. I would love it if someone would help do a proper quantitative analysis), I see an approximate 2-to-1 ratio. For every unabashedly condescending, non-self-reflecting, pro-surveillance gung-ho article in WaPo, the beltway insider rag that once attributed the invention of email to a self-proclaimed child-prodigy from MIT who made misleading claims through Wikipedia, there are at least two articles critical of the alleged activities. The cloud companies have already started to feel the sting of lost profits and have initiated a push for transparency. And in line with our estimates, the reaction from the public has been much stronger than the tepid call for transparency from industry.

The numbers suggest that the US will emerge out of the Snowden debacle with a set of processes that prohibit the kind of domestic surveillance that Snowden exposed. But the forces are fairly close, and the victory will be a highly qualified one. We'll get the minimal set of changes such that a figurehead can say "we do not perform domestic surveillance" with a straight face, for a specific definition of every word in that sentence.

Loopholes
For instance, we may be left with loopholes big enough to drive trucks through, say, trucks containing UK data from the US to the UK, and lorries containing US data from the UK to us. Cloud computing makes it trivial to escape pesky jurisdictional obstacles by sending queries that used to go to a datacenter in Utah to, say, Ireland instead. Closing this loophole will prove to be difficult, because information-sharing between different nations is actually desirable, and the legal system is better at binary decisions than those of degree.

Or we may end up with only token changes to the way court orders are retroactively issued. The idea behind the current scheme is that the surveillance engine can collect data now, under hot pursuit, and justify it later. While there is merit to the hot pursuit argument, its unrestricted use garners an environment where results come first, principles are an afterthought, and there are no effective checks against overreach.

Or we may never get what is most needed, which are strict limits on how the data, once collected, is used. We now know that a contractor in Hawaii has access to the entire crown jewels of the NSA, namely, "metadata" about the kind of information they can collect and analyze. How many contractors have access to the lower-value phone call "metadata"? There are now naive proposals to avoid a second Snowden mishap by doubling-up every analyst and have someone look over their designated buddy's shoulder. Besides doubling employment in the DC metro area, this ill-conceived attempt only makes it more likely for data to fall into the wrong hands. What we need are trustworthy processes, backed perhaps by trustworthy operating systems that can provide assurance that no one, not even system administrators, can violate a policy associated with a piece of data. Anything less opens us up to a "deep state," where certain elements within government misappropriate the surveillance data for their own ends.

A Success Indicator
In big battles, it's instructive to pay special attention to certain smaller engagements that serve as litmus tests. In this case, I expect the "abandoned email loophole" to serve this purpose. The 1986 Electronic Communications Privacy Act classifies old emails left on a server for more than 180 days as "abandoned" and gives the government authority to read these emails without a warrant. This is clearly an outdated loophole, one that would undermine discerning users' and companies' willingness to store emails in the cloud. If the cloud providers really feel the sting of user apprehension about surveillance, and if they really put their weight into fighting on the same side as the public, this loophole would quickly be shut down. Whether or not it is indeed firmly closed will be an indicator of genuine change of surveillance policies.

Overall, it's time to be somewhat optimistic: the fundamentals point to a ground shift, where the commerce and public forces are now exerting an influence on previously unchecked elements in government, and the net vector points towards a freer, better Internet. But there are reasons for concern: the battle will be drawn out, victories will be partial, and the extent to which loopholes get left behind will determine how much privacy we have online.

Dr. Strangelove meets Gen. Ripper
Dr. Strangelove
He can actually walk if we let him.

Back in 2003, the rest of my conversation with my esteemed visitor was a lot of fun. I told him what I'd do to handle such a large graph. And I gently told him that I wasn't building such a graph database. I wasn't then, but I am now. We have a system called Weaver in the works, inspired by the revolutionary HyperDex database. I realize that its utility is much lower now that bin Ladin has been located and rightfully dispatched, but there will undoubtedly be other massive data sources, and we'll need systems that can handle them. For we cannot afford a graph database gap, any more than we can afford a mineshaft gap.

But in addition to such a database, we need tightened definitions for what kinds of surveillance data can be collected, as well as technical and legal measures to keep that data used solely in accordance with appropriate policies. Interestingly, there are technologies that can restrict what users, including Snowden-like "super users", can do with data. Once we re-establish our principles, we have the technical means to enact them. But first, the current era of covert, boundless data collection must come to an end.

Related
In the movie, Dr. Strangelove meets with the President and Gen. Turgidson but not Gen. Ripper. It had long been an open question what would have happened had the technocrat met the power-hungry general. Thanks to Snowden, we now know.

David Clark and Susan Landau disentangle the discussion over Internet driver's licenses.

- See more at: http://hackingdistributed.com/2013/08/01/framework-for-surveillance/#sthash.C0ipedZL.dpuf

So, Yahoo is in the news for buying Summly. Summly is a company that extracts summaries of natural language text, a TL;DR of sorts.

Summly licensed its core technology from SRI, which, previously, spun out Siri and sold it to Apple. Summly had 5 engineers, only 2 of whom will be moving to Yahoo. Summly is reported to have 1M downloads of their app in mobile app stores.

I want to take a few minutes to process this, because it points to some trends that should cause any technologist to raise an eyebrow.

Topics that are off the table
Let's leave aside three unrelated factoids, namely, the Summly founder is only 17 years old, the company was in existence for only 18 months and Yahoo reportedly paid $30M for it. Why should we put an artificial barrier around these topics?

First, it really doesn't matter if the founder is a high-school student. If he had a bright idea, he deserves the spoils. Big Media, which has perfected the art of trolling the masses, is making sure to include a picture of the youthful founder in every story already, and we need to rise above that.

Second, dividing someone else's spoils by amount of time they spent earning them leads us to question all kinds of things. We know where that leads: to more progressive taxation, class warfare, and the end of the world as the uber-rich know it. Our current zeitgeist cannot broach these topics, so they're off the table.

Finally, it doesn't matter how much money Summly got or whether Yahoo wasted its money. By definition, Yahoo's directors know how to spend Yahoo's cash reserves best. In any case, company valuations are far out of the expertise of most people. So let's leave this boring topic alone. The directors have clearly said that this is the best thing Yahoo can do with $30M at this moment in time, which we have to take at face value.

And if we don't take these factoids off the table, others may think that our analysis is tainted. Do keep in mind that every academic of my generation stayed in graduate school through at least one dotcom boom. I interviewed at DE Shaw when Bezos was there and turned them down for 1/10th of their salary offer. The field is lucrative enough that we've all done very well anyway.

Rise of Bolt-On Engineering
Bolt-on bike
Bolted together in the US of A, but with pride?

I want to first focus on this from a technologist's perspective, and there is only one germane fact: the company developed little Natural Language Processing technology of its own.

They licensed the core engine from another company. They are the quintessential bolt-on engineers, taking a Japanese bike engine, slapping together a badly constructed frame aligned solely by eyeballs, and laying down a marketing blitz. That's why the story sells. "You, too, can do it." But do you want to?

In some sense, everyone is a bolt-on engineer. Nobody rolls out their own fab and builds up from raw silicon; we all reuse some component or another, even if it's a language runtime, a web framework like Django or Rails, a protocol like Paxos, a fast database, or a library, say for numerical analysis or even natural language processing. Even CS theoreticians are, in a sense, reusing techniques from math. Everybody stands on the shoulders of the giants that came before them, and all that.

But it's critical to keep tabs on the ratio known as "glue versus thought." Sure, both imply progress and both are necessary. But the former is eminently mundane, replaceable, and outsource-able. The latter is typically what gives a company its edge, what is generally regarded as a competitive advantage.

So, what is Yahoo signaling to the world? "We value glue more than thought."

If Summly is an innovative company worth purchasing, I have some news for Yahoo: my AI colleagues have tricks up their sleeves that will blow your minds!

Let's get some perspective here: Summly wasn't reading Ulysses by James Joyce and extracting the fact that the three-masted ship Leopold Bloom sees on the horizon is a metaphor for the Holy Trinity and therefore represents the Catholic Church. It wasn't reading a 12 page article in Harper's and extracting the cleverest puns and pop culture send-offs lovingly embedded by a writer who is good at his craft and earning below his potential. And it wasn't taking my blog posts and somehow conveying the nuanced ennui I harbor for bolt-on engineering.

It was summarizing news. Articles that are already written with a TL;DR in the first paragraph.

For 95% of the news I read, that can be done with a regexp that slices out the first sentence. Very rarely, the first paragraph contains what journalists call a "hook," and the infamous 5-W's are embedded in the second paragraph. So if it worked perfectly, Summly would eliminate one extra sentence 5% of the time.

And if Yahoo were to look at the work of anyone who is active in NLP (e.g. Claire Cardie, Lillian Lee), it'd immediately discover that this is a deep field full of exciting developments at its core. Gluing an NLP engine up to news surely adds some value, but pales in comparison to what cutting edge NLP algorithms can accomplish.

So if Yahoo is to be a technology company, it needs to do core technology acquisitions that give it a competitive advantage. Glue is not that kind of advantage.

Rise of TL; DR Culture
tldr
Too Hard; Didn't Try

From a societal perspective, we seem to be in the midst of a TL;DR wave. I wrote about this before when my post on MongoDB's fault tolerance was getting really dumb responses from people who seemed to have difficulty reading:

Look, I realize that we live in a TL;DR culture. I lived through 8 years of a non-reading president along with everyone else. I know that the brogrammers out there are constantly getting texts from their buddies to plan the weekend's broactivities, trying to decide in whose mancave they'll be setting up their lan party, and are thoroughly distracted in between futzing with their smart phones and writing a few lines of code per day by cutting and pasting it from stackoverflow. But it's really not ok to act functionally illiterate when you're not actually illiterate, when an advanced society that once put a man on the moon worked so hard to educate you.
Summly's entire business model seems to revolve around catering to this demographic. Frankly, it pains me.

Our time is valuable, and we definitely need tools that help. And because digital information sources have become repetitive echo chambers, I would welcome tools that can extract the latent signals. "This article is really a fluff piece paid for by tobacco interests." "This picture of attractive happy people of different races mixed together in the same proportion as society at large, sipping lattes at Starbucks, is probably an image ad by Starbucks." "Yahoo makes outrageous purchase to get people to talk about its dying brand, and perhaps to indicate that it has cash to waste, the same way a wildebeest chased by lions paradoxically jumps into the air to telegraph 'I am so healthy and have so much energy that I can afford to waste some of it; you're after the wrong prey.'"

That is, analysis would be useful. And analysis requires content that is not syntactically in the message itself. What's wrong with the world is not that we do not have time to read, but that reading is so frustrating. On many topics outside our expertise area, we lack the extra information to extract informed opinions -- we lack the capacity and context to judge. Anyone can read the topic sentences of paragraphs to extract a summary (a "tl;dr") from any piece of writing of any length. The action in this space is to get at the hidden message that lies behind the words.

That's why TL;DRs are often just as worthless as the text they summarize. They mark pieces of badly structured thoughts that look like a wall of text and read like one. The action in AI is happening elsewhere, tackling the deeper problems such as sentiment analysis.

How to Do Due Diligence
Finally, the entire Summly discussion seems to be missing a critical element: there seem to be no happy users of Summly. A search for "I use Summly" reveals 6 hits, only two of which seem unique. For 1M downloads, that's a strange outcome.

But it's not unexpected. For only a robot would fall in love with a product that robotically extracts TL;DRs.

Taking the FFT of a News Announcement
I have this hunch that every news announcement can be expressed as a combination of basic universal sentiments (yes, there is some irony discussing this here), the way an FFT breaks down a complex signal into sine waves.

So, some might disagree on the coefficients, perhaps, but it's clear that the following sentiments have non-zero terms:

"bolt-on engineering is considered innovative in our company,"
"our demographic is of the TL;DR variety,"
"we're not caught up to the cutting edge in NLP, and we're buying our way into it not from the top-down, starting with intellectual leaders, but from the bottom-up, starting with iPhone applications."
- See more at: http://hackingdistributed.com/2013/03/26/summly/#sthash.9UCQmJpQ.dpuf


As you're undoubtedly well-aware, there are some very strong geek fashion trends in the valley. I don't mean fashion in the sense of geek haute-couture -- the fashion trends we're talking about here have to do with tech components. You've heard of it before: "here's how we built fubar.com using X, Y and Z." The blogosphere regurgitates this dreck, and people base their adoption strategies on how much noise they have heard about various X's, Y'z and Z's.

OCC fire bike.
A mockery of good engineering.

This kind of software construction is the modern equivalent of the bolt-on motorcycle shows on TV. You know the shows where the main bro-mechanics spend at most 5 minutes of any 44-minute show actually building something, combining the least amount of effort with the maximal amount of personal aggravation and bickering, very much like a soon-to-fail valley startup. They spend absolutely no cycles on essential design. No part is ever sized or spec'd or chosen for its merits, or for any reason besides looking like the other cool kids. The core of the operation, the engine, comes ready-made from a factory and is bolted onto the frame any which way that fits. The frame is most certainly not aligned with anything more precise than a pair of eyeballs. The only thing the builders spend their time on are useless design elements (e.g. "we welded this head of a 5-iron golf club onto the gas tank since the owner loves to golf"). It's as if the whole show was designed to mock American engineering.

Ugly chopper.
MongoCycle: unsafe at any speed.

For a while, I thought that the problem with this approach was that the engineering had been outsourced. But that's not the problem at all; there is nothing wrong with reusing components designed by other people. The problem here is that the thinking has been nullsourced -- there is none taking place on or off set. These guys put these weird-looking bikes together this way solely because they've seen others do the same. They question none of the assumptions, can provide no performance guarantees, and they don't even check how the bike handles.

And us geeks are no better. There is no shortage of valley startups where component selection is based on hearsay alone, where group-think and blog noise determine what components get used, and frankensoftware is pieced together by slapping together components without thinking about how they mesh together. I want to focus on just one popular software component today, called MongoDB.

How is Mongo Broken? Let me count the ways
MongoDB is a NoSQL data store. NoSQL offers a range of features, namely, performance, cost, scale, and elasticity, that are at best difficult and expensive, if not impossible, to achieve with traditional databases. So MongoDB could be cool. In fact, it used to be cool, because it was one of the earliest NoSQL systems out there, but there are second-generation data stores now and we can afford to apply normal standards. And by any standard, a data repository, especially one whose main selling point is high scale, needs to deal with failures.

It lies
So, let's imagine that you're building an Instagram clone that you'll eventually sell to Facebook for $1B, and see if we can accomplish that with MongoDB. Your site probably has a front-end web server where users upload data, like putting up new pictures or sending messages or whatever else. You can't really drop this data on the ground -- if you could lose data without consequences, you would just do precisely that all the time, skip having to configure MongoDB, and read all those dev blogs. And if you possess the least bit of engineering pride, you'll want to do an honest stab at storing this data properly. So you need to store the data, shown in yellow, in the database in a fault-tolerant fashion, or else face a lot of angry users. Luckily for you, MongoDB can be configured to perform replication. Let's assume that you indeed configured MongoDB correctly to perform such replication. So, here's the $1 billion dollar question: what does it mean when MongoDB says that a write (aka insert) is complete?

written to one
A. When it has been written to all the replicas.

written to all
C. When it has been written to one of the replicas.

written to majority
B. When it has been written to a majority of the replicas.

The answer is none of the above. MongoDB v2.0 will consider a write to be complete, done, finito as soon as it has been buffered in the outgoing socket buffer of the client host. Read that sentence over again. It looks like this:

written to all
D. When it hasn't even left the client.

The data never left the front-end box, no packet was sent on the network, and yet MongoDB believes it to be safely committed for posterity. It swears it mailed the check last week, while it's sitting right there in the outgoing mail pile in the living room.

Now, if machines crashed infrequently or never, this design choice might make sense. People with limited life experience can perhaps claim that failures, while they might "theoretically" happen, are too rare in practice to worry about. This would be incorrect. Hardware fails. More often than you'd think. System restarts are quite common when rolling out updates. Application crashes can occur even when using scripting languages, because underneath every type-safe language are a slew of C/C++ libraries. The software stack may need a restart every now and then to stem resource leaks. Bianca Schroeder's careful empirical studies show that failures happen all the time and if you don't believe academic studies, you can talk to your ops team, especially over a beer or five. Failures are common; it'd be silly to pretend they would never strike your system.

Since NoSQL systems depend fundamentally on sharding data across large numbers of hosts, they are particularly prone to failures. The more hosts are involved in housing the data, the greater the likelihood that the system as a whole will encounter a failure. This is why RAID storage systems, which spread data across multiple disks, have to spend some effort making sure that they're as reliable as a single large expensive disk. Imagine if you had invented Arrays of Inexpensive Disks (AID) back in the day, but weren't clever enough to invent the part involving the "R", namely, redundantly striping the data for fault-tolerance. I hate to break it to you but, far from applauding you for getting pretty close to a great idea, everyone would just make fun of you mercilessly. So, we can't just not replicate the data, not if we want to retain our engineering pride.

getLastError won't
Now, seasoned MongoDB hackers will say, "wait! there is an API call named getLastError. Yeah, it's badly named, and yeah, it probably was not meant to be used to control fault tolerance, but it checks the number of replicas to which the last write was propagated. You could use that to see if your write propagated sufficiently for your fault tolerance needs." Ok, I'll play along. Let's see what it takes to use this call correctly.

It's Slow
First of all, using this call requires doubling the cost of every write operation. You would have to follow every insert with a getLastError to make sure that that write succeeded. This generates extra network traffic. Yes, there is some noise about piggybacking the insert with the getLastError request in the MongoDB code, but at the time of this writing, it is unimplemented. MongoDB disables Nagle, so your getLastError call will require a separate packet to the server. Your MongoDB performance, which was not the fastest to begin with, just took a dive.

Performance loss would not be a huge problem, except that MongoDB's entire design goal is speed. It compromises consistency and fault-tolerance to gain performance. But it's a strange Faustian bargain to give up on fault-tolerance and consistency, and to not end up with performance, either.

It doesn't work pipelined
To get some performance back, you might have your thread pipeline its inserts. So your thread would perform "insert(A), insert(B), insert(C)". When you then issue getLastError(), and MongoDB says that the insert completed, which actual insert is it talking about? A, B or C? This is one area where the MongoDB documentation actually provides some explicit guidance and clarity. Here's what it says:

 If the bulk insert process generates more than one error in a batch job, the client will only receive the most recent error. All bulk operations to a sharded collection run with ContinueOnError, which applications cannot disable. [1]
 Great, the documentation explicitly says that there is no way to check for errors on any operation except the last one.

 It doesn't work multithreaded
 But further, unless you're building a joke of a site for which a scalable NoSQL data store is completely unnecessary, you'll have multiple threads performing writes in parallel. And what exactly does getLastError return when you've got 16 concurrent threads issuing 16 writes at more or less the same time? Which one of those 16 concurrent writes is the last one? The behavior of getLastError in a multithreaded environment is undefined in the MongoDB documentation. And in the actual implementation, the MongoDB Java bindings recycle connections in a connection pool between threads in a way that is completely outside the control of the programmer, so a getLastError call by one thread may very well end up returning information about the last write issued by another thread.

 WriteConcerns are broken
 Now, the faults above are pretty damning, but the thing about badly-designed APIs is that there will be multiple, redundant alternatives for every task. For instance, the MongoDB documentation says that in lieu of calling getLastError() manually, one could use WriteConcern.SAFE, FSYNC_SAFE or REPLICAS_SAFE for the insert operation [2]. There are 13 different concern levels, 8 of which seem to be distinct and presumably the remaining 5 are just kind of there in case you mistype one of the other ones. WriteConcern is at least well-named: it corresponds to "how concerned would you be if we lost your data?" and the potential answers are "not at all!", "be my guest", and "well, look like you made an effort, but it's ok if you drop it." Specifically, that's three different kinds of SAFE, but none of them give you what you want: (1) SAFE means acknowledged by one replica but not written to disk, so a node failure can obliterate that data, (2) FSYNC_SAFE means written to a single disk, so a single disk crash can obliterate that data, and (3) REPLICAS_SAFE means it has been written to two replicas, but there is no guarantee that you will be able to retrieve it later. More on REPLICAS_SAFE and MAJORITY levels of concern in a future blog post; it's subtle, so it requires its own detailed writeup.

 Frankenbike.
 This frankenbike is probably more robust than a MongoDB-based website.

 So, MongoDB is broken, and not just superficially so; it's broken by design. If you're relying on its fault tolerance, you're probably doing it wrong.

 The Latest MongoDB is no better
 Edit: I wrote the blog entry above based on MongoDB version 2.0. It accurately reflects the state of affairs in the MongoDB universe for their first five years. Yes, a full five years of broken software.

 In November 2012, a new version of MongoDB was released with a significant change. MongoDB drivers now internally set WriteConcern to 1 instead of 0 by default. While this is a change for the better, it is still not sufficient. To wit:

 This change does not make MongoDB fault-tolerant. A single failure can still lead to data loss.
 It used to be that a single client failure would lead to data loss. Now, a single server failure can lead to data loss for the reasons I outlined above.
 MongoDB is now a lot slower compared to v2.0. On the industry-standard YCSB benchmark, MongoDB used to be competitive with Cassandra, as seen in the performance measurements we did when benchmarking HyperDex. Ever since the change, MongoDB can no longer finish the entire benchmark suite in the time allotted.
 NOT a TL;DR
 Edit #2: Some MongoDB fans seem to be reading only until they see the first problem, the one involving the default setting of WriteConcern NONE. MongoDB has been criticized so widely over this issue that this has become a well-known sore point for them. So your Joe MongoDev, with a pre-prepared reaction to this well-known problem, stops the moment he sees it and starts writing responses that say that this problem was fixed, or would only be fixed if I read the manual.

 I read the manual. I even quoted it. I also read the code. I named so many other problems besides the NONE problem that I actually lost count. I say explicitly that the simple fix in v2.2 is not sufficient. The code doesn't work when pipelined or multithreaded. Those SAFE settings don't work, and I explain why for all but REPLICA_SAFE, which I'll handle in a future blog post because it's kind of subtle, and clearly, it's already too taxing for some people to read a blog post of this length. If you're going to tell me that Mongo is this way because it's making some tradeoffs for speed, I already linked to benchmarks that show that, even with these terrible tradeoffs, it is dog slow.

 Look, I realize that we live in a TL;DR culture. I lived through 8 years of a non-reading president along with everyone else. I know that the brogrammers out there are constantly getting texts from their buddies to plan the weekend's broactivities, trying to decide in whose mancave they'll be setting up their lan party, and are thoroughly distracted in between futzing with their smart phones and writing a few lines of code per day by cutting and pasting it from stackoverflow. But it's really not ok to act functionally illiterate when you're not actually illiterate, when an advanced society that once put a man on the moon worked so hard to educate you.

 Consistency and fault-tolerance are hard. Genuinely hard. The devil is always in the details, and it takes a concentrated mind to navigate through these waters correctly. I know a thing or two about building large, complex systems, and I've done this for quite a while now, and yet it still takes me all my attention. So, chances that a Mongo fan can engage only his hind-brain, react with "LOL, tl;dr, fixed in v2.2 with a one-liner as described in Mongo manual" and actually have something meaningful to contribute are absolutely nil.

 At a higher level, this is not your typical blog, of the kind that consists of 2 or 3 paragraphs regurgitating something else gleaned from elsewhere on the Internet. It's different. It's written by a different kind of person than the typical blog regurgitator. It's written as a reaction to this very culture of non-thinking system development. And it targets a different kind, a thinking kind, of audience. So, some people will have to step up their game a notch or two if they want to be taken seriously.

 --

 Related
 Alternative NoSQL Systems that treat your data carefully and gingerly:

 HyperDex.
 Riak.
 Cassandra.
 Follow on thought to this post, describing when Mongo might make sense.
 HyperDex's Warp feature provides a glimpse of what "strong properties" look like. It provides ACID transactions, over multiple keys. It is fully distributed, with no dedicated transaction managers.

 You do not need to read blog entries describing complex NoSQL mechanisms and consistency models, and other blog entries (like this one) showing all the weird failure scenarios, gotchas and edge cases, to use it correctly.
 Recovery Oriented Computing (ROC) is all about restarting components frequently and proactively to reset their state. While the ROC approach to reliability rubs me the wrong way (I'd much prefer to build software from first principles that does not crash), the ROC papers are a treasure trove of anecdotes about how frequently systems need to be rebooted in practice.
 - See more at: http://hackingdistributed.com/2013/01/29/mongo-ft/#sthash.XYbsOzCN.dpuf


 We spend more and more of our lives online. A large fraction of our lives are digital now.

 Yet there is hardly any witness to the digital trail we leave behind. Digital events are malleable and fragile. It is possible for someone to do things online that have significant consequences for others, and to later retroactively delete, modify or change the online record, in effect altering history. A digital, neutral, dispassionate witness is critical to recording online facts and conveying them to third parties in a trustworthy manner. Such witnesses are also useful when the party being witnessed is oneself. Take, for example, the case of an inventor with a discovery, a job applicant who'd like to prove her credentials, or a borrower who wants to demonstrate the last sale price of a house as collateral. At the moment, all such claims are based on one's say-so and reputation. Ideally, there'd be a trustworthy way to acquire meaningful certificates that electronically encode such facts and present them to others.

 This prompted my group to develop a service called Virtual Notary that attests to online factoids. Virtual Notary can issue certificates on a wide range of topics, including document possession, job titles, weather conditions, and financial data, among others. A novel implementation issues independent certificates that can be verified by third-parties many years from now. The implementation also encrypts, embeds and publicizes the state of the notary on both Twitter and also in the public Bitcoin transaction chain. This ensures that the virtual notary cannot be subverted and the historical record altered, even if the website were to be compromised.

 Those of you who can immediately see the value of such a service can check it out here. It's free and will remain so for the foreseeable future. For the rest, let me first describe some of the usage scenarios that we had in mind as we were developing this service, and then illustrate how the service uses X.509 certificates and the Bitcoin public record behind the scenes.

 Document Possession
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/document.png
 It is sometimes critical to show that you possessed a particular document at a particular time.

 Imagine, for instance, that you just invented a new design for a quantum computer. The old adage was that you should write up your invention and mail it to yourself with certified mail, thereby using the postal service as a timestamping service. Having to go to the post office and waiting in line is an inefficient way of obtaining such a timestamp, not to mention how archaic it is to be dealing with stamps and certified mail slips to certify digital content. And a prolific inventor would face a logistical nightmare of unopened envelopes.

 Or imagine that you're distributing open source software. You would like to make sure that no one tampers with your software, so you diligently publicize its hash right next to the tarball link. What is to keep a hacker, who has owned your server, from changing both your tarball and the publicized hash at the same time? What measures do you have in place so you'd even notice?

 Virtual Notary provides a document attestation feature for these kinds of scenarios. In particular, Virtual Notary will issue a certificate that says, in essence, that you possessed a digital document (file, document, picture, audio recording, etc) that has a particular hash on that particular date. It issues you a certificate that you can later use to demonstrate that you had the corresponding file on that date. If one were to craft an altered document later, there would be no way to create a valid certificate with a backdated timestamp on it. So, for software distributions, a hash certificate issued for a modified version would plaintively contain a much later date than the original, making the tampering easy to check. And even better, Virtual Notary does not make or retain a copy of your document or embody it in the certificate. It provides the timestamping service while keeping the content private.

 Web Page Contents
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/webpage.png
 Lots of things get published on the web. And sometimes you need a trustworthy record.

 Imagine that someone published a web page slandering you and your life's work. "Joe User drop-kicks babies and hates apple-pie" they said, whereas everyone knows you just love apple-pie. Or they went to a rating site for people in your profession (say, educating the wayward) and wrote "he's an excellent professor but uses comic sans on his slides", a clear falsehood (you decide which part is false), and did not click the "hot pepper" icon next to the review, a grievous slight. You'd naturally call your lawyer and make arrangements to sue them to smithereens, but how do you establish that they published false claims in the first place? Search engines would not have the page indexed if the site administrator turned indexing off in robots.txt. The Internet Archive, on its shoestring budget, is unlikely to have archived that page at the instant you care about. And to a tech-savvy judge, your website printout is worth about as much as those other pieces of paper that are hanging in the court's bathroom, as it's all too easy to download a page and edit it any way you like.

 What you need is a third party that will download and attest to the contents of a web page. Virtual Notary can provide certificates that show the contents of a web page and thus serve as an online witness. The certificate is issued with a timestamp and contains the full contents, and is thus immutable even if the website is modified later.

 Twitter Feed
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/twitter.png
 People tweet questionable content fairly often, and later edit their tweet streams to excise material. The most notable example of this was the Weinergate scandal, which is too risqu for this workplace-friendly blog, but in essence, Congressman Anthony Weiner tweeted inappropriate pictures, quickly deleted the offending tweets, and then denied the entire episode using ever more creative excuses. He was ultimately forced to resign solely because some individuals had been monitoring and recording his tweets. While that worked in his case because of his position and the number of zealous eyes following his every move, what would be useful would be a dispassionate, neutral, trustworthy watchdog service that can work for everyone at all times.

 Virtual Notary uses the Twitter API to fetch and attest to the tweet stream of any Twitter user. It creates a timeless certificate that captures solely authentic tweets, as seen from the notary's standpoint on the Internet. Virtual Notary does not operate with your permissions, does not know who you are, and does not have access to your followers list, so you can use it without revealing any information. On the flipside, the target user's tweet stream needs to be public, as the notary cannot certify private tweets it cannot see.

 DNS Entries and WHOIS data
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/dnslookup.png
 I once served as a technical expert in a very public lawsuit (yes, you've heard of it). The plaintiffs, let's call them Poor Forensic Data Association of America (*AA for short), had collected evidence that users at specific IP addresses had participated in some criminal activity and demanded astronomical fines. On the defendants' side, our first reaction was to look through the IP address evidence that the plaintiffs brought to the table. And it was full of problems.

 Critical to legal proceedings is a notion of jurisdiction: if the users at those IP addresses were inside the US, they counted as an aggrieved party; outside, they were nobodies as far as that particular court case was concerned. And when I checked the IP addresses *AA had flagged as offenders, I found that a significant percentage were outside the US. Were the IP addresses reassigned during the court proceedings? That would bolster the plaintiff's case. Or was the data collection technique sloppy, placing *AA's entire argument on shaky footing? Those of you who know how often IP blocks are reassigned between RIRs will have a good grasp of the correct answer in this specific case. But the bigger point is that, had the people collecting IP data used a third-party like Virtual Notary to record the DNS information as of the moment of data collection, their case would have been much less flimsy, and they would look much more professional in court.

 A more egregious example came from court proceedings in another trial that garnered much debate in Europe, where the Turkish government jailed a large percentage of its military high-brass. At stake was a claim that the military establishment had purchased a domain name and used that website to promote incendiary speech that sought to divide the country, a charge that carries up to a 25 year sentence (ok, before someone naively exclaims "but all speech should be free", let's take it as an axiom that laws on acceptable speech and conduct varies between countries). Critical to this claim was DNS data, WHOIS data and web contents. The defendants claimed that the website was one of many that was owned and abandoned, and that it had been picked up later and used by the government to frame them. Indeed, the government could not furnish evidence that tied the DNS information, WHOIS information and the web contents together at the same time to the defendants. (Interestingly, this did not keep the government from keeping the defendants in jail. Many experts believe that the government has also used backdated forgeries in this case, where the font of the document did not exist on the date placed on the document. Around 415 people are currently in jail).

 Virtual Notary can provide certificates that show DNS mapping data as well as WHOIS data. If you need to tie a DNS name to an IP address, or an IP address to a DNS name, or a DNS name to its publicized owner, Virtual Notary can help. Keep in mind that Virtual Notary cannot provide information beyond what is available in public databases. We collect, retain and research absolutely no private information; instead, the notary acts as an impartial witness to publicly observable factoids.

 Exchange Rates
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/exchangerates.png
 Suppose you went to France and purchased a bottle of wine using your credit card. And let's imagine that your loving bank, which you voted to rescue from the brink of bankruptcy with your hard-earned taxes when it was in trouble, decided to use this opportunity to stiff you by using an exchange rate that was less like Euro to USD and more like Euro to the Kyrgyzistani Som. To launch a proper dispute, you need an official record of the exchange rate. Virtual Notary can furnish that. The certificate contains the full details of the currency service so the provenance of the data can be traced back to the original source.

 Web Page Contents
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/stocks.png
 You told your broker to sell Dangdang Enterprises (a real Chinese retailer) if the price drops 5% below your buy. Yet you find yourself with a stock portfolio full of Dangdang despite the stock having tanked this year, and all you can do is repeat the name of the company to yourself. Resolving the dispute with the broker will require some demonstration that the stock did indeed tank below 5% of your buy.

 Virtual Notary provides a plugin that can attest to the near-real-time stock quotes provided by the Yahoo Finance Service. Now you can have a verified, dispassionate third party weigh in on the stock price.

 Weather Conditions
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/weather.png
 It can get cold in upstate New York. Not just cold enough that your eyes tear up, but cold enough that they tear up and freeze in your eyelashes, and leave you unable to see anything but a fuzzy white blob. You're left wondering what happened to your eyesight, whether it's physically possible for your cornea to get freeze-dried, and whether you'll see the world again. When I tell this story, some Minnesotan always counters with a story about how all four doors on their car froze shut so they had to crawl through the trunk, and all this happened in a heated garage. Well, we need a way to settle these disputes on who has the worst conditions.

 More usefully, farmers making insurance claims may have to document the extent of their crop damage. They, as well as the many people who buy and sell derivatives based on weather events, may need tools to attest to weather.

 Virtual Notary can provide an attestation to the current weather conditions at any zipcode. This plugin is limited to the US at the moment; if you know of a weather data source that extends beyond the US, please let us know.

 Job Affiliation
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/universitysearch.png
 We all get asked what we do for a living. While this is a critical question in certain settings (e.g. mortgage applications), I doubt that more than a few percent of these self-declared titles actually get checked. And while it's kind of fun to make up job titles (e.g. my favorite euphemism for any job where you're just hanging around, running out the clock is "chief eventual consistency engineer"), on occasion, someone needs a verified credential check. The main way people verify someone else's job title is by performing the check themselves. Not only is this tedious, but things get complicated if you were employed at time X but the check happens at a later time Y.

 Virtual Notary can provide a job and title attestation for a handful of employers. This service is limited to Cornell, UIUC and MIT at the moment. We welcome code for a larger number of employers.

 Housing
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/realestate.png
 Sometimes, it is useful to ascertain that 1060 West Addison is indeed 3 bedroom property that is assessed at $650,000 and was last sold three years ago for $750,000. Virtual Notary can do this, based on data from Zillow.

 Promissory Note
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/promissorynote.png
 It's often useful to record certain statements and agreements. Whether it's a certain promise from a boss, friend, or co-worker, there is a need for a website where someone can say "if X were to happen, I will do Y."

 Virtual Notary enables people to issue such promissory notes. The format of the promissory certificate is free-form English, which provides maximum flexibility on what can be expressed. And the identity of person X can be tied down to a specific email or IP address. Entered a bet about how you'll eat your shirt if some startup is still alive in a year? Sign it in (virtual) blood with a promissory note.

 Officially Random
 http://hackingdistributed.com/images/2013-06-19-virtual-notary/random.png
 Many cryptographic protocols need an officially agreed-upon, public random number seed. Virtual Notary can serve this function, with the aid of random numbers provided by random.org. The randomness is derived from measured atmospheric variations that are difficult to predict or model, and considered trustworthy by many cryptographers.

 Limitations
 Virtual Notary is a technological proof-of-concept. It is not an official, legally-recognized notary, whose definition is provided by law, whose statements are court-recognized, and whose procedures are regulated. For operations that require legal standing, you should seek the help and services of an official, state-recognized notary. Please do not use Virtual Notary for something critical, like your will -- you should enlist the services of a lawyer and a certified notary public.

 Truth vs. Notarization
 A common misconception is that notaries stand for or attest to the absolute truth. That task is far too difficult and far beyond the scope of the well-defined, narrow task ascribed to notaries.

 When you walk up to a notary, provide id for someone named X and make a statement S, and the notary issues you a certificate, that certificate does not say "X said S." Instead, it says "The notary says that someone presented two forms of ID bearing the name X, and made the statement S." The difference is subtle but critical. The first one requires that the notary establish true identity, a task that is impossible to do with total accuracy. The second one is much better-defined, 100% correct and defensible in court, and therefore it is much more useful in practice. We have all sorts of other mechanisms for establishing absolute truth; a virtual notary's statements serve as input to those processes. So, all statements made by Virtual Notary explicitly state the attestation, and point to the true authority on the topic. For instance, the exchange rates factoid says "According to Yahoo Finance Data, the exchange rate from Rimnimbi to USD was X," all entirely checkable statements.

 Note that the world may look different from the notary's vantage point. So a web site might be served differently to a notary than to you, in which case you may have to resort to other mechanisms to establish its content.

 Implementation
 For the Virtual Notary's certificates to be useful, they need to be trustworthy. And we've designed the service to be resilient, compartmentalized and failure-isolated in the presence of attacks. In fact, the entire design is structured such that even a complete break-in of the Notary service confers no ability to attackers to affect statements issued in the past.

 There are some neat tricks in the implementation that ensure this security property.

 First of all, all statements made by the Virtual Notary form a hash chain. Every single certificate issued by the notary contains a serial number derived by hashing all past certificates. Thus, someone who breaks in and acquires the notary's keys (akin to a seal in real life) cannot go back in history and issue a backdated certificate, as there would be a discrepancy in the hashchain.

 Second, any discrepancies in the hashchain would be immediately obvious and public. Virtual Notary tweets every single new addition to the hash chain. Further, twice a day, a transaction for a very small amount of money is recorded in the public Bitcoin transaction ledger, along with the state of the Virtual Notary public. This ensures that the state of the notary cannot be unwound or rolled back.

 Finally, Virtual Notary keeps the master key for the notary off-site, and uses temporary signing keys that are routinely replaced for signatures. This ensures that the effects of an attacker who has compromised the site can be bounded in time once detected, as the attackers cannot forge new keys without the master key.

 Internally, the system is modular and composed of various Python plugins for determining and attesting to different kinds of factoids. This makes it easy to extend it to attest to new kinds of online factoids as there is demand and as people develop plugins for extracting those factoids.

 The certificates issued by Virtual Notary are X.509 attribute certificates. The X.509 standards are open. The certificates can be parsed by anyone with a standards-compliant parser, even if, for some reason, the virtual notary service were to be shut down. The attested certificates are available on the web at a private URL, so you can easily pass them around. Optionally, one can download the X.509 certificates for posterity and fault-tolerance.

 Overall, Virtual Notary issues certificates that possess a level of inherent trustworthiness, verifiability and technical defensibility. We hope that they are useful for your needs.

 What Next?
 There are lots of interesting new services possible once people are equipped with the ability to pass attribute certificates. At the moment, online services are based either on direct evidence, where a website itself checks a fact (a laborious process that involves some work at least on behalf of the website, and perhaps yours -- you may be familiar with the various email account activations messages, for instance), or else they were based solely on hearsay (and sometimes, for instance when entering email addresses on web forms, the website makes you say the same thing twice just to make sure that it has a chance of being correct). Attribute certificates can obviate much of this and make the online experience much better. Imagine, for instance, that a user furnishes a certificate to show that they are over 18, but does not have to reveal her identity to anyone. Such abilities have far-reaching ramifications, and Virtual Notary is a first step in tackling some of these questions that happens to also use some cool technology under the covers.

 The Virtual Notary service is currently in alpha. That means that there are many rough edges, but the general sketch of a useful service is taking shape and it may be useful to some people as is. So we're making the service public, and would like to enlist your help. This help can come in several different forms:

 We'd be grateful to any and all early adopters who use the system and provide feedback and suggestions.
 If you have suggestions for new online factoids that are of direct relevance to you and your company, please get in touch. We're looking to expand the universe of statements that Virtual Notary can issue.
 If you notice bugs or rough edges, please get in touch. We're striving to improve the current factoid plugins and make them more robust.
 If you would like to contribute new plugins, and are familiar with Python/Django, please get in touch.
 If you are interested in the legal aspects of online notaries, have the requisite legal background, and would like to help make the current legal landscape more friendly for modern ways of attesting to online facts, please get in touch.
 If you find some aspect of the system useful and would like to contribute, you can donate Bitcoin through the link below. We use 0.01 bitcoins per day to record the state of the notary.
 The Virtual Notary service will remain a free service for the foreseeable future. And I hope it will grow, with community support. Many thanks in advance for your hand in making it better.

 - See more at: http://hackingdistributed.com/2013/06/20/virtual-notary-intro/#sthash.SzgHkLaX.dpuf


 Looks like my post on how MongoDB is broken by design got a response from 10gen PR. It's an illustrative example of how corporate PR can let down regular developers.

 First, the 10gen spokesperson seems to have read only the H2 elements on my original writeup -- he seems to not have read or understood the actual text that goes along with them. Do they have some local CSS applied that makes regular text invisible?

 I had a few words to say about this TL;DR-culture at the end of my post. It's sad when developers can't be bothered to read things, but that's at some level, understandable -- we're all pressed for time (e.g. I'm writing this note in an airport lounge while traveling). But if you're the spokesperson at a company, and someone does a careful analysis of your flagship product and says "hey guys, looks like you goofed up," you probably should read the technical reasoning, no?

 This is not a game of making correct-in-a-narrow-sense but misleading statements. The developer community will not have any sympathy for you if they trust your product and later find that their trust was misplaced. My connection is too flaky to check the proggit and HN discussion, but I suspect the dev community will not be kind.

 Here's my quick pass:

 Issue #1: I said that MongoDB v2.0 lies. My statement included the version number in it. I then described how the fixes to the defaults issued in v2.2 are insufficient. The response pretends that 5 years of brokenness never took place, and it does not address the concern that the fix is insufficient in any shape or form. With the old lying Mongo, a single client crash could lead to data loss. With the new much-improved Mongo, a single server crash can lead to data loss. Just because they get a confirmation from a single server does not mean that the data has been recorded anywhere except in the volatile memory of a single server. Even if it were recorded on disk, the fault-tolerance guarantee would not change at all (though the nature of the fault would change, from node failure to disk failure -- perhaps Mongo assumes that disks don't fail?). How many faults does it take to lose data when all you have is a single copy on a single host?

 This is not what normal people mean by fault-tolerant.

 Issue #2: 10gen misses the point:

 Intuitively, waiting for an operation to complete on the server is
 slower than not waiting for it.
 There is some unavoidable waiting to be done to get the data to be committed. But getLastError incurs an additional latency and overhead for the client to communicate its desire to wait for that commit. And the response is clearly failing to account for the difference. It's as if there is no difference between the two. MongoDB is evidently building its systems as if networks have 0 latency, network bandwidth is a free commodity, and NICs have no overheads.

 Pact with the devil.
 When making a pact with the devil, be sure to draw up a bullet-proof contract. For he may take your integrity and not give the "speed" you seek.

 That's fine by me. I've gotten countless questions on "what makes HyperDex so fast? what's your secret sauce?" We've detailed some of the secret sauce, but people still find it hard to believe that HyperDex can be so fast (3000 ops/sec; for comparison, the carefully designed Cassandra code gets 1500 ops/sec) and so much faster than MongoDB (at 6 ops/sec) that the latter is unable to finish a benchmark. I hope this demonstrates the difference between keeping performance tradeoffs front and center throughout your design and implementation, and acting like physics don't apply to you.

 Anyhow, if they're happy with their performance, who am I to complain? It's weird, though, to give up consistency and fault-tolerance for performance, but then to fail at achieving performance as well. If you're going to sell your soul and your data integrity to the devil in exchange for speed, well, make sure he delivers, or else it'll look really bad.

 Issue #3: 10gen confirms that getLastError does not work when pipelined. Anyone can see through the attempt to spin the bug as a feature.

 I agree with them that not every application needs every write confirmed. I covered this in my article on when data is worthless. It's a slippery slope, one that I suspect most developers will find difficult to navigate.

 I note with concern that the pattern recommended by 10gen here is broken. If you pipeline 10000 inserts, call getLastError, and see success, you cannot count on 9999 of those inserts as having been committed without errors. It's a sharded data store. The previous operations may have hashed onto any set of servers, and some of those servers may well fail or have failed. The success message you see about the last write implies nothing about the success of the preceding requests.

 Issue #4: MongoDB forums and blog posts are full of messages about how one can use getLastError to check on the outcome of the last operation. I pointed out that if you follow their advice and write code that looks like this:

 db.insert(...);
 DBObject err = db.getLastError();
 then your code may very well see the result of a completely different operation that was last performed on that connection by another thread.

 Here's the MongoDB documentation that confirms this (skip the first sentence) [1].

 The Java MongoDB driver is thread safe. If you are using in a web
 serving environment, for example, you should create a single Mongo
 instance, and you can use it in every request. The Mongo object
 maintains an internal pool of connections to the database (default
 pool size of 10). For every request to the DB (find, insert, etc)
 the Java thread will obtain a connection from the pool, execute the
 operation, and release the connection. This means the connection
 (socket) used may be different each time.
 Let's get back to that first sentence and use this opportunity to talk about "thread-safe" vs "system-safe." The Mongo driver may indeed be thread-safe, in that it uses locks correctly internally and maintains its own invariants for its own correct operation. But the programs that use it are a different matter. I can build a component that is internally thread-safe, and yet its API makes it difficult or impossible for threaded programs to maintain their own correctness invariants.

 Until recently, MongoDB did not talk about requestStart() and requestDone() in any context except when talking about how to ensure a very weak consistency requirement. Namely, if you don't use this pair of operations, then a write to the database followed by a read from the database, by the same client, can return old values. So, I write 42 for key k with a WriteConcern.SAFE, read key k, and get some other number, because the Mongo driver can, by default, very well send the first request to one node over one connection, and the second one to another, over another connection. So requestStart() and requestDone() were billed as a mechanism to avoid that scenario; I saw no mention that they were required for correctness in multithreaded settings. I bet there is plenty of multithreaded code that does not follow that pattern. Such code is broken; if you're a Mongo user, it'd be a good idea to check if you ever use getLastError without a bracketing requestStart() and Done().

 Issue #5: What a non-committal answer. What's the Mongo setting for "I don't want you to lose my data?" I'll even specify a very benign fault model: my cluster can have at most one fault at a time. I've paid my tithing and sacrificed all the right kinds of animals to various deities, so I've made sure that a second fault will not occur before my devops team fixes the first fault. What Mongo setting will ensure that I don't lose data?

 I already described how all but one of the SAFE settings that Mongo provides are broken, and described how they cannot withstand a single fault. True, I did not yet describe how REPLICA_SAFE is broken. Yes, yes, I know, how dare I write a huge post cataloging a number of bona fide errors that half the Mongo fans, including the very fellow whose job is to respond to the post, did not read beyond the H2 tags, and leave this part to a future post? It's partly because I actually have a full time job and life, but it's mainly because I wanted Mongo to come out and say "REPLICA_SAFE provides a fault-tolerance guarantee." They are steadfastly refusing. Read this fellow's response carefully:

 WriteConcerns provide a flexible toolset for controlling the
 durability of write operations applied to the database. You can choose
 the level of durability you want for individual operations, balanced
 with the performance of those operations. With the power to specify
 exactly what you want comes the responsibility to understand exactly
 what it is you want out of the database.
 This says "we give you what you get." I guess if the system loses data, it's always the developer's fault for not understanding the internal workings of Mongo.

 - See more at: http://hackingdistributed.com/2013/02/07/10gen-response/#sthash.MeBWukxL.dpuf


 I gather that Google is planning to cancel Google Reader. I also gather that this is, "literally," the end of the Internet for some people.

 There have been far too many HN articles on how evil Google is for canceling a free service, how this product cancellation is a sign of the end times, how Google should never have given anything to anyone if they were going to cancel it later. There are blog posts that explicitly say, and I quote, "we've entered a darker timeline in the history of the net". There was even a White House petition, which Obama's people thankfully deleted before it got too embarrassing.

 Since the "H" in "HN" stands for Hacker, and since a sizable fraction of the people who are up in arms about Google Reader have Twitter profiles that say hacker (and we all know, even before reading their blog posts, that every single one of these people do indeed have Twitter profiles, and they are entrepreneurs also, and they're thinking about their next pivot, and they certainly have an opinion on NodeJS versus Clojure even though they've never written code on either platform, and yeah, they're attached to their Twitter feed but dontcha-know-it's-just-not-the-same-thing as Google Reader and now give me what is rightfully mine dammit, I used your service and you owe me in perpetuity!), let me remind people what the H-word used to stand for.

 Old School vs. Non-Coding Hackers
 A hacker would not whine about missing code. Hackers see missing code as an opportunity to build. /EOT

 Reader is Replaceable
 It's not like Google Reader was a particularly inspired service that's difficult to replicate. It's certainly a nice service, especially because it kept track of what you had seen across multiple platforms, but it'd be a reach to claim that there is any technical impediment for why someone else could not make an exact clone, minus the trademarked logos. As far as I can tell, Reader takes advantage of none of the proprietary machine learning techniques that are internal to Google. And it takes little advantage of Google's scale besides amortizing update detection across multiple users. Anyone with a web framework and a NoSQL data store has the exact same technology that powered Reader.

 The area is probably not encumbered by Google's patents, and even if it were, Google is not going to initiate patent wars on small players in a niche they have willfully abandoned.

 And it's not like Google is keeping anyone's subscriptions hostage. They provided a way for all Reader users to get their data out of their system. Unlike some other companies I can name without thinking, they have been particularly open about letting their users take their data out of their systems.

 So, the only thing in between a hacker who misses Reader and a working Reader implementation past-June is... nothing. Nothing, that is, besides these so-called-hackers' decision to write blog posts instead of code.

 Impact on RSS
 Reader's cancellation will have absolutely no impact on RSS. Reader isn't RSS. It isn't anything but a consumer, one of many, of the RSS standard. There will be alternative RSS readers.

 If and when RSS dies, it'll die because we will have techniques that can extract the same information just as efficiently, not because any single RSS reader died off. And in case you think it's some random professor saying this, do keep in mind that I published the first study of RSS feeds and people's RSS usage patterns, and discovered many of the Zipf popularity distributions in RSS feed usage. Those same patterns of popularity make it highly efficient to provide Reader-like monitoring services. The costs are very small, and there are tangible benefits, so undoubtedly others will step into Google's place.

 Impact on Competitors
 Some have claimed that Google's great sin was to offer a free, subsidized service, thereby killing the competition. There is a nugget of truth here. My research group had an RSS reader at the time, called CorONA, which was essentially a peer-to-peer AIM-bot that messaged you whenever new content appeared on any of your feeds. And we watched people go over to Reader. So there is a twinge of irony here in that Twitter is being billed as the new mode of content discovery, but you know what? Google didn't start out with an evil agenda to embrace a market and then make it collapse. They just competed with everyone else, fairly, in a world where every competitor was just one click away. They offered a genuinely good service, as everyone who is kvetching about Reader will readily admit. And 8 years later, they have decided that their effort is better invested elsewhere. This is what it means to be a free agent.

 Reader as a Stand-In for Cloud-based Services
 Stallman and others have used Reader's demise to make the only worthwhile point that is worth making; namely, free cloud services aren't quite free. A decision to rely on such a service should not be made lightly; whoever is in possession of your RSS feeds probably knows you more intimately than Target, which supposedly knows when a woman is pregnant before her father. It's an intimate relationship, based on trust.

 We should be up in arms if that trust were to be violated, say, by selling this information to third-parties. But canceling a service with advanced notice is by no means a trust violation. I can't fault FSF for using this opportunity to remind people what it means to use a cloud-hosted service, and I agree with their point wholeheartedly, but the people outraged at Reader, the self-titled technocrati, knew exactly what they were getting.

 Reader As a Symbol and Shibboleth
 Arnold tells people to stop whining
 Arnold puts it well.

 So what explains the current outpouring of love, hatred, derision, envy and everything in between for Google Reader? Well, that's the one thing that the bloggers got right: Reader has become a symbol. Not a symbol for "Google has turned evil" as many have claimed. Not a symbol for "Google is abandoning open standards." And not a symbol for darker times for the Internet.

 Reader has instead become a symbol for "I, too, am an information junkie, a member of the digital cognoscenti, with demanding needs." So the discussion is not about Google Reader, not at all. It's about the so-called hacker-turned-entrepreneur-turned-info-junkies. It's about labeling oneself as part of the technocrati. Here is a direct quote that reveals how these people want to be perceived:

 Only a relatively small fraction of the population, mostly highly intelligent, and well informed, curious individuals used Reader *
 It's Lake Wobegon and all you need to enter is a Reader account and some outrage.

 So we learn that anyone who was an avid Reader fan is automatically above average. Not just that, but the ones speaking up with an extra serving of outrage must be above average among the above average crowd, something that our current science of statistics cannot even express. Many people spend their lives trying to make their mark by advancing science, technology, the arts or the human condition at large. Meanwhile, it turns out that anyone could have demonstrated the same just by signing up at a web service. Quick, let's all express our outrage lest there be some doubt about our above-averageness! Never mind the fact that the task at hand is not much different from watching TV.

 This preoccupation with how one is perceived is known as a "narcissistic tendency." And it's perfectly fine insofar as it doesn't interfere with one's perception of reality and normal behavior. If someone really thinks a cancelled product for which there are many alternatives is the end of what they're calling "the net," or if a self-proclaimed hacker views Google abandoning an area as anything but a huge opportunity, they need to see a professional.

 And in the same way that individuals are using their outrage about Reader to market themselves, companies are doing the same. Digg, a company whose existence I had practically forgotten about, jumped into the fray with a claim that they'd replace Reader. It's only a matter of time before Yahoo does the same. Do you want to know how hackers feel about people who announce software projects way before there is any code at all? No you don't, this is a family-friendly blog.

 Looking Forward
 All of these are, I guess, annoying but benign behaviors, as long as we recognize them for what they are. But I want to raise three parting thoughts.

 First, I worry that our values are so far out of touch with what is normal that we can read "give me back my Reader" posts without cringing. It's not ok to demand someone else's services. It's not ok to claim other people's time and effort. It's not ok to act or feel entitled. And it's not ok to upvote someone else's tamper tantrum. It's true that there are tacit societal agreements that go beyond the EULA. The users demand and deserve respect because they have given their time to a service provider. But in this case, the relationship is entirely one-sided. These users did no special service for Google, and Google's response, to give them enough time to switch, to get their data out, is par for the course. Entitlement without effort is like representation without taxation.

 Second, I suspect that the Reader-loving non-coding-hacker crowd, which is deeply insecure about its position in society, is perhaps so sensitive because they see this as a sign that Google, and by proxy, the computer industry, values the masses with their dirty mobile devices and effervescent Twitter feeds more than the self-titled "digital cognoscenti" with their MacBooks and Reader accounts and IFTTT recipes that save noteworthy articles in Evernote. They want to flex their digital muscle. And to some extent, I sympathize with this. But let's call a spade a spade, and let's pick something more worthy over which to flex our muscle. My greatest fear is what would happen if Google were to go back on their decision. It will feed the crazed, self-important "technocrati" so much that we'll be deluged with overblown outrage at every perceived online slight. We know how 2 year olds turn out when they get their way with tamper tantrums.

 Finally, wouldn't it be much better to channel all this energy towards something more productive? Like, say, writing a few thousand lines of code in Rails or Django or Node JS or Clojure or whatever other language to implement a replacement, either from scratch or by contributing to an existing OSS project. It's a big world out there, we have not entered a darker timeline in the history of the net, and it's always time to make it better, not demand that someone else do that for us.

 --

 HN discussion. This post challenges established HN group-think, so expect some vitriol from non-coding hackers. If someone really wants to petition the White House, it bolsters the "this is not at all about Reader, it's all about the need to be perceived as cutting-edge consumers" theory. One does not need to be a constitutional scholar to see that such a petition is futile, for Obama cannot bring Reader back, but it's perfect for putting one's name on a list of Lake Wobegon dwellers. And that's kind of the point.
 Eric Raymond on the hacker attitude.
 Edited:

 Mainstream writers have started to weigh in on this issue. What strikes me is that every single one of these articles reflect the narcissistic image-building comment I excerpted above about how Reader users occupied Lake Wobegon, an enclave where there is no such thing as the average person. See if you can read these without cringing (especially if you also have a Google Reader account):

 Google's Google problem: "GOOGLE is killing Google Reader. That may not matter much to many of you; use of Google Reader was concentrated among a small group of relatively intense users."
 Googles trust problem: "Now, most people dont use Google Reader, or even know its being canceled.... Most people, however, also arent the sort of early adopters who will rush to download Google Keep. But Fallows is that kind of early adopter. So am I."
 When a news article (ab)uses the the first-person singular pronoun, it's clear that we've left the core topic behind for other, self-oriented pastures. It's not about Google Reader anymore, it's about how the Ezra Klein's of the world are so much different from, and so much better than, everyone else
 - See more at: http://hackingdistributed.com/2013/03/17/google-reader/#sthash.3rg97XIk.dpuf

 In the beginning, there was no documentation. These days, infrastructure software has terrible documentation. Take heed of these commandments, for, among hackers, judgment day is every day.

 There is a distributed systems revolution underway. Cloud computing and mobile systems are now ubiquitous, and web services are exploding (BTW, I firmly believe that we have barely scratched the surface. There are so many more exciting services to build that we will see decades of growth and excitement in computer science). This trend has not only turned almost every regular developer into a distributed systems developer, but also pointed out just how little infrastructure we currently have for building distributed systems. As a result, the last decade of systems research as well as industry development has been marked by a worldwide effort to come up with frameworks, components, and other underlying systems infrastructure. Such systems infrastructure software includes web frameworks, NoSQL systems, databases, recommendation engines, lock managers, replication packages, analytics engines, machine learning packages, and so forth. All of this infrastructure needs to be documented in order to be useful. This post is about how to document such systems software.

 Documenting systems software is qualitatively different from documenting consumer software. Whereas consumer software documentation needs to be task-oriented, documentation for systems software needs to be contract-oriented. If you're a developer or user of systems software, this post provides some principles that might help you write, and demand, better documentation.

 This is not a post about how (not) to write internal documentation, e.g. of the form:

 ++i; /* add one to i */
 Internal documentation, whether it is in comments embedded in code or in white papers, is for your own consumption. Make it as terse or as verbose as you like; it's your own business. I do have strong opinions on how it needs to be written for my own projects, but they're beyond scope here. This manifesto is about user manuals and web pages that are released along with systems software, that is, companion text to code intended for public consumption. Having read a lot of such text in the NoSQL and database space, I want to call for better practices across the industry.

 Why is Documenting Systems Software Different?
 Documenting systems software is qualitatively different from documenting other kinds of consumer software because the users have a different kind of relationship with the software. With consumer software, the target is the end-user who will carry out a task, and the documentation has to center around questions of the form "how do I use this software to do something?" We've all wondered about how to use Photoshop for red-eye reduction (answer: there are more than a dozen ways, but they either produce terrible results or require you to figure out alpha channels and color theory), how to use Office to generate form letters (answer: I hate to admit that I once had to do this and carried it out with Clippy's help but cannot any longer), or how to use GIMP for, well, anything (answer: it's just like Photoshop and insanely more difficult). The central point here is that consumer documentation needs to be mechanistic and illustrative, because the target audience is task oriented.

 In contrast, the target of documentation for systems infrastructure is another developer. Systems documentation is there to help facilitate the development of other software. To do that effectively, there needs to be a contract between the developer, who is using the system, and the system, which is providing the functionality that the developer needs. The documentation serves as that contract. It needs to be clear, declarative and comprehensive. This article will make the case that writing such a contract should be treated as writing a software specification.

 Breaking with the Orthodoxy
 Holy Book.
 Time for holy documentation.

 What I've enunciated so far is, from an academic standpoint, a very orthodox approach. Since the '60s, academics have argued for formal specifications for software. Such specs would be fantastic: they would take out any and all ambiguity of English; enable one to check the implementation against its spec, eliminating all bugs save for the ones in specs themselves; facilitate reasoning about the composition of different modules; among others.

 But it turns out that writing formal specifications for software is such a difficult religion to follow that few academics follow the orthodox teachings themselves. Instead, all but a few dedicated academics appear at the high altar of formal specification only for occasional talks and specially crafted (and, invariably, very short) examples. Most academic software is documented in exactly the same manner that most industry software is documented: ad hoc English text.

 I, too, will deliberately break with the orthodox view: I will not issue an idealistic but non-realizable call for formal specifications. The orthodox religion is too difficult to follow for all but the most dedicated. And sadly, there is little in between the orthodox approach and a life of hedonistic, heathen practices where pretty much anything goes as far as documentation.

 In this manifesto, I want to enunciate some principles for structuring that ad hoc English text so it can help facilitate more robust systems. I realize the religious nature of this discussion, so I'll try to remain true to form and plagiarize heavily from previous books that successfully documented the human-heavens API in the past.

 Principled Documentation: A New Religion
 Because every breakaway movement needs a clear identifying name, we shall call our splinter movement Principled Documentation. Chances are, if you're reading this, you already are a disciple; there are many who practice it without realizing it. If you don't, I hope you'll convert and zealously preach that others do the same. The goal of this post is to enunciate our principles clearly so others can follow them. For no splinter religion would be worth its salt if it did not try to convert everyone else on earth.

 Fair Warning
 If you think you and you alone (or people like you) own certain words, phrases, references, or sentence structure because of your belief system, here's a fair warning: while I am sure that there is absolutely nothing here that is blasphemous, objectionable to a hacker, or offensive to a thinking individual, you should probably read something else.

 Tenets of Principled Documentation
 The central commandments of the Principled Documentation movement are as follows:


 Say what you will, he knows his audience.
 There is only one audience and their name is Developers.

 It's critical to choose your audience with care and to write solely for that audience. The developers who will be using your system are drastically different from just about everyone else, especially the people in suits who make purchasing decisions. You can have as much marketing material as your budget permits, but you should not confuse such material with system documentation, or vice versa. Mixing the two heralds the end times, like breaking down the barriers that hold back Yajuj and Majuj, Gog and Magog.

 Thou shalt view thy documentation as a specification instead of a description.

 The documentation forms the contract between a system and its users. Like any good contract, system documentation must specify what both parties expect from each other and can count on in return. This requires describing what software needs, what it does, and what it provides -- the three angels that fit on the head of a pin, assuming that the pin also holds some RAM, for they typically need to appear in a PDF file or web server. A holy book that simply provides a description of, say, the various machinations among the gods on Mt. Olympus, is of little utility. The successful books have a prescriptive specification for what their users ought to do. Phrases such as "thou shalt," "thou shalt not," "we shall provide," "we guarantee" and "surely, you can count on" are part and parcel, not just of a religious manifesto such as this one, but every holy book.

 Thou shalt specify thy assumptions.

 First, the docs must specify all the contractual obligations that the users must fulfill to use the system correctly. That is, all assumptions of the system about the inputs must appear in the documentation. Does an input array need to be sorted? Must a netmask parameter be large enough to accommodate the number of hosts passed in another parameter? Should the caller be holding a special lock when calling that routine? Perhaps a call to a function needs to be bracketed in some special way or else it will never work and all the multithreaded code in the world that uses it will be broken? The documents must enunciate these preconditions. If it's not in the docs, the system developers do not get to blame their users.

 Thou shalt specify thy functionality.

 Second, the docs need to spell out the functionality that the code carries out. This typically comes naturally to most seasoned programmers. For instance, this is a good specification: "given an unsorted array, this function returns the median." So is "this function will issue an eventual update to the database." A good rule of thumb is that every such specification should have a verb in it.

 Strange as it may seem, it is possible to find documentation, issued by large companies even, where the API description gets hung up on all kinds of irrelevant details without concisely specifying what it is that the API does. This kind of oversight marks the footsteps of Beelzebub, for how can the code be correct if there is no indication as to what it does? Worse, how can it even be incorrect?

 If the documentation does not actually say "here is what this function promises to do," then it could be replaced with a NOP, or the universal answer 42. In fact, there is plenty of NoSQL documentation out there that is so non-specific that it does not say what the "read data from database" function will or will not return. This is a clear sign that at least the docs, and perhaps the project, have failed. It's just a matter of time before a bright developer, having improved benchmark scores by considering a write finished before it has been written anywhere, will want to improve the read path by returning 42. Such a database is, after all, eventually consistent.

 Adam and Eve.
 You had one job, one invariant: "Do not touch the apple, and you stay in Eden." But it wasn't written down, and we all know how that turned out.

 Thou shalt honor thy invariants.

 Finally, the docs need to specify the guarantees the code provides and the invariants it upholds. This clearly telegraphs to the developer additional facts about the universe she can count on. So if your code always ensures that, say, there will be at most N outstanding operations written to a journal, or that journalled operations will spend at most tau seconds before being committed to the main store, the documents need to spell this out (I have taken it as a given here that the concept of a journal is something your system must expose to the user; its exposition actually violates some of the other commandments). Undoubtedly, the code went to great lengths to maintain that invariant. Incorporate it into the contract to make it useful. Say it out loud and proud, then make sure your code lives up to it. Perhaps one can build additional functionality, such as a consistent backup mechanism, based on this invariant.

 Conversely, if the docs leave out an invariant, clear-thinking users must assume that the software will fail at maintaining that invariant, probably at the most precarious moment when the going gets tough. If an invariant is not in the contract, it's just happenstance. Only a fool would count on it.

 What is unacceptable is to imply an invariant without actually having the conviction to say so openly. Does a database ensure that an object is resilient against F faults? If so, the documentation needs to clearly say this aloud. For instance, no one knows if some very popular NoSQL stores actually offer a fault-tolerance guarantee. Certainly, there is no shortage of implications that they do, and certainly, all the language points to it, but the documentation is often, at best, mercurial.

 Being clear about your invariants in your documentation has the crucial side-effect of keeping them front and center in the developers' minds, so later modifications to the code base will continue to maintain the same invariants. Documentation that implies fancy, desirable features without actually clearly offering them bears the footprints of the great shapeshifter Loki, and Ragnark is just around the corner when Loki has been unchained.

 Thou shalt write complete, self-contained documentation.

 The docs constitute the sole and full agreement with the user. They cannot assume that the user has read or will read any and every blog entry, press release and tweet issued by a company. They most certainly cannot depend on information in transient, ephemeral media. Docs are like the newspaper of record: everyone may well have watched OJ slowly run away in a white Bronco on TV, but the documentation needs to clearly describe what happened for posterity.

 But it is possible for documentation to incorporate other documents by reference, same as with contracts. So long as these documents are as permanent, uniquely identifiable, and referable as the documentation itself, this is a fine practice. It's perfectly acceptable to say that a compiler implements the ANSI C standard or that a database implements a particular SQL variant, with a reference to the appropriate text. Note the insistence on unambiguous, specific references. What documentation cannot do is incorporate vague notions or collections of utterings from corporate twitter feeds, or else the trickster Nanabush will sneak into applications.

 Dante's Inferno.
 Layered OSI model of hell. The data link layer, with Lucifer, is reserved for mechanistic descriptions.

 Thou shalt cover thy naked mechanistic descriptions.

 It is a cardinal sin to describe the behavior of a complex piece of software in lieu of its contract. It is a dishonest cop out to explain, even in great detail or especially in great detail, the complex machinations the system performs with data, and then to say "well, we described what we do in great detail, and if you got caught by an edge case and lost data, well, it's your fault for not reading our description carefully enough." This is at best lazy, some might say duplicitous, but in any case not the way smart people conduct their affairs. Dante undoubtedly has a special level of hell dedicated to people who practice it. A plumber who fixes a tap is legally obligated to not leave that house with a leak; he doesn't get to say "well, my routine involves spinning the nut 5 turns no matter what and if it leaks after that, it's your problem." Similarly, a software engineer cannot pre-issue a disclaimer that goes: "well, I developed a weird interpretive dance routine (European high art, NSFW) for your data that looks like this," and thereafter make fun of you when he loses data, saying "it's your own fault! it's not like I geared up to guard Ft. Knox! I was skateboarding in a loincloth all along, and yet you entrusted me with your data." Our current laws do not compel the software engineer to behave better, but engineering pride must.

 There are good reasons why smart developers will avoid mechanistic descriptions. The users adopt a component because they do not want to understand the complexities that go into its internals. Butler Lampson observed in the mid-90's, when Sun was making a lot of noise about how Java would herald in a new age of reusable applets and microcomponents, that the metric that determined software reuse was the complexity of the component divided by the complexity of its documentation. So, components that perform complex tasks behind a relatively simple interface provide better value to their adopters than small components with complex descriptions. He boldly predicted that Sun's component reuse model would fail. There is some information in the fact that no Java Beans were involved in serving this page, or, roughly speaking, any other page on earth.

 So if a system forces its users to have to understand its internals before it can be used competently, it is in a losing predicament. Let's do an optometrical test. Is the documentation more useful like this:

 This code goes through an array pair by adjacent pair, compares each,
 and swaps the pairs where the latter element is larger than
 the former element, repeating the process until no swaps are
 performed on a complete pass through the array.
 or this:

 Returns a sorted array.
 And did you catch the bug in the mechanistic description? Just imagine how much worse the description would have been, and how many little things could have gone wrong, if the implementation had been based on quicksort.

 Now imagine a NoSQL store trying to maintain consistency and fault-tolerance invariants, which it cannot even articulate concisely, by describing its own operation from the ground up. Shouldn't it just offer an API, perhaps named safe_write, whose description is that data written with it can be retrieved after a failure? Of course, the tricky extra credit question for advanced players is: what if the same data store provides 5 different kinds of SAFE writes, each with a complex mechanistic description -- which one is the proper safe write you want when you don't want your data lost? Every time a user reads the manual for a NoSQL system and is forced to go through a gory description of various-things-they-do-to-your-data-which-you-must-master-or-else-they-will-lose-your-data-haha-just-kidding-they-are-going-to-lose-your-data-anyway-even-after-you-mastered-their-inner-workings-because-they're-broken-by-design, she thinks "Cool story, bro, what with all those implementation details and everything. Now give me an actionable guarantee."

 A side effect of principled documentation is that it creates manoeuvering room to change the implementation later. You want quick sort instead of bubble sort? No problem. The documentation doesn't need to change.

 Yamantaka of Tibetan Buddhism.
 Yamantaka gets upset if you make up new terms for old concepts.

 Thou shalt use existing, well-known terminology and concepts wherever possible.

 It's not easy expressing all the invariants that complex software maintains. If I had to describe everything that HyperDex does to ensure fault-tolerance in a mechanistic fashion, it would easily take dozens of pages of English text. But luckily, we do not exist in a vacuum -- we all have a shared geeky background, with well-defined terms that are so well-known that even academics have gotten to know them intimately and teach them to new generations of programmers.

 So, documentation must take advantage of this common heritage by using concise, well-understood terminology. Thanks to the work of dozens of database and systems researchers, I can specify HyperDex's guarantees for NoSQL transactions very simply, like this: it provides one-copy serializability and ACID guarantees for transactions. There, anyone who is familiar with the area can immediately build services on top, without me having to describe, and them having to master, anything about the internal implementation. An inability to use existing terms for existing concepts demonstrates that the developers are not familiar with the topic, that what is elementary to others was new material to them, which is fine, but it often indicates that they may not have thought about all the well-known edge cases. As a certain mystic poet put it, they are like fish in water, who know not of the sea.

 A corollary to this is that documentation should not invent new terms unless necessary to clearly mark, and refer back, to new concepts introduced uniquely in that system. Minor tweaks to known concepts, or combinations of two well-known concepts, do not get new names. For instance, no one, except perhaps Dilbert's boss, is impressed by the mumbo jumbo names NoSQL developers have invented for age old concepts such as delayed writes, quorums and gossip protocols. We're not at the weekend SCA meeting, these devs are not Prince Bromir, son of Nohsiquel, and they do not need to sound medieval or learned. My anthro colleagues would call the made-up 4-word dual-hyphenated special phrases "in-group identification" and point out how it's primarily used for exclusion. He who controls the language frames the debate, and keeps out critics who use standard terms everyone can understand. And sometimes, groups of developers will develop their own language, repeating each others' non-sensical words in the blogosphere, like unsupervised twins inventing their own lingo. "Water," the mother points out. But the twins spend much more time with each other reinforcing their words than they do with the mother. "Gaga" twin #1 calls it. "Gaga" twin #2 repeats. So gaga it is. Or perhaps it is "Mongo.WriteConcern," and no one can remember if the original concept was "fault-tolerance" or "durability" or "consistence."

 Seven Sins.
 There is a lost panel, and the eighth cardinal sin, reserved for bad documentation writers.

 Thou shalt not covet existing terminology and assign it new meaning.

 A related cardinal sin in this space is to co-opt existing terms, but to slightly re-define them. The more subtle the redefinition, the more possessed the documentation.

 I once reviewed an academic paper which was evaluating a particular approach to routing and wanted to compare their routes to the optimal path possible. Optimal paths are NP-hard to compute, so they didn't compute them; the authors picked a good path using a simple heuristic and called it OPTIMAL throughout their write up. The peer review process pointed this out, and instead of fixing this oversight, the authors just dropped a sentence into the introduction of their paper that says "In the remainder of this paper, the word optimal refers to heuristically determined paths, which may not necessarily be optimal." Their conclusions section argued that their idea was successful because the "system is within a few percent of optimal." Science is hard when words don't have any meaning.

 That's why people get very upset when system documentation redefines terms. This is especially true for words like "consistency" which have both a colloquial usage and a technical definition. System documentation should be written for a technical audience; any mixing and matching, especially when it can lead to misunderstandings, is unacceptable. A document that uses technical terms with their colloquial meanings is like a medieval leprosy patient mixing in with the general population; that document needs to shout UNCLEAN every few sentences. Such usage must be avoided at all costs, and, ever the optimist, I will ignore the possibility that the documents are purposefully written to mislead the audience, for surely those authors are destined for hellfire.

 When someone uses pre-defined words, they implicitly import their connotation. For instance, the term "ACID transactions" has a very specific and well-defined connotation. And data stores that perform transactions on at most one object at a time come nowhere near that connotation. Technical terms mean something to people who know the area. The people coopting a phrase might perhaps not know the area, and that's totally ok, but they sure as hell know and are taking advantage of the positive connotation the word carries. Such behavior dumbs down the entire industry; bad science drives away good science, it also drives away the smart developers, and it even drives away business. And this kind of behavior will earn a well-deserved slap on the wrist. Why? Because the Followers of Principled Documentation are too nice to cut off hands.

 Thou shalt specify performance expectations whenever possible.

 Performance properties often get short shrift. It often matters to the user of an interface whether they can expect an operation to take a while, or whether it'll be fast. So it's invaluable to document such expectations.

 One of my favorite theses of all time is Sharon Perl's PhD thesis from MIT. She built a system by which developers not only express their expectations of the performance of different API calls, but can convert that specification into performance unit tests.

 Sadly, I know of no such open source tools available at this time, and expressing performance properties formally is still quite difficult. But simple English specifications of "this operation takes time linear in X" or "this operation may block until Y is completed" go a long way in letting developers use an API in an informed manner. The state of the art is so backwards at the moment that many developers will happily take, and thank you for, imprecise specifications of the expected case. Worst-case bounds are not necessary, though, of course, it helps if the system can provide them.

 A few fully-defined parables are as good as many small detailed microspecifications.

 There are two main ways of writing API specifications.

 The first approach is to go interface by interface, call by call, and specify at the micro level the assumptions, functionality and the upheld invariants of each function. This provides a micro-level, bottom up description of the contract to which users can hold the system. Think of these as micro-theorems. Such micro-theorems are excellent for reference, but reasoning bottom-up from tiny factoids is difficult.

 The second approach is to provide examples and tutorials. Think of tutorials as macro-theorems. Properly done, a tutorial states that "if you follow these steps, our system is obligated to produce the following output." Often, this is just as, and sometimes more, insightful than a slew of micro-specifications. A tutorial can provide coverage over wide swaths of code and pin the interface down in a way a thousand words cannot, as long as it is fully grounded. Snippets of free-floating, incomplete code, of course, are worthless. So are tutorials that fail to cover sufficiently interesting edge cases. Imagine a simple function that computes a quadratic equation. A tutorial that displays f(1) and f(2) is insufficient -- any number of functions can be fitted through those two points. If the documentation clearly states that this is a quadratic, a tutorial with three values in it fully defines the function. Similarly, a good tutorial will, when coupled with the rest of the documentation, illustrate so much of the system behavior that nothing will be left up to the imagination of the reader.

 Neither of these approaches is superior to the other. Whenever possible, documentation should provide both. The HyperDex manual provides ample examples and tutorials in addition to documenting each API call. Every good CPU spec documents every single instruction in the ISA (bottom up) along with sample programs, say, for correct synchronization (top down). I don't even know if Django provides a bottom-up specification, as I have found the top-down tutorials to provide sufficient coverage. So, it's a folly to relegate the tutorials and examples to second-class status; they can be just as useful in establishing a contract.

 Cain and Abel.
 Your users will make Cain look like a loving sibling if you delete their docs underneath them.

 There shall be one true book. Thou shalt clarify and reissue documentation instead of creating countless less-than-holy texts.

 Often, there is a mishmash of documents surrounding any software component. Whenever possible, a good software release will collect and gather them into a single one, true, internally consistent, "consistent cut" of documentation. The onus for this lies with the system developers. Even in the age of excellent search engines, it is not realistic to expect users to track and determine a consistent cut across an ever changing set of documents. This is especially true because so many non-Followers of Principled Documentation leave behind slews of out of date documents scattered on the web that no one can figure out where current, actionable information resides.

 Thou shalt not kill old documentation.

 That said, there are good reasons to leave behind clearly marked old documentation. Not everyone upgrades their software. Pulling old documentation off the web is a vengeful practice. You should mark the documentation as having been deprecated, but leave it behind for posterity.

 Summary
  We are our brogrammer's keepers. Let's hold them, gently but firmly, to higher standards for systems documentation and guide them through the valley of the shadow of marketing-speak to make the world a better place. It's either that, or jihad.
  Related
  I believe HyperDex documentation to
  incorporate these principles to a practicable level. Feel free to use it for inspiration or to tear it apart.
  I am indebted to Fred Schneider for the phrase "honor thy invariant" as well
  as the observation that systems without a specification not only cannot be correct, they cannot even be incorrect.
  I am similarly indebted to Butler Lampson's observations about
  when software reuse makes sense, as well as his approach to software specification.
  The observation that tutorials are macro-theorems while API documentation
  corresponds to micro-theorems was inspired by an excellent talk by Benjamin Pierce with collaborators Nate Foster and Michael Greenberg that examines strong typing versus unit-tests.
  Automated theorem provers and proof assistants have apparently made tremendous
  improvements in recent years. The Coq project has a healthy and devoted fan base. The NuPRL project embodies an even more aggressive approach, where the spec is used to derive the corresponding program, and is able to synthesize protocols such as Paxos from specifications. The resulting code is nowhere near the performance of a hand-crafted protocol such as OpenReplica, but it is guaranteed to implement the specification faithfully.
  Literate programming was something that got a lot of press back
  in the early 90's. Knuth proposed a system called web, and Norman Ramsey had an alternative system called noweb. These systems intertwined documentation with the implementation. I am not sure how I feel about literate programming. I think strong hackers can combine the internal documentation with the external contract, and still remember which facts are for internal consumption and which others are exposed to outsiders. But most people would find the separation hard to maintain. Given the sad state of infrastructure documentation, we'd probably make more progress by focusing on just the external contract problem right now.
  There must have been much work on performance specification since
  Sharon Perl's thesis, but I have not followed this area closely. Let me know if there are outstanding projects that I missed, especially with open-source tools.
  Have additional commandments for the Principled Documentation movement?
  Email me with subject "New gospel". Ours is an all-inclusive, collaborative religion
  - See more at: http://hackingdistributed.com/2013/02/11/principled-documentation/#sthash.BbfoppUj.dpuf



  The world's biggest Bitcoin exchange recently declared bankruptcy, with close to $400M in Bitcoins missing. There has been much talk about what may have happened at Mt.Gox, with speculations running wild. I want to quickly go over what did not happen at Mt. Gox, and how to avoid that which did happen in the future.

  It's Not Transaction Malleability
  Thief
  The initial claim from Mt. Gox was that they lost money to a problem with Bitcoin known as "transaction malleability." Lots of people jumped on this explanation (for one of the better ones, see this one, though its depiction does not match what actually happened). I stayed away from this topic, because to even accept the format of the discussion would have been to lend credence to Mt. Gox's ludicrous claims about transaction malleability. But it was inescapable, and I started getting inquiries from Wall Street about what transaction malleability is and what it implies.

  Essentially, transaction malleability refers to the fact that an attacker can ask Mt. Gox to transfer some Bitcoins, capture the transaction order Mt. Gox issues, and modify it in a way that causes the money transfer to take place yet confuses Mt Gox about whether or not it actually did. This confusion then enables the attacker to contact Mt Gox, claim that the transaction did not go through, and get issued a second payment, thereby stealing money. It's outright fraud and/or theft.

  There are two obstacles to a successful malleability attack, one technical, and one social.

  The technical obstacle is that the attacker has to either front-run Mt Gox or wait for it to slip up. Specifically, she has to capture a legitimate transaction issued by Mt. Gox, modify it, and have her modified transaction accepted by Bitcoin miners in lieu of the original one. Evidently, this technical feat is not hard to do, since we know that people have successfully launched transaction malleability attacks. Interestingly, this fact has a dire implication for Bitcoin that has nothing to do with malleability; namely, a successful malleability attack involves a technical step similar to what happens in selfish mining when the selfish miner goes tete-a-tete against a block found by honest miners, in that it involves a race in the peer-to-peer network. If it is indeed the case that an attacker can front-run a transaction so easily, then that suggests that a selfish miner can outdo the honest miners with high probability. As a co-author of selfish mining, I welcome this bit of news.

  In the case of Mt. Gox, the attacker does not even have to front-run the exchange, because Mt. Gox seems to have guaranteed a win for the attacker by generating bad transactions. Specifically, Mt. Gox occasionally produces signed-but-ill-formed transactions that will never go through the Bitcoin system (like a bank check missing the date field), which gives someone else the opportunity to patch things up and make the transaction go through (by the equivalent of filling in the date field). But Mt. Gox then gets very confused about whether or not its check is accepted, because it is monitoring for checks that are identical to what it issued, with the missing date. No one knows how frequently Mt. Gox issued such bad transactions that an attacker could take advantage of, and no one knows how frequently an attacker could front-run Mt. Gox's good transactions. So there are some technical challenges here for the attacker to overcome.

  But the real obstacle is that the attacker needs to get Mt. Gox to reissue the transaction that she modified. Some people seem to think that just modifying the transaction is sufficient to get paid twice. It isn't, because every Bitcoin transaction exhausts all of its inputs, so once the attacker's transaction goes through, Mt. Gox's original transaction will not. To be paid more than her fair share, the attacker has to socially engineer Mt. Gox to issue a brand new payment. This likely requires some contact with Mt. Gox, like an actual phone call. Doing this at scale requires a lot of contact. Doing it to the tune of $400 million dollars, and draining cold wallets, requires the equivalent of the attacker twerking the Mt. Gox support hotline. There is no practical way it can be done without raising eyebrows.

  Sure enough, people who have analyzed malleable transactions have found only very modest evidence of malleability attacks. There has not been a comprehensive analysis of the 9 sources of malleability in Bitcoin, but a preliminary study shows that the volume of malleable transactions is very small, too small to account for $400M.

  It's Not Lost Keys
  Thief
  Some people have speculated that Mt. Gox may have lost the keys to their wallets. There are two variants of this theory, the "dude, where are my keys" theory and the "bad php" theory. Neither seem likely.

  The "dude, where are my keys" theory postulates that Mt Gox people realized one fine morning that they had digitally misplaced their keys, prohibiting them from accessing their Bitcoin wallets. This is the equivalent of "the dog ate my homework and your $400M" excuse, and is about as believable. People who have analyzed the blockchain have observed activity in accounts that are linked to Mt Gox (though not with 100% certainty), so that observation argues against this theory. But it's not surprising to hear this idea tossed around. Part of the Bitcoin creation myth is that Satoshi and friends have mined a huge number of the initial coins and promptly lost the keys to the fruits of their own labor. People parrot this line even though no one knows who Satoshi is, what he stands for, what his long-term plans and motivations are. So if we are to believe that the tooth fairy took Satoshi's keys, we can also believe that the easter bunny took Mt. Gox's.

  The "bad php" theory is a bit more savvy. It postulates that Mt. Gox's code for computing Bitcoin addresses ended up transferring their cash to an account number to which they lack the private key. Apparently, Mt. Gox's CEO made some statements on an IRC channel to indicate that he lacks access to the money at this time. The theory is that they moved the money to an address for which they lack the key, or parts of the key, and therefore they are busy at work, having rented a huge datacenter trying to brute-force the parts of the key that they lack. It's like someone installed a new lock on their front door, and instead of pocketing the key that came with the new lock, placed some other key on their keychain, and now they're outside the door, metal file in hand, modifying the incorrect key and trying it repeatedly, hoping to break into their own apartment.

  As ridiculous as it may sound, this kind of technical error is actually plausible. So I downloaded the most commonly used PHP library for elliptic curve cryptography, donned the tyvek suit and welding mask I use when approaching PHP, and looked into just how plausible this is.

  There are certain domains where code is inherently full of complicated corner cases. Such code involves systems with huge, typically exponential, numbers of states, and correspondingly complex code paths with many conditionals. For instance, fault-tolerance in distributed data stores is one of these cases, and it remains a topic that certain kinds of software can never hope to conquer, and yes, I'm looking at you MongoDB.

  But elliptic curve crypto is not one of these topics. If the code can generate a handful of Bitcoin addresses and corresponding keys correctly, there is hardly any reason why it cannot do so for all addresses and corresponding keys. My colleagues Nate Foster, Michael Greenberg and Benjamin Pierce argue that unit tests are big correctness theorems: elliptic curve key generation and Bitcoin transaction prep are very big theorems over a fairly uncomplicated code path, so if you can pass that bar for some random addresses, you can likely pass all bars of equivalent height for all addresses. This badly generated key theory seems quite implausible.

  It's Not "Hackers"
  Thief
  The most recent claim by Mt. Gox is that its internal holdings were stolen by computer hackers.

  When I was in middle-school, we'd get chided by a teacher for some misbehavior, like making a mess in the cafeteria. And our response was always the same, delivered with the crackling voice of a boy in the midst of puberty, mixed with the whine of a little kid: "we're not the ones making the meeeesssss, it's the kids from upper classsssseeeees." It was always the kids from upper classes. And in the days before cameras, constant surveillance, and zero tolerance, the teachers had to accept this ridiculous claim. But it was always us. Not once did a kid from an upper class come and make a mess.

  This claim makes absolutely no sense for Mt Gox because they lost the coins in cold storage. "Cold storage" is a fancy way of saying that the keys, required to move the funds, are not online. Someone has to physically touch something to gain access -- a USB stick, a computer, and perhaps a safe. It takes a thief with a corporeal presence to do this.

  It's Not the Web Server or Database
  Thief
  I'm actually shocked that Mt. Gox did not lose money to a database screwup. There are so many flawed NoSQL databases out there that, if you adopt the technologies advertised as "hip" on techcrunch, you'll most likely end up with a broken exchange (more on this in subsequent blog posts, because there are many funny examples that deserve their own discussion). It is quite easy for well-meaning developers to build an exchange on a database that loses transactions, or to restore their database from backups and find themselves in a state where the accounts don't match up. It's safe to guess that their losses were not due to their database, because if they had been, Mt. Gox would have played it up already.

  As an aside, quite a few of the Bitcoin exchanges are, technologically speaking, one giant cluster-love-affair-without-feelings-involved. Coinbase and others tried to assure the public with a very nicely spun PR release, but the bottom line is that many of these exchanges have had recurrent problems. Historically, Wall Street has had a crisis every seven years, because the entire system is based on a broken premise. Again, objectively, Bitcoin exchanges have had a crisis every six months, because they use terrible databases whose code quality matches that of a masters project. I went to the Cornell dairy barn, I drank the fizzy milk drinks that the agriculture students designed for their masters projects. The code being peddled under the first-generation NoSQL rubric is analogous in its construction and quality. This broken-by-design technology is, amazingly, not yet implicated in Mt. Gox's demise. If I had to guess, I'd venture that Mt. Gox is based on something like MySQL -- strongly consistent and not broken by design, though perhaps slow and hard to scale.

  It's maybe the US Government?
  Thief
  Some people claim that the "men in black took 97% of the cash." Perhaps. One would have to believe that they took the keys with the help of the Japanese government, that they seized the coins because of their involvement with Silk Road, that they served a gag order, and that the gag order is still in effect. This is, in theory, possible, but there is no evidence for it, except for a link to 9gag that the Mt. Gox CEO dropped in an IRC chat room. Get it? 9gag? We're punning on URLs now.

  I'd continue the discussion, but I just got a brain aneurysm from the sheer amount of IRC stupidity coming from this CEO who held close to half a billion dollars. I knew that some companies employ developers not to write code but to create noise on social media. But the CEO? On IRC? And some people entrusted their life savings to this guy? Ok, so you may not have known that he had faced computer fraud charges, but it seems like everyone except me was on IRC with this fellow, ROFL'ing when Bitcoin was going to the moon. I have harbored doubts about the solvency of my upstate-NY-hicksville bank on occasion (specifically, when I tried to purchase Russian sovereign bonds, and their "investment advisor," an older gentleman, said "we don't know how to purchase those, but if you're looking for high yield bonds, I'd recommend GM." I pointed out that Russia has nuclear weapons and isn't going anywhere soon, except perhaps towards the warmer seas, and in contrast, his chosen junk-bond company manufactures steel death traps for humans. He said "well, you claim that Russia will be around forever, and some people would claim the same about GM." Couple of years later, Russia paid 14% on their bonds, GM needed a bailout). If I were to encounter my bank's CEO on IRC, I would not think "how cool and edgy, lemme get a screen cap, LOL," I'd put my money in a mattress and short the bank's stock.

  In any case, there is not enough data to rule this explanation in or out. I am suspicious because it plays so well into Bitcoiners innate distrust of the government. Bitcoiners are deathly afraid of governments, fiat, and inflation, even though most of them have never seen two-digit interest rates in their entire lifetimes, don't hold much fiat, and are trying to minimize their tax-related societal obligations to the government. The blame-Obama explanation is too convenient, far too pandering to the masses. Can the Feds seize funds in Japan? Possibly. Can Mt. Gox be storing all of their cold storage keys in safe deposit boxes? Unlikely, but possibly. Can the Feds, combined with the Japanese police, seize the keys from those safe deposit boxes? Plausibly. Can they decrypt them? Not easily, but let's just say they can. Can they go beyond the funds implicated in the Silk Road and seize almost all the funds at Mt. Gox? Yes on the Feds side, unknown on the Japanese side. Can they place an indefinite and binding gag order that no one would dare violate? Maybe. Can they sit on the coins without charging Mt. Gox, even after charging the Dread Pirate Roberts and his money-laundering accomplices? Perhaps. But multiply out those probabilities and we're exploring an unlikely scenario. Add the fact that Mt. Gox initially blamed transaction malleability, it becomes even less likely. If Mt. Gox is solvent, but had its coins seized, it could simply publish its Bitcoin addresses which held the cash, to show that they indeed have some funds. Since this did not happen, I'm inclined to not lend too much credence to this scenario.

  Where Does That Leave Us?
  Thief
  Human history is full of people who were entrusted with valuables, who then absconded with them. Whenever anyone is in a position of trust, whenever the illegal gains to be obtained from breaking that trust exceed the value of one's reputation, there will be a temptation to steal. Jail is not quite a deterrent in this case, where the jurisdiction is Japan and the technology is too new for the justice system. Chances are that this is a simple case of theft, involving at least one insider.

  And it's important to not get too carried away with reading the tea leaves in the blockchain when performing forensic analysis. For instance, some people believe that Mt. Gox transferred $75M three months after they were sued for $75M for breach of contract. Let me go on a limb and claim that in the long and fabled history of lawsuits, not a single one for $75M was settled within 3 months for $75M. That's just not how the justice system works. You sue for X, with the expectation that you'll get less than X, and the other side, even if they are completely wrong, has tons of legal tricks to slow down the process. The blockchain contains many transactions, and the laws of small worlds imply that one can always find a short path between a wallet and a transaction of the desired size. These connections are not always meaningful.

  Overall, Bitcoin has been an ongoing massive online course on economics and distributed systems for the libertarian masses. It's ironic that Mt. Gox turned into a chapter on fractional reserve banking.

  How Do We Avoid The Next Mt. Gox?
  Thief
  So, this is where I could score easy points with the crowd by delivering some well-accepted "wisdom" such as "do not risk more than you can afford to lose." I'll do no such thing; I hate repeating content-free advice. I took stupid risks when I was younger. I'd get seriously depressed if young people were not taking equally stupid risks these days. So I don't expect that anyone's wise words will have any impact, and as well they should not.

  I could also repeat the party line which goes "do not trust your wallet to an exchange." I won't say this, either. The people who parrot this line know full well that a normal person is incapable of running their own wallet. And it's not even good advice: your Macbook or Android phone is not a secure device. The Bitcoin demographic is 98% male, between the ages of 20-30. I base this on official online polls, confirmed by pictures of the Bitcoin Christmas party at Bitcoin HQ in NYC, which somehow managed to look more depressing than my high-school dances. This demographic uses their laptops for viewing highly questionable content and their phones for installing flapping bird software from Jimmy Bob's Software Inc. These are the same people who have started but not yet finished reading The Fountainhead, the same people who grant permissions to make phone calls, activate cameras and send SMSs to phone apps whose sole function is to act as a flashlight. Running a wallet is the last thing they should be trusted with.

  My Personal Approach
  Thief
  I wanted to end on a note of cautious optimism, similar to those visionary futurists from the 70s who were all full of hope for mankind, extrapolating from their experience after a decade of awesome parties. They expected a future filled with tech wonders, strawberries the size of watermelons, and carefree love with strangers. But it's just not possible. Cryptocurrencies are here to stay, but future systems will look nothing like the currency systems we have today. Before Bitcoin, we had Karma, and before Karma, we had millicent, with plenty of others before and in between. There will be others.

  What Nigerian scams are to your grandfather, Bitcoin exchanges are to the 20-30 semi-tech-savvy libertarian demographic. Even if the Bitcoin protocol were perfect, and it isn't, our computing infrastructure is not up to the task of handling high-value transactions. The exchanges are built on the latest hyped technologies that have incredibly poor guarantees, and routinely run into technical problems. They require full trust for their operation and are open to attacks from insiders and out. In a world where secret agents are hopping across machines and networks, keeping coins safe in a computer is a losing battle. Even if you keep everything in cold storage, laptops and phones can be infected with malware that steals coins when they come out of cold storage.

  Then there is the increasingly disconcerting social side. I've always steered my own life towards doing fun and interesting things with smart people in happy and positive communities. Bitcoin, at the moment, is in a slump, with a community that has become its own parody. While the underlying cryptocurrency is quite interesting and the wallet software is fairly good, the exchanges are based on layers upon layers of bad software, run by shady characters. The Bitcoin masses, judging by their behavior on forums, have no actual interest in science, technology or even objective reality when it interferes with their market position. They believe that holding a Bitcoin somehow makes them an active participant in a bold new future, even as they passively get fleeced in the bolder current present. And as if the world does not have enough schmucks with Macbooks who call themselves entrepreneurs, we have the term "Bitcoin entrepreneurs" used unironically by mainstream media. The community has designated a Nobel leaurate as its nemesis, solely because he asked some inevitable questions every thinking person in his profession ought to ask. As far as I'm concerned, the only winning move is to not play this game. Sure, you may make money, perhaps lots of it, on the inevitable ups and down that are sure to come, but you'll be associating with the wrong kinds of people. If your life goals did not include some amount of pride and self-respect for you and your community, there were tons of other, easier ways of making money fast that you could have taken.

  The only "positive" news about Mt. Gox is that money lost at Gox is apparently tax deductible. This is so wrong on so many different levels that I don't know where to begin.

  If one must pick a cryptocurrency, the lowly dogecoin, of all things, is doing everything right. It's based on economic principles that provide the right incentives for a healthy economy. The community does not take itself seriously. Most importantly, no one pretends that Doge is an investment vehicle, a slayer of Wall Street, or the next Segway. No one would be stupid enough to store their life savings in Dogecoins. And people freely share the shiba goodness by tipping others with Doge. So, young people who are excited about cryptocurrencies and want to get involved: Dogecoin is where the action is at. Much community. So wow.

  - See more at: http://hackingdistributed.com/2014/03/01/what-did-not-happen-at-mtgox/#sthash.kMZILrqU.dpuf


  In distributed storage systems, data is often partitioned across multiple servers for scalability and replicated for fault tolerance. The traditional technique for performing such partitioning and replication is to randomly assign data to replicas. Although such random assignment is relatively easy to implement, it suffers from a fatal drawback: as cluster size grows, it becomes almost guaranteed that a failure of a small percentage of the cluster will lead to permanent data loss.

  There is a far smarter way place replicas, explored by a paper from our colleagues at Stanford, titled "Copysets: Reducing the Frequency of Data Loss in Cloud Storage" [CRSK+13]. It was presented at the USENIX conference, co-chaired by one of us, where it received the best student paper award. In this post, we want to first explain how random replica sets, as used in many NoSQL solutions, actually increases the likelihood of data loss, explain how Copysets mitigate the data loss risks, and then describe what it takes to render Copysets to practice in a NoSQL data store.

  Random Replication Isn't Good
  To gain an intuition for how random replication increases the chances of data loss, let's walk through a simple example on a cluster with a replication factor of three. A typical NoSQL data store will divide the cluster into shards, and the data in each shard will be replicated on three randomly chosen servers.

  You can see that, as we add more nodes (as n, the number of total nodes in the system goes up), we will have more and more shards, each of which will contain 3 servers chosen at random. The more shards we have, the more of the 3-server combinations the system will contain. This is especially true for NoSQL systems that use virtual nodes to achieve load balancing. Eventually, we will have enough shards to exhaust all n choose 3 combinations of servers.

  At this point, the simultaneous failure of any three servers anywhere in the cluster is guaranteed to lose data. There are only n choose 3 possible combinations, and the more of them that the system uses, the greater the chances are that a loss of 3 nodes will lead to a completely unavailable shard.

  The figure below, reproduced from the Copysets paper, illustrates how the probability of data loss increases with cluster size.

  Data Loss vs. Cluster Size
  Traditional techniques for randomly selecting replica sets guarantees data loss as cluster size increases.

  It shows that small failures affecting just 1% of the nodes can quickly increase data loss probability to 100%. The HDFS and RAMCloud curves are representative of the way random replication is deployed in many systems (and the Facebook curve hints at the insight that will allow the authors to improve on the state of the art).

  Copyset Insights
  The innovative idea behind Copysets is to limit the total number of replica sets generated. For example, we could pick our replica sets such that no server is assigned to more than one replica set. Instead of randomly generating all n choose 3 combinations, we would generate n/3 replica sets in which every server appears at most once. In this configuration, it's unlikely that killing a small number (~1%) of servers in the cluster would completely kill an entire replica set.

  Of course, nothing comes for free, and Copysets embody two trade-offs. First, when a server fails in the Copyset described above, its replacement can only recover data from two servers, whereas with random placement, state can be recovered from every server in the cluster. In practice, the speed of recovery is typically bottlenecked by the incoming bandwidth of the recovering server, which is easily exceeded by the outgoing read bandwidth of the other servers, so this limitation is typically not a big deal in practice.

  Second, there is a trade-off involving the frequency and magnitude of data loss. Random replication uses more replica sets, and incurs an increase in the frequency of data loss. Copysets reduce the number of replica sets, and reduce the frequency of data loss. In short, catastrophes become a few orders of magnitude more rare (from "one every year" to "one in a few lifetimes"). And it doesn't quite matter that one scheme loses a little bit of data while the other one loses more -- after all, in both cases, one will have to recover from backups (you're running a NoSQL system that can take instantenous, consistent and efficient backups, right? :-). It should not matter much how much data you have to recover; the main costs stem from the inconvenience of having to manually initiate a recovery.

  Cidon et. al. express the latter trade-off as the scatter width of the cluster. The scatter width is the number of servers which hold copies for each server's data. As the scatter width grows, Copysets begin to approximate random replication. Going a level up, the scatter width for a cluster is the average of scatter widths for the servers. For example, a scatter width of two with a replication factor of three will assign each server to exactly one replica set.

  Data Loss vs. Scatter Width
  Data loss probability versus scatter width for an N=5000 cluster.

  The Copyset Algorithm
  Let's see how this idea can be implemented in practice. The core idea is so elegant that it's pretty easy to code up, though it takes more effort than random replication.

  Recall that the dumb random replication algorithm is just:

  For each shard, randomly select R servers

  It's so easy that even MongoDB gets it right. Probably.

  The algorithm for Copyset replication is only slightly more complicated. It takes as input a set of servers, a specified scatter width S, and a desired replication factor R. To generate the Copysets:

  Create S / (R - 1) permutations of the servers.

  Divide each permutation into groups of size R.

  When placing an object, select a server from the cluster as the primary replica for the object.

  Select secondary replicas by randomly choosing one replica set that contains the primary.

  This will create a number of replica sets that achieve the desired replication factor while approximately upholding the desired scatter-width.

  For example, consider a cluster with 9 servers and a replication factor of 3. If we specify a scatter width of 2, we'll need just 2 / (3 -1) = 1 permutation. Let's randomly pick this permutation:

  [1, 6, 5, 3, 4, 8, 9, 7, 2]
  In step two, we generate the replica sets by grouping servers from this permutation into groups of size 3:

  [1, 6, 5], [3, 4, 8], [9, 7, 2]
  We can see that every piece of data is 3-replicated, and in this instance with a scatter width of two, all replica sets are completely independent of each other.

  Now is a good time to step back and contrast the Copyset approach with traditional practices in distributed systems. In a 5000 node cluster with replication factor 3, and where 1% of the nodes fail simultaneously, random replication would lose data 99.99% of the time. In contrast, under the same conditions, Copysets bring data loss probability down to just 0.15% of the time.

  Let's also examine what happens when scatter width is set to four. In this case, Copysets will generate 4 / (3-1) = 2 permutations. Let's say that the two permutations are:

  [1, 6, 5, 3, 4, 8, 9, 7, 2]
  [1, 2, 7, 5, 4, 6, 3, 9, 8]
  For this cluster with scatter width four, the replica sets are:

  [1, 6, 5], [3, 4, 8], [9, 7, 2]
  [1, 2, 7], [5, 4, 6], [3, 9, 8]
  Note that this strategy does not always produce an optimal assignment because it is not possible to always do so. Some servers do indeed have a scatter width of 4, such as server 9, which is in replica sets with 2, 3, 7, and 8. Other servers, such as server 5, which appears jointly with server 6 in two separate replica sets, fail to have a scatter width of 4, but do come close with a scatter width of three. Determining the replica sets optimally looks like a very difficult, possibly NP-hard, problem, so this proposed randomized technique is a great way to get the benefits of Copysets without getting trapped in an intractable problem. The Copysets paper explores the costs of this assignment technique in more depth.

  From Copysets to Chainsets: Cool to Cooler
  Because Copysets seem like an elegant way to reduce the likelihood of disastrous events in a cluster, we decided to implement them in HyperDex, the next-generation NoSQL data store that provides strong consistency and fault-tolerance guarantees, ACID transactions, high scalability and high performance. Because Copysets are an incredibly elegant idea, rendering them to practice is fairly straightforward, but there were two challenges when adopting them for use in HyperDex's chain replication.

  First, it's not immediately apparent how to modify the algorithm for use in a dynamic environment, of the kind we face in a HyperDex cluster where nodes can be added and removed on the fly. The permutations generated by the Copysets algorithm are inflexible and generated prior to online operation of the cluster. Adding or removing servers in the middle of the permutation would require reshuffling all subsequent replica sets to accommodate the change. Adding new servers solely to the end of each permutation would reduce scatter width by creating redundant replica sets.

  Second, Copysets are agnostic to the replication algorithm used by the storage system. In settings where the NoSQL system itself exhibits some structure, the default Copyset generation algorithm is oblivious to that underlying structure and can generate bad sets that require excessive data motion. In particular, HyperDex uses value-dependent chaining, a variant of chain replication, to store its data. We extended Copysets to take into account the structure of value-dependent chains.

  Our implementation of the Copyset algorithm, which we've dubbed Chainsets, improves upon the Copyset algorithm in several key ways:

  Chainsets operate efficiently in a dynamic environment, allowing the replica sets to be computed quickly on the fly as nodes fail and are added back into the system. Because the chainset algorithm is dynamic and incremental, it enables one to integrate new servers and respond to failures without expensive, global data remapping.

  Chainsets naturally integrate with chain replication. The replica sets generated by Chainsets specify the order and position of each server in the chain. Each new server is positioned at the tail of chains for the data it holds, which ensures that it will be easy to integrate into the chain without excessive data motion. The algorithm can probabilistically ensure that servers are assigned to positions with any desired distribution, therefore enabling one to assign servers to specialized tasks (e.g. acting as a primary) according to target goals (e.g. load balancing).

  Chainsets ensure that scatter width is respected even as replica sets are constructed and reconstructed on the fly according to the two bullets above.

  Wrapping Up
  Copysets are a great alternative to random replication. If you are hacking on a data store that uses random replication strategies (or some variant thereof), you really owe it to yourself to check out the Copysets work and see it can reduce your expected frequency of data loss. We've switched the allocation strategy in HyperDex to Chainsets by default because it is strictly better than the random replication algorithm we had previously used as the default.

  - See more at: http://hackingdistributed.com/2014/02/14/chainsets/#sthash.MuihohGK.dpuf

  We are proud to announce HyperDex 1.1, the next generation NoSQL data store that provides ACID transactions, fault-tolerance, and high-performance. Some key features of HyperDex are:

  High Performance: HyperDex is fast. It outperforms MongoDB and Cassandra on industry-standard benchmarks by a factor of 2X or more.

  Advanced Functionality: With the Warp add-on, HyperDex offers multi-key transactions that span multiple objects with ACID guarantees.

  Strong Consistency: HyperDex ensures that every GET returns the result of the latest PUT.

  Fault Tolerance: HyperDex automatically replicates data to tolerate a configurable number of failures.

  Scalable: HyperDex automatically redistributes data to make use of new resources as you add more nodes to your cluster.

  This release brings the following changes and improvements:

  Backup Management: This release introduces HyperDex's automatic backup manager. HyperDex now provides consistent, fast backups. For more information about these tools, see here.

  Support for OS X 10.9: HyperDex compiles from source on OS X 10.9 Mavericks using the stock clang compiler. Install it with brew tap HyperDex/hyperdex && brew install hyperdex.

  Network Console: This release includes a preview of HyperDex's network management console (NOC). With the network operations console, you can monitor the performance of your HyperDex cluster.

  Improved bindings: Numerous improvements to the Python, Java, and Ruby bindings provide a consistent experience across all languages.

  HyperDex runs on 64-bit Linux (Ubuntu, Debian, Fedora, Centos) and OS X. Binary packages for Debian 7, Ubuntu 12.04-13.10, Fedora 18-19, and CentOS 6 are available, as well as source tarballs for other Linux platforms.

  This release provides bindings for C, C++, Python, Java, Ruby, and Go.

  - See more at: http://hackingdistributed.com/2014/02/12/hyperdex-1.1/#sthash.wcVmttmU.dpuf


  We often get asked if there is anyone engaged in selfish mining right now. So let's discuss the telltale signs of selfish mining, how one might go about detecting them, and what the inherent limitations are of different detection techniques.

  Good Indicators of Selfish Activity
  Detector for mining
  There are two distinct network signatures of selfish mining:

  Number of abandoned (orphaned) blocks is a strong indicator of selfish mining activity. The entire idea behind selfish mining is for the selfish pool to outcompete the work of the honest pool. And this will leave behind a series of discarded blocks, where either the honest guy's work was wasted, or the miner took a slight risk and failed. Since every block race will leave such detritus behind in its wake, one could just count the number of such abandoned blocks to see if the rate is stable over time. A rise in the rate would indicate that a selfish mining pool is operating in the network.

  The problem with this approach is that abandoned blocks are pruned inside the Bitcoin network, so it is very difficult to get a definitive count. A measurement tool that connects to the network from just one or a few vantage points may very well miss abandoned blocks, and it may erroneously give the impression that everything is fine when there are fierce battles being fought out inside the network. I am not sure how well blockchain.info is connected, but no web service can be everywhere at once to detect every single orphan, so I take its count of orphans with a large grain of salt.

  Timing of successive blocks provides a hint that someone is engaged in selfish mining. Two blocks in close succession should be a rare occurence with the honest protocol, and more common when someone is quickly releasing selfishly mined blocks in order to squash the honest miners.

  By "close succession," we mean within seconds or perhaps a minute or so of each other. The particular case we're detecting is that of a selfish miner who publishes a chain of length two to squelch a single block discovered by an honest pool. This corresponds to a particular transition in the selfish miner state machine; it is just a subset of the selfish miner's activity, the "double-down-and-squelch" action that a miner can engage in only when it is two or more blocks ahead.

  One could detect this doubling-down scenario by looking at the timestamps on successive blocks in the blockchain. Since mining is essentially an independent random process, we'd expect the interblock time gap to be exponentially distributed. Any deviation from this expectation would be suggestive of selfish mining.

  The problem with timing gap analysis is that it is a statistical test, and it may take a fair bit of selfish mining activity before it detects that something is amiss.

  Bad Indicators of Selfish Activity
  Note that ownership of succesive blocks is not a good sign of selfish mining. This is partly because a miner can mine selfishly, and profitably, without mining two blocks in a row. It's also partly because it's too easy to hide the true owner of a block.

  Concerned Bitcoin enthusiasts were worried about selfish mining when the BTC Guild mined 4 blocks in succession recently. If you examine the blockchain carefully around the time of this occurrence, it looks like these blocks were released within minutes of each other, too far apart in time to indicate selfish behavior, and without any evidence of a corresponding orphan. This kind of successive mining by a given pool is kind of like tossing a coin and getting 6 heads in a row; it's rare, but it's bound to happen every so often in a long sequence. Timing, not ownership, of succesive blocks is the better indicator, ideally coupled with detected orphans at the same time.

  Selfish Countermeasures
  Countermeasures
  As a result of the increased awareness of our findings, it is now inconceivable that someone could engage in selfish mining without facing some kind of backlash. So, any good selfish miner worth her salt will want to do so clandestinely. To stay one step ahead of the attacker, it's worth thinking about what she might do:

  If the detectors examine block ownership, the attacker can easily cover her tracks and cloak her identity. She'd use different Bitcoin and IP addresses, she'd tumble her payouts before using them, and generally masquerade as N separate pools. Each of the N separate pools would look like they are competing with one another, and they would each look like they are too small to matter, and too small to successfully launch a selfish mining attack. But behind the scenes, and unknown to the public, these pools would be coordinated by the same single entity. Outing such collusion is difficult, and this is one of the main reasons why block ownership is a bad indicator.

  If the detectors examine block timing, they are in effect detecting just a subset of the behaviors of a selfish miner. As shown in the state diagram in our paper, a selfish miner makes money by various different schemes, corresponding to different transitions of the state machine: sometimes she reveals 2 or more blocks to squelch an honest miner's single block (this is what the timing detects), and at others, she reveals a single block and competes head-to-head inside the network (this behavior is more common and would be undetected by the timing detector). A selfish miner determined to hide her activities might forego the former behavior to remain below the radar. She would make less money, but she'd remain undetected.

  If the detectors examine abandoned (orphaned) blocks, the attacker is aided by the current behavior of the network where abadoned blocks are silently pruned and discarded, which makes accurately counting orphans impossible. In essence, the Bitcoin network is helping the attacker to destroy the evidence of her activities. If the protocol were to be modified to propagate every solved block, this countermeasure would be mitigated. Such propagation of all viable block solutions would not pose any denial of service or excess load, as block solutions are quite rare and, by design, very difficult to compute.

  All of this shows that detecting selfish mining is possible, but difficult to perform accurately. So, no one can definitively claim that selfish mining is or is not taking place.

  Current Evidence
  That said, both we and others have been looking for suggestive evidence of selfish mining. In particular, Matt Springer has done a fascinating timing gap analysis. You should read his analysis, but since he reveals the punchline in his first sentence, I'll mention it rihgt now: Luckily, the evidence so far indicates that selfish mining is not taking place in Bitcoin. Let's hope that things remain this way for the foreseeable future.

  Related
  We originally described the Selfish Mining attack on Bitcoin, where an attacker can game Bitcoin and mine more than his fair share of coins.

  Some members of the Bitcoin community, in a frenzy to the moon, did not want to acknowledge that the Bitcoin protocol could be game-able and funded the development of an independent simulator whose subtext was to show the error of our ways. That simulator confirmed our findings.

  Frequently-asked questions about selfish mining.

  The novelty of the selfish mining attack

  Altruism among capitalists

  Response to feedback on selfish mining

  BTC Guild got lucky and discovered multiple blocks in a row, most likely without selfish mining.

  - See more at: http://hackingdistributed.com/2014/01/15/detecting-selfish-mining/#sthash.8UsWuBGX.dpuf

  Every data store needs a good backup mechanism. Backups provide disaster recovery and protect against user error. Replication mechanisms provided by modern data stores can provide some of this same protection, but are no substitute for a proper backup mechanism.

  We recently committed support for backups in HyperDex and believe that the resulting tools provide the best backup experience available in any open source or commercial data store. HyperDex backups enable administrators to backup the on-disk state of each HyperDex server in its native form. Backups are fast, and naturally support incremental backup and restore.

  Let's illustrate how one can use these tools to easily take consistent backups, and talk about how they compare to other NoSQL backup solutions.

  Backups at a glance
  Taking a backup of the entire HyperDex cluster is as easy as issuing the command:

  $ hyperdex backup-manager --backup-dir /path/to/backups
  Behind the scenes, this command will:

  Put the cluster into read-only mode

  Wait for ongoing writes to quiesce

  Take a backup of the coordinator

  Take a backup of each individual daemon

  Put the cluster back into read-write mode

  Copy the coordinator state and each of the daemon's state to /path/to/backups/YYY-mm-ddTHH:MM:SS for the current date and time, exploiting data available in previous backups if possible.

  Suppose the unthinkable happens (a datacenter outage, cooling failure, flood, act of God, or more likely, some human error that causes data loss, in which case some manager has to trot out the well-known "we had an awesome network but a rodent ate through the cables, fried itself and your data" defense, which is why every good manager keeps a fridge full of burnt rodents in her datacenter, but we digress) and we need to restore from backup on a different set of machines.

  Restoring the cluster is as simple as copying each directory within /path/to/backups/YYY-mm-ddTHH:MM:SS to a different daemon server, and then launching a new coordinator from the backup:

  $ hyperdex coordinator --restore coordinator.bin
  You can then re-launch each daemon using the individual data directories within the backup. The daemons can be re-launched on entirely different servers, with different IP addresses than they were originally bound.

  All freshly-restored clusters come online in read-only mode. Once all daemons have come online, you can make the cluster fully operational by executing:

  $ hyperdex set-read-write
  That's everything required to do a full backup and restore.

  Backup Efficiency
  Backups
  HyperDex's backups are extremely efficient. It's possible to take a snapshot of terabytes of data in sub-second time. The secret sauce behind HyperDex's backups lies in the structure of its on-disk storage.

  HyperDex uses HyperLevelDB as its storage backend, which, in turn, constructs an LSM-tree on disk. The majority of data stored within HyperLevelDB is stored within immutable .sst files. Once written, these files are never overwritten. The backup feature within HyperDex can extract a snapshot of these files via an instantaneous hard link in the filesystem. This state, once snapshotted so efficiently, can then be transferred elsewhere at will.

  At the cluster level, backups pause writes for a short period of time while the HyperDex daemons take their individual snapshots. Because individual snapshots are efficient, the pause generally takes less than a second.

  Finally, the backup tool ensures that the cluster is restored to full operation before transferring any data over the network.

  Advanced Backup Techniques
  Backup solutions often require customization; some setups use network attached storage and back up to a centralized location, others will demand custom control over where and how the snapshots are transferred. Let's walk through a more detailed example where the snapshots are manually managed.

  Every custom backup solution will start by initiating a snapshot operation within the HyperDex cluster. For example, this command will create a snapshot named "my-backup":

  $ hyperdex backup my-backup
  11419051871340077972 127.0.0.1 /home/rescrv/src/HyperDex/d1/backup-my-backup
  13855129396075833686 127.0.0.1 /home/rescrv/src/HyperDex/d2/backup-my-backup
  14576759822280270796 127.0.0.1 /home/rescrv/src/HyperDex/d3/backup-my-backup
  When this command succeeds, it will exit zero, and output a list of the servers that hold individual snapshots. For instance, we can see that the server identified by "11419051871340077972" on 127.0.0.1 backed up its state to /home/rescrv/src/HyperDex/d1/backup-my-backup.

  We can then transfer that state to a directory of our choosing with rsync:

  $ rsync -a --delete -- 127.0.0.1:/home/rescrv/src/HyperDex/d1/backup-my-backup/ 11419051871340077972
  This command will create a directory named "11419051871340077972" and transfer all of the server's state to this directory. This directory is suitable for passing to the --data= parameter of HyperDex's daemon.

  The backup command above also creates a file in the current directory called my-backup.coordinator.bin. This file contains the coordinator's state, suitable for using to bootstrap a new coordinator cluster as shown above.

  The backup-manager command shown above executes the backup command, and the appropriate rsync commands to transfer state into the backup directory. Of course, this other possible schemes include:

  Using filesystem-level snapshots to save the backup directories. Daemons may then be launched from the snapshots in the future.

  Having the daemons directly rsync their state to an off-site storage location. This avoids any resource constraints that may limit the effectiveness of the backup-manager.

  Restore multiple instances of a cluster simultaneously to experiment with your production data. For example, before deploying a new application to production, backup the cluster, and restore the backup in parallel to the production cluster. Then, deploy the new application against the parallel cluster before deploying to production.

  Incremental Backups
  The backup-manager command demonstrated above supports incremental backups, wherein successive calls will only transfer state that has been modified since the last backup. Each backup appears as a full backup of the cluster and shares storage resources with previous backups. Consequently, the backup manager will store several complete backups of the cluster without requiring the storage space of several complete backups.

  The implementation leverages the --link-dest= option to rsync to avoid making redundant copies of the data. When provided this option, rsync will look for files in the specified directory. If the file in the source directory is also present, unchanged, in the link-dest directory, then it will be hard-linked into place and not copied over the network.

  Assuming an existing backup exists in the directory 11419051871340077972-1, we can create an incremental backup in 11419051871340077972-2 by issuing:

  $ rsync -a --delete --link-dest=11419051871340077972-1 -- \
      127.0.0.1:/home/rescrv/src/HyperDex/d1/backup-my-backup/ \
          11419051871340077972-2
          Consistent versus Inconsistent Backups
          In a sharded data store, the simple but flawed way to make backups is to take a snapshot at each of the shards independently. Needless to say, this is precisely what a number of other systems do. The problem with this approach is that backups taken without coordination among the shard servers are not going to take a consistent cut through the data. A portion of the data in the backup might come from a snapshot at time T0, while others come from T1, and yet others from different points in time. Depending on the application, these kinds of uncoordinated backups can lead to inconsistencies in the database. Restoring from a backup might take the system to a state that actually never existed. And that defeats the entire point of a backup.

          In contrast, HyperDex backups are coordinated. The network pauses for slightly less than a second, the operation queues are drained, pending operations are completed, and therefore the snapshots capture a consistent, clean view of the data. A restore will take the system back to a precise point in time.

          Comparison to other NoSQL backup solutions
          Backups
          The flexibility enabled by HyperDex's backup and backup-manager tools sets a new industry standard for NoSQL backup solutions. Other systems require support from expensive filesystem- or operating system-level primitives, a complete shutdown, or are inconsistent.

          Cassandra:
          Cassandra's backups are inconsistent, as they are taken at each server independently without coordination. Further, "Restoring from snapshots and incremental backups temporarily causes intensive CPU and I/O activity on the node being restored."
          MongoDB:
          MongoDB provides two backup strategies. The first strategy copies the data on backup, and re-inserts it on restore. This approach introduces high overhead because it copies the entire data set without opportunity for incremental backup.

          The second approach is to use filesystem-provided snapshots to quickly backup the data of a mongod instance. This approach requires operating system support and will produce larger backup sizes.
          Riak:
          Riak backups are inconsistent, as they are taken at each server independently without coordination, and require care when migrating between IP addresses. Further, Riak requires that each server be shut down before backing up LevelDB-powered backends.
          The HyperDex backup/restore process is strongly consistent, doesn't require shutting down servers, and enables incremental backup support. Further, the process is quite efficient; it completes quickly, and does not consume CPU or I/O for extended periods of time.

          Wrapping up
          HyperDex's backup tools enable administrators to take periodic backups of a live HyperDex cluster. The new backup tools are currently available in the open source git repository and will be included in the upcoming HyperDex 1.1 release.

          Related
          HyperDex is a next generation NoSQL data store developed originally at Cornell, available as open source, with commercial support.

          - See more at: http://hackingdistributed.com/2014/01/14/back-that-nosql-up/#sthash.KvVWUhil.dpuf



          BTC Guild, one of the bigger mining pools, released a series of 4 blocks in quick succession today. Bitcoin difficulty is adjusted to achieve a block to be mined roughly every 10 minutes, whereas the blocks in this chain were released within a few minutes of each other. Since this is a relatively rare event, it triggered a discussion on what could account for this.

          Is It Due to Selfish Mining?
          Some people prudently asked whether this could be a sign of selfish mining. Let's do a quick check through the evidence.

          Charles Dickens
          A Tale of Two Chains, involving orphans, right out of a Dickens novel.

          A series of blocks released in quick succession is indeed a sign of selfish mining activity, though it is not definitive (i.e. neither necessary nor sufficient). While selfish miners will occasionally release series of blocks, they do not necessarily need to release multiple blocks every time; selfish mining can be profitable even when the selfish miner releases a single block at a time.

          A necessary sign of successful selfish mining, however, is that there be a corresponding abandoned chain, rooted at the same blockchain height as the first block released by the suspected selfish miner. This abandoned chain would be of length equal to or one block shorter than the number of blocks released by the selfish miner.

          Two quick asides: (1) "abandoned blocks" are sometimes referred to as "orphans." We don't like the latter term because, unlike a real orphan, a Bitcoin orphan actually has a well-defined parent, so the term is a misnomer. Misnomers indicate muddled thinking, and Satoshi himself does not use the term, so we'll stick to the more apt term "abandoned." (2) Neither of the signs above are sufficient to accuse a pool of selfish mining, as these events may occur naturally by chance. This is partly why identifying selfish miners is difficult.

          While the quick succession of blocks is clearly present in this case, there is no evidence of an abandoned chain in blockchain.info.

          Can We Really Tell?
          But blockchain.info is not the definitive source of abandoned blocks. Bitcoin's decentralized architecture makes the detection of abandoned blocks difficult. A (to-be-abandoned in a hypothetical scenario) block may indeed arise somewhere in the network, trigger a selfish miner to double-down with his pre-mined and privately-held blocks, and then simply get discarded by intermediaries who adopt the longer chain offered by the selfish miner. So it's possible for a blockchain reporting website to miss bona-fide abandoned blocks, especially if the website is connected to the Bitcoin network via a small number of peers.

          And this is part of the other reason why it is difficult to tell when a selfish mining attack is being launched.

          Getting back to BTC Guild, the chances of a public solving blocks, triggering the release of 4 selfishly mined blocks in a row, all of which lead to abandoned blocks that are not recorded anywere are actually very small. There would have to be at least 3 independently computed and later abandoned blocks, all of which must have lost out to the selfish miner. Unless this selfish miner is particularly adept at gaming the network and winning the races in such a definitive way to leave no trace of three separate abandoned blocks injected into the network from (likely) different locations, we'd expect to find some evidence when a relatively long series of blocks are involved.

          Other Signs of Innocence
          Not to mention, it would not make sense for a respected and public pool such as BTC Guild to engage in selfish mining under its own name. A smart selfish miner would use fresh Bitcoin addresses and hop between IP addresses to hide her tracks.

          Our Call
          At the moment, it does not look like this episode is due to selfish mining.

          It could plausibly be due to a chance event: BTC Guild may have gotten lucky a few times in a row in quick succession. As the masses were transfixed to their TVs and tablets for the results of their national fiat lotteries on Jan 1st, BTC Guild may have outdone most of them with an $83,000 jackpot.

          It could also be due to a significant amount of mining power going online, joining BTC Guild, and being taken off shortly thereafter.

          Or it could be a combination of the two.

          In any case, we see no evidence that points to selfish mining in this instance. But this event calls for diligent monitoring of the blockchain dynamics from multiple vantage points.

          Related
          We originally described the Selfish Mining attack on Bitcoin, where an attacker can game Bitcoin and mine more than his fair share of coins.

          Some members of the Bitcoin community, in a frenzy to the moon, did not want to acknowledge that the Bitcoin protocol could be game-able and funded the development of an independent simulator whose subtext was to show the error of our ways. Interestingly, that simulator confirmed our findings.

          Followup post on frequently-asked questions

          Followup post on the novelty of the selfish mining attack

          Followup post on altruism among capitalists

          Followup post on response to feedback on selfish mining

          - See more at: http://hackingdistributed.com/2014/01/01/btc-guild-selfish-mining/#sthash.7XlnODyr.dpuf


Failures are the bane of existence for distributed systems. The primary technique for masking failures is, of course, to replicate the state and functionality of the distributed system so that the service as a whole can continue to function even as some pieces may fail.

But this replication has been, historically, very difficult to perform. While the first papers in this space appeared in the late 80's [OL88, Lam98], it wasn't until the last decade that we got actual reusable components. The two most commonly used components are Google's Chubby (based on Paxos) and its open-source counterpart, ZooKeeper (based on the ZAB protocol). Both of these systems provide a very similar interface, namely, a filesystem API where one can read and write replicated files.

When we were developing a distributed coordinator for HyperDex, we found that these systems suffer from a common shortcoming, because of their API. Inspired by OpenReplica's approach to replicating Python objects, we decided to build a new open-source state machine replication library, called Replicant, that makes it easy to build high-performance replicated state machines specified as C/C++ code.

Let's first illustrate this API problem, and then we'll explore how Replicant's improved API overcomes this problem and provides a superior way to develop distributed systems.

The distributed synchronization problem
Conceptually, the filesystem API is very easy to use. Applications can create files and directories, and they communicate using the filesystem hierarchy and the files' contents. In practice, this results in rather convoluted code to perform straightforward tasks. Here are the steps to acquire a lock in ZooKeeper (copied from ZooKeeper recipes):

Call create() with a pathname of "_locknode_/guid-lock-" and the sequence and ephemeral flags set.

Call getChildren() on the lock node without setting the watch flag (this is important to avoid the herd effect).

If the pathname created in step 1 has the lowest sequence number suffix, the client has the lock and the client exits the protocol.

The client calls exists() with the watch flag set on the path in the lock directory with the next lowest sequence number.

If exists() returns false, go to step 2. Otherwise, wait for a notification for the pathname from the previous step before going to step 2.

This code essentially maps a queue onto a directory in the filesystem, using the lexicographic ordering of files within the directory to enforce a FIFO order. Of course, it would be even better if we could just write a linked-list, but ZK's API doesn't allow for such a structure.

Photo by JB-London http://www.flickr.com/photos/jb-london/
ZooKeeper Parkour:  one wrong move and you'll break your back-end.

As an exercise, look at the above code, and try to understand why Step 5 is necessary for the algorithm. Is Step 5 even necessary? What bad things can happen if Step 5 is omitted?

Can you extend the above example to implement a semaphore that permits a configurable number of processes to enter the critical section? Despite the similarities in the two problems, the result of such an exercise would be a subtly different algorithm that is incompatible with the above protocol.

The filesystem abstraction is the wrong abstraction for building distributed systems.

There exist better abstractions.

State machines to the rescue
Replication in distributed systems is a well-studied topic that spans decades of research. A good starting point is this tutorial.

The key concept in this space, around which Replicant is built, is that of a replicated state machine: the programmer defines the initial state that should tolerate failures and the set of allowable transitions on this state, and the system ensures that every replica starts in the same state, and every replica makes the same state transitions in the same order.

State machine replication is much more elegant than the filesystem API used in ZooKeeper and Chubby. In Replicant, programmers deal with objects, and worry about methods that define legal transitions on those objects. This is a lot easier than having to shoehorn a complex operation back onto directory and filesystem operations.

Replicant replicates state machines
Replicant is designed from the ground up for replicating state machines. You provide Replicant with a state machine in the form of a .so file; it replicates it and performs the hard work of keeping all the replicas in sync at all times. Its clean interface means that applications need not be a mess of tangled filesystem calls, but instead can be expressed as plain, single-threaded C code that executes as if it's on a single machine even though there will be many replicas of the code running simultaneously.

In Replicant, you model your application as a set of named procedures that mutate an arbitrary piece of state. Replicant ensures that these procedures are called in the same order at all replicas, and provides a convenient RPC-like mechanism for making calls in a fault-tolerant manner.

Let's examine how this works.

A simple Replicant object
For the sake of simplicity, let's first imagine that we are building a reliable counter service, possibly for tracking page views or orders.

We will first need some boilerplate code to get started:

#include <stdint.h>
#include <string.h>
#include <replicant_state_machine.h>
Next, we'll need a way to create our counter, by allocating enough memory to hold our crucial state, which in this case happens to be just 64 bits, and setting it to its initial value:

void* counter_create(struct replicant_state_machine_context* ctx) {
    uint64_t* x = malloc(sizeof(uint64_t));
        if(x) {
                *x = 0;
                    }
                        return x;
                        }
                        Now, on occasion, our replicas might fail, and they may need to be recreated. Similarly, on occasion, we might want to take a snapshot of our counter, so that we can propagate it to failed replicas to bring them up to date, or to store it on permanent storage. For these functions, we'll need two additional helper functions:

                        void* counter_recreate(struct replicant_state_machine_context* ctx,
                                               const char* data, size_t data_sz) {
                                                   uint64_t* x = malloc(sizeof(uint64_t));
                                                       if (x) {
                                                               memmove(x, data, data_sz < sizeof(uint64_t) ? data_sz : sizeof(uint64_t));
                                                                   }
                                                                       return x;
                                                                       }

                                                                       void counter_snapshot(struct replicant_state_machine_context* ctx,
                                                                                             void* obj, const char** data, size_t* data_sz) {
                                                                                                 char* ptr = malloc(sizeof(uint64_t));
                                                                                                     *data = ptr;
                                                                                                         *data_sz = sizeof(uint64_t);
                                                                                                             if (ptr) {
                                                                                                                     memmove(ptr, obj, sizeof(uint64_t));
                                                                                                                         }
                                                                                                                         }
                                                                                                                         And on occasion, we might want to shut down a replica and clean up its state. In this case, there isn't much to do except to free the little bit of space our counter is using:

                                                                                                                         void counter_destroy(struct replicant_state_machine_context* ctx,
                                                                                                                                              void* f) {
                                                                                                                                                  free(f);
                                                                                                                                                  }
                                                                                                                                                  That takes us to the critical increment operation. It's incredibly easy to specify in Replicant:

                                                                                                                                                  void counter_increment(struct replicant_state_machine_context* ctx,
                                                                                                                                                                         void* obj,
                                                                                                                                                                                                const char* data, size_t data_sz) {
                                                                                                                                                                                                    uint64_t* count = (uint64_t*)obj;
                                                                                                                                                                                                        *count += 1;
                                                                                                                                                                                                            replicant_state_machine_set_response(ctx, obj, sizeof(uint64_t));
                                                                                                                                                                                                            }
                                                                                                                                                                                                            Replicant represents each object as a void*, which is passed to every method invocation on the object. Typically the user will cast this void* to a type suitable for holding the object's state. In our counter, the state is a uint64_t that is allocated to store the count. The code increments the counter, and returns the new value to the client.

                                                                                                                                                                                                            All we need to do now is to package this up, let Replicant know where to find our maintenance functions, and let the world know what the interface to this shared object is:

                                                                                                                                                                                                            struct replicant_state_machine rsm = {
                                                                                                                                                                                                                counter_create,
                                                                                                                                                                                                                    counter_recreate,
                                                                                                                                                                                                                        counter_destroy,
                                                                                                                                                                                                                            counter_snapshot,
                                                                                                                                                                                                                                {{"increment", counter_increment},  // list of object-specific methods
                                                                                                                                                                                                                                     {NULL, NULL}}  // sentinel
                                                                                                                                                                                                                                     };
                                                                                                                                                                                                                                     In just 74 lines of code, we've defined a complete Replicant object that maintains a counter.

                                                                                                                                                                                                                                     This object is instantiated as a Replicant object by compiling the code to a .so file and uploading the shared object to the Replicant cluster. Replicant will call the counter_create, counter_recreate, counter_destroy, and counter_snapshot methods as appropriate to create a new instance of the object which can then be passed to all subsequent calls.

                                                                                                                                                                                                                                     On the client side, the application may connect to a cluster and send remote calls to the instantiated object. For example, to invoke the increment method on an instance of our counter named mycount, we can issue code like:

                                                                                                                                                                                                                                     ...
                                                                                                                                                                                                                                     replicant_client r("127.0.0.1", 1982);
                                                                                                                                                                                                                                     int64_t sid = r.send("mycount", "increment", "", 0, ...);
                                                                                                                                                                                                                                     ...
                                                                                                                                                                                                                                     This method invocation will pass the name of the object, "mycount", and the method name, "increment", to the replicated object. Replicant will order all method calls and invoke the counter_increment C function with the object's state.

                                                                                                                                                                                                                                     We've tailored the above code samples for the sake of exposition and have omitted several details about how to build and deploy our Replicant object. In the next section, we'll walk through a complete end-to-end example that shows you how to build an object, start a new Replicant cluster, instantiate your object, and make method calls.

                                                                                                                                                                                                                                     Distributed locking: Replicant style
                                                                                                                                                                                                                                     Let's take a look at the distributed lock example, described above. The entire implementation is pretty compact, at just over 500 lines, available on GitHub. Replicant enables us to invoke the resulting replicated object easily from the command line.

                                                                                                                                                                                                                                     First, make sure you have the latest version of Replicant installed. On the HyperDex downloads page we offer instructions for installing HyperDex. Instead of hyperdex, install the replicant and libreplicant-dev packages.

                                                                                                                                                                                                                                     Once you have Replicant installed, clone our repository and make build the code:

                                                                                                                                                                                                                                     $ git clone https://github.com/rescrv/demo-distributed-lock
                                                                                                                                                                                                                                     $ cd demo-distributed-lock
                                                                                                                                                                                                                                     $ make
                                                                                                                                                                                                                                     This will create the .so that defines the state machine, as well as the client programs (called lock, unlock, holder) that can be called to invoke methods on that replicated state machine.

                                                                                                                                                                                                                                     If all went well, you should see the following files:

                                                                                                                                                                                                                                     $ ls
                                                                                                                                                                                                                                     holder  holder.cc  lock  lock.cc  lock-object.c  lock-object.o
                                                                                                                                                                                                                                     lock-object.so Makefile  unlock  unlock.cc  util.h
                                                                                                                                                                                                                                     Then, in one terminal, launch the Replicant daemon:

                                                                                                                                                                                                                                     $ replicant daemon --foreground --listen 127.0.0.1 --listen-port 1982 --data=/path/to/tmp
                                                                                                                                                                                                                                     I1217 00:50:52.963048  9199 daemon.cc:211] running in the foreground
                                                                                                                                                                                                                                     I1217 00:50:52.963330  9199 daemon.cc:212] no log will be generated; instead, the log messages will print to the terminal
                                                                                                                                                                                                                                     I1217 00:50:52.963489  9199 daemon.cc:213] provide "--daemon" on the command-line if you want to run in the background
                                                                                                                                                                                                                                     I1217 00:50:52.971564  9199 daemon.cc:246] started new cluster from command-line arguments: configuration(cluster=4047024000452744282, prev_token=0, this_token=7139395057213071932, version=1, command=[13325666613164145467], config=[13325666613164145467], members=[chain_node(bind_to=127.0.0.1:1982, token=13325666613164145467)])
                                                                                                                                                                                                                                     I1217 00:50:52.972247  9199 daemon.cc:511] resuming normal operation
                                                                                                                                                                                                                                     I1217 00:50:52.972265  9199 daemon.cc:1162] deploying configuration configuration(cluster=4047024000452744282, prev_token=0, this_token=7139395057213071932, version=1, command=[13325666613164145467], config=[13325666613164145467], members=[chain_node(bind_to=127.0.0.1:1982, token=13325666613164145467)])
                                                                                                                                                                                                                                     I1217 00:50:52.972565  9199 daemon.cc:1251] the latest stable configuration is configuration(cluster=4047024000452744282, prev_token=0, this_token=7139395057213071932, version=1, command=[13325666613164145467], config=[13325666613164145467], members=[chain_node(bind_to=127.0.0.1:1982, token=13325666613164145467)])
                                                                                                                                                                                                                                     I1217 00:50:52.972784  9199 daemon.cc:1252] the latest proposed configuration is configuration(cluster=4047024000452744282, prev_token=0, this_token=7139395057213071932, version=1, command=[13325666613164145467], config=[13325666613164145467], members=[chain_node(bind_to=127.0.0.1:1982, token=13325666613164145467)])
                                                                                                                                                                                                                                     W1217 00:50:52.973001  9199 daemon.cc:1258] the most recently deployed configuration can tolerate at most 0 failures which is less than the 2 failures the cluster is expected to tolerate; bring 4 more servers online to restore 2-fault tolerance
                                                                                                                                                                                                                                     I1217 00:50:52.973194  9199 daemon.cc:1952] we are chain_node(bind_to=127.0.0.1:1982, token=13325666613164145467) and here's some info: issued <=1 | acked <=1
                                                                                                                                                                                                                                     I1217 00:50:52.973361  9199 daemon.cc:1955] our stable configuration is configuration(cluster=4047024000452744282, prev_token=0, this_token=7139395057213071932, version=1, command=[13325666613164145467], config=[13325666613164145467], members=[chain_node(bind_to=127.0.0.1:1982, token=13325666613164145467)])
                                                                                                                                                                                                                                     I1217 00:50:52.973575  9199 daemon.cc:1956] the suffix of the chain stabilized through 0
                                                                                                                                                                                                                                     I1217 00:50:52.973718  9199 daemon.cc:2199] command tail stabilizes at configuration 1
                                                                                                                                                                                                                                     Let's create an object called "lock" using the lock-object.so you built above:

                                                                                                                                                                                                                                     $ replicant new-object lock lock-object.so
                                                                                                                                                                                                                                     If all goes well, you'll see the following log messages in your Replicant daemon:

                                                                                                                                                                                                                                     I1217 00:53:37.413707  9894 daemon.cc:1910] registering client 5857825500350879133
                                                                                                                                                                                                                                     I1217 00:53:37.414326  9894 daemon.cc:1915] disconnecting client 5857825500350879133
                                                                                                                                                                                                                                     I1217 00:53:37.415477 10025 object_manager.cc:891] spawning worker thread for object 7813573189723750400
                                                                                                                                                                                                                                     Here, 5857825500350879133 is the client that executes the new-object command. The number 7813573189723750400 is the ID of the "lock" object. Your numbers may vary, and that's OK.

                                                                                                                                                                                                                                     Now let's have Alice grab the lock:

                                                                                                                                                                                                                                     $ ./lock 127.0.0.1 1982 Alice
                                                                                                                                                                                                                                     lock acquired @ 0
                                                                                                                                                                                                                                     release with: unlock 127.0.0.1 1982 Alice@0
                                                                                                                                                                                                                                     Here, Alice has acquired the lock, and is currently holding it. We can see that this is the case by looking in the Replicant daemon's log for:

                                                                                                                                                                                                                                     I1217 00:56:35.535212 10025 object_manager.cc:959] lock:lock @ 5: lock acquired by Alice@0
                                                                                                                                                                                                                                     When Alice is done with the lock, she can release it with:

                                                                                                                                                                                                                                     $ ./unlock 127.0.0.1 1982 Alice@0
                                                                                                                                                                                                                                     and we see in the daemon's console:

                                                                                                                                                                                                                                     I1217 00:57:38.499760 10025 object_manager.cc:959] lock:unlock @ 7: lock released by Alice@0
                                                                                                                                                                                                                                     Now what happens if Alice and Bob try to acquire the lock at the same time? One of the two must block and wait for the other to release it:

                                                                                                                                                                                                                                     $ ./lock 127.0.0.1 1982 Alice
                                                                                                                                                                                                                                     lock acquired @ 1
                                                                                                                                                                                                                                     release with: unlock 127.0.0.1 1982 Alice@1
                                                                                                                                                                                                                                     $ ./lock 127.0.0.1 1982 Bob
                                                                                                                                                                                                                                     Here, Bob's command will block until Alice runs (in another terminal):

                                                                                                                                                                                                                                     $ ./unlock 127.0.0.1 1982 Alice@1
                                                                                                                                                                                                                                     For convenience, the implementation enables anyone to see who holds the lock:

                                                                                                                                                                                                                                     $ ./holder 127.0.0.1 1982
                                                                                                                                                                                                                                     lock held by: Bob@2
                                                                                                                                                                                                                                     If you browse the code for the lock object, you'll see that each command line program translates to a function on in the lock object. For example, the unlock command calls the lock_unlock function, passing the command-line argument directly to the object. The linked code is pretty well-documented, and is easy to use as a starting point for building Replicant objects.

                                                                                                                                                                                                                                     Where to from here?
                                                                                                                                                                                                                                     We developed Replicant because it enables a cleaner, easier way to develop fault-tolerant distributed systems. Replicant handles the complexity of replication and lets us focus on our applications. Rather than having to mash an application onto a filesystem API, it allows us to implement shared state directly as objects. The resulting applications are simple to understand and easy to prove correct, even though the underlying replication protocols typically are subtle and complicated.

                                                                                                                                                                                                                                     We currently use Replicant to implement HyperDex's coordinator. The hyperdex coordinator command you see in the HyperDex Tutorial is just a thin wrapper around Replicant that automatically creates a new Replicant daemon and associated object.

                                                                                                                                                                                                                                     The next time you reach for ZooKeeper, ask yourself whether it provides the primitive you really need. If ZooKeeper's filesystem and znode abstractions truly meet your needs, great. But the odds are, you'll be better off writing your application as a replicated state machine.

                                                                                                                                                                                                                                     In a future blog post we'll describe ChainSaw (an extension to Chain Replication [vRS04]), the new consensus protocol behind Replicant that does the heavy lifting for replicated state machines. In the meantime, you can get started with Replicant today by grabbing the code from GitHub and building your own replicated objects.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/12/26/introducing-replicant/#sthash.LSKvNzW9.dpuf



                                                                                                                                                                                                                                     We are proud to announce HyperDex 1.0, the next generation NoSQL data store that provides ACID transactions, fault-tolerance, and high-performance. With this release, we pass the 1.0 milestone.

                                                                                                                                                                                                                                     Some key features of HyperDex are:

                                                                                                                                                                                                                                     High Performance: HyperDex is fast. It outperforms MongoDB and Cassandra on industry-standard benchmarks by a factor of 2X or more.

                                                                                                                                                                                                                                     Advanced Functionality: With the Warp add-on, HyperDex offers multi-key transactions that span multiple objects with ACID guarantees.

                                                                                                                                                                                                                                     Strong Consistency: HyperDex ensures that every GET returns the result of the latest PUT.

                                                                                                                                                                                                                                     Fault Tolerance: HyperDex automatically replicates data to tolerate a configurable number of failures.

                                                                                                                                                                                                                                     Scalable: HyperDex automatically redistributes data to make use of new resources as you add more nodes to your cluster.

                                                                                                                                                                                                                                     HyperDex runs on 64-bit Linux (Ubuntu, Debian, Fedora, Centos) and OS X. Binary packages for Debian 7, Ubuntu 12.04-13.10, Fedora 18-20, and CentOS 6 are available from the Downloads page[1], as well as source tarballs for other Linux platforms.

                                                                                                                                                                                                                                     This release provides bindings for C, C++, Python, Java, Ruby, and Go.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/12/11/hyperdex-one-oh/#sthash.PL2PRY21.dpuf



                                                                                                                                                                                                                                     Bitcoin had a problem. A problem so big that the community came together to issue a bounty of 10BTC and 200.2LTC, or an excess of $10,000 at today's exchange rate. In this article, we'll look at the problem they encountered and how we fixed it.

                                                                                                                                                                                                                                     The Problem
                                                                                                                                                                                                                                     Wanted Dead or Alive.
                                                                                                                                                                                                                                     Wanted Dead or Alive. But preferably dead.

                                                                                                                                                                                                                                     On Mac OS X, LevelDB can become corrupt and fail to re-open the database. This issue can prevent users from storing the block chain and participating in the Bitcoin network. Users affected by the bug will typically see an error to the effect of Corruption: block checksum mismatch or missing start of fragmented record(2). These log messages both stem from the same underlying problem within LevelDB.

                                                                                                                                                                                                                                     For efficiency, LevelDB always performs I/O in a sequential fashion: every write generated by LevelDB appends data to a disk. Consequently, data is never overwritten, only recreated in another file. In a straightforward implementation, this mechanism could be accomplished using the write system call sequentially.

                                                                                                                                                                                                                                     But the write call can be costly when under contention by multiple threads, so the LevelDB developers implemented a user-space buffering mechanism using memory-mapped I/O. This buffering mechanism maps regions of fixed size such that the number of pages mapped in any one file will never exceed a particular constant. At any given time, the writing process is filling whichever region is mapped. It then unmaps the region before moving onto the next.

                                                                                                                                                                                                                                     Of course, it's possible for a user's write to straddle the boundary between the current region and the next region to be mapped. When such an overlap occurs, the writer places whatever it can from the user's write into the current region, unmaps the current region, maps the next region and finishes placing the data.

                                                                                                                                                                                                                                     On Linux, this sequence of operations is safe because the virtual memory subsystem will keep these pages around until they can be written to disk. Programmers refer to these pages with unwritten data as being "dirty". As far as we are aware, POSIX imposes no requirements on the handling of dirty pages after an unmap. It's entirely possible for a POSIX-conformant system to simply discard any dirty pages that the user unmapped.

                                                                                                                                                                                                                                     And therein lies the problem. On Mac OS X, the dirty pages seem to be discarded without being flushed to disk.

                                                                                                                                                                                                                                     The Root Cause
                                                                                                                                                                                                                                     If the munmap call discards the data at the tail of a memory-mapped region, there will be zero-filled holes in the output file. For SST files, these zero-filled holes will (likely) cause block checksums to not match, triggering the first error message.

                                                                                                                                                                                                                                     Manifest and log files, to which the second error above refers, use a slightly different on-disk format to aid in error recovery. Each write is represented by one or more "fragments" such that no fragment extends across a 32KB boundary. Whenever a write would extend across a 32KB boundary, it is broken up into at least two consecutive fragments, one on each side of the boundary. Any data discarded is most likely to be immediately prior to the 32KB boundary. When reading the log or manifest, the reader will miss the first fragment (it's all zeroes), but will see the second fragment, and complain that the first fragment is missing.

                                                                                                                                                                                                                                     The Fix
                                                                                                                                                                                                                                     The short term fix is to always sync data to disk before calling munmap. Our patch does this for OS X, but the fix could easily be applied on other systems, too. We're already in talks with the upstream LevelDB authors for a more portable solution to avoid further errors of this kind.

                                                                                                                                                                                                                                     Non-Fixes
                                                                                                                                                                                                                                     Since the bounty was posted, people have come out of the woodwork to propose various non-fixes to this problem. In this section, we explain why these proposals do not explain the behavior and why they do not fix it.

                                                                                                                                                                                                                                     Everyone thinks they're a real Sherlock Holmes.
                                                                                                                                                                                                                                     Apple's fsync is broken:
                                                                                                                                                                                                                                     That may be, but it doesn't explain the problematic behavior. Even an fsync call that truly does write outstanding data on a file descriptor to disk cannot fix this problem as the interaction between mmap and fsync is not well defined. Using an ioctl will not guarantee that dirty, unmapped pages will be written to disk.
                                                                                                                                                                                                                                     Missing memory barrier:
                                                                                                                                                                                                                                     Some people conjecture that the order in which memory barriers are defined can impact the bug. These memory barriers have absolutely no bearing on how sequential files are written because LevelDB uses the platform threading primitives for synchronization. Changing the definition order of these memory barriers will not affect the platform's primitives.
                                                                                                                                                                                                                                     Time Machine:
                                                                                                                                                                                                                                     Some people conjecture that Time Machine is responsible for these hundreds of NULL bytes at the end of page-aligned blocks. This explanation is too outlandish to consider -- this kind of an error with the Time Machine would have manifested itself in other programs, not just with LevelDB.
                                                                                                                                                                                                                                     Always Fsync:
                                                                                                                                                                                                                                     Some people suggest using the sync=true option for every write. Because fsync is called after the unmap, this cannot fix the issue.
                                                                                                                                                                                                                                     Bounty Hunting
                                                                                                                                                                                                                                     I first learned of this bug many months back in a manner unrelated to Bitcoin. Within HyperDex, we use LevelDB as a storage backend, and a few users have reported to us that they were experiencing corruption within LevelDB on OS X machines. I've been trying to find a live sample of this corruption for a few months. Because OS X is a development, but not production, platform for our users, it was hard to isolate a live test case.

                                                                                                                                                                                                                                     When the Bitcoin bounty turned up, I moved into bounty-hunter mode, partially because of the bounty, but mostly because I was hoping we could fix the issue for HyperDex users as well.

                                                                                                                                                                                                                                     Dog the Bug Bounty Hunter
                                                                                                                                                                                                                                     Bug Bounty Hunter:  42.5 minutes of one individual cursing at a computer screen.

                                                                                                                                                                                                                                     Early this morning, I contacted Gavin Andresen of the Bitcoin project to ask for a live sample and went to bed. When I woke, he had provided a sample and within three hours I sent the fix off to the LevelDB mailing list.

                                                                                                                                                                                                                                     Our motivation for finding this bug stemmed primarily from our desire to fix LevelDB for HyperDex. Fixing problems affecting the Bitcoin and Node.js communities is just a bonus.

                                                                                                                                                                                                                                     Where to From Here?
                                                                                                                                                                                                                                     We've identified the problem, and have proposed a fix that will immediately address the needs of the HyperDex and Bitcoin communities. So where are we going from here?

                                                                                                                                                                                                                                     We've got a fix for Google's LevelDB, which the Bitcoin community can immediately merge into their LevelDB code.

                                                                                                                                                                                                                                     We'll be publishing an updated HyperLevelDB containing a modification of our proposed fix.

                                                                                                                                                                                                                                     We look forward to working with the Bitcoin and LevelDB communities to integrate our proposed patch. In the future, we're available to address future issues reported within LevelDB.

                                                                                                                                                                                                                                     Further Reading
                                                                                                                                                                                                                                     Look Inside HyperLevelDB to see the improvements we've made to LevelDB performance.

                                                                                                                                                                                                                                     Checkout our patch on the LevelDB mailing list.

                                                                                                                                                                                                                                     Are you using LevelDB in your production application? We offer support and consulting for LevelDB, including bug fixes like the one we're providing today.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/11/27/bitcoin-leveldb/#sthash.mecXyvlX.dpuf


                                                                                                                                                                                                                                     Recall that we showed that what everyone thought about Bitcoin, namely, that the system is safe as long as 50% of the miners are honest, is false, and we outlined an attack on Bitcoin called selfish mining by which miners make more Bitcoins than their fair share.

                                                                                                                                                                                                                                     Selfish mining employs, but does not strictly depend on, races in the network, where the selfish miners propagate their blocks and try to outcompete the blocks propagated by honest nodes. Winning these races is not necessary -- a selfish miner that has sufficient mining capacity, 33% or above, could lose every single race and still make more money than an honest miner. But the more of these races a selfish miner can win, the more money he can make. Further, the higher a selfish miner's ability to win these races, the less mining power he can have and yet still outperform honest nodes.

                                                                                                                                                                                                                                     An interesting question at this point is: how likely is the selfish miner to win these races? To answer this question, Christian Drecker has set up a measurement framework that examines how fast blocks, and transactions, propagate through the network.

                                                                                                                                                                                                                                     The results are surprisingly in favor of selfish miners. It takes 4.5 seconds for a block to reach 50% of the miners. That's approximately an eternity in computer terms. That's plenty of time for a selfish miner to push his own block and win some races, especially if he makes his block smaller than average.

                                                                                                                                                                                                                                     The selfish mining attack looks more and more lucrative as more data comes in.

                                                                                                                                                                                                                                     Related
                                                                                                                                                                                                                                     This selfish mining simulator, developed independently by the Bitcoin community, confirms our findings that selfish mining attack is feasible.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/11/25/block-propagation-speeds/#sthash.x8Q6LLyQ.dpuf



                                                                                                                                                                                                                                     Welcome
                                                                                                                                                                                                                                     Welcome to my blog. There are many many others like it, but this one is mine. I'm a professor at Cornell who likes to build systems. My background is quite straightforward: I saw a computer for the first time when I was 13 and knew right then and there that these devices would revolutionize the world. I got my own Commodore-64 at 14 and have been writing software systems since then. I initially thought I'd study artificial intelligence and build ber-intelligent robots that would take over the world, then I realized that what the world desperately needed was computer infrastructure that worked, so I specialized in systems. In the process, I got a PhD, developed two research operating systems (SPIN in the '90s and Nexus in the 00s) and built countless distributed systems. I'm writing this blog because of a confluence of two trends, one positive, and one is a bit negative.

                                                                                                                                                                                                                                     The Positives
                                                                                                                                                                                                                                     Moore's original graph
                                                                                                                                                                                                                                     The underlying process that has placed us computer geeks at the forefront of a new wave of industrial revolution.

                                                                                                                                                                                                                                     The opportunity I saw back in the mid-80's in the computing field has not abated one bit. True, back then, few companies were using computers in their daily processes. Government offices required forms in triplicate, often created by pressing really hard on sheets of paper separated by carbon paper. Exams were written on a typewriter, often with little errors here and there, and replicated on a precursor to the Xerox machine that used a blue ink that smelled so fantastic that the first thing you did was typically to sniff the exam paper like a creepy printed word fetishist. It was clear that computers and automation would bring tremendous value, which they did. And even though we've had one computing bubble since then, and are likely in the midst of a second one, the opportunities are still there. Imagine how much better life can be with further application of computers. Yes, the easy automation has been done, but the fun stuff is just beginning. The fact that most of the world conducts business online now opens up more, not less, opportunities to new people entering the field.

                                                                                                                                                                                                                                     My pet theory is that every field that sits on top of an exploitable exponential process wields immense power in society. Physics wields power through fission and fusion. Medicine through the exponential process of pathogen growth. Finance through compound interest. And we geeks wield immense power through Moore's Law, an observation about an ever-increasing trend in the number of transistors that can be squeezed into a unit area. As an aside, it's worth sanity checking my claim by looking at fields like literature, music, sociology, mechanical/civil engineering and others that do not involve exploitable exponential processes. Use any metric for measuring societal power (funding, column inches, mind share, etc) and see where these fields land relative to physics, medicine, finance and computing. Also, note that my observation says nothing about the relative virtues of any of these fields. Some of the softer fields are deeply fulfilling, and finance is essentially pure evil when it is not useless. So if you're in a soft field, awesome! I love what you do, and please make it a Grande, no cream. For full disclosure, if I had to do everything all over again and could not pick CS for some reason, I'd actually be an English major (I draw a mean espresso as well). Anyhow, it is clear that computing will continue to wield immense power through the computational opportunities made possible by Moore's Law.

                                                                                                                                                                                                                                     And the specific field of distributed systems, as of this writing, offers many exciting challenges and opportunities. We're now able to build massively parallel hardware, but have not yet figured out the software infrastructure to make effective use of this hardware. There are healthy commercial ecosystems, where each large company including Google, Facebook and Amazon, has its own unique approach to how this infrastructure should be built. Academics, through testbeds like PlanetLab, EmuLab, VICCI and others, have the ability to experiment with distributed systems. The peer-to-peer wave, which came and passed, has paved the way to innovative deployments. In short, there are tons of opportunities with very few ossified, hard-to-penetrate components; an ideal scenario for bright young minds.

                                                                                                                                                                                                                                     The Not-So-Positives
                                                                                                                                                                                                                                     BSOD
                                                                                                                                                                                                                                     BSOD. This is why we can't have nice things.

                                                                                                                                                                                                                                     You might ask, given that there are a ton of tech blogs already, whether the world needs yet another one. Well, the jury is still out and it probably doesn't or, at least, shouldn't, but it needs something, because there is a lot of regurgitation out there in the blogosphere, and half the stuff being regurgitated doesn't even begin to make sense. There are many outdated ideas that are holdovers from an ancient era when the tradeoffs were different (and by ancient era, I mean a few years ago when processors had just a single core or disks had moving parts or whatever). There are dumb ideas being pushed by the industry to turn a profit. And there are a lot of falsehoods and muddled thinking that permeates the field. People build bad systems and they peddle broken software.

                                                                                                                                                                                                                                     And this has very real, tangible societal costs. Why did the Therac-25 fry people to death and how is this ancient system related to why a Windows box will someday also kill people? Why can't the FAA revamp an air traffic control system? Why did Twitter have to go through three revisions before it started to half-way work, and yet it still can't reliably produce a list of people I'm following? Why do people pony up millions of dollars for klunky database software? Why are all the purported modern replacements for klunky database software broken by design? How come so many people talk about RESTful design without anyone being able to define what it means? Who thought that standardizing syntax on XML without agreeing on semantics would solve any problem at all, and what are those people doing today? Why can I still watch, spoof and redirect everyone's Internet traffic when I'm at a cafe, or more interestingly, at a hospital? Who creates split-DNS setups, and who peddles these as a best practice? Could these people all be members of Iranian sleeper cells tasked with slowly but surely undermining our civilization? Perhaps, but more likely, what we're seeing is simply what happens when your regular, well-meaning developers slowly but surely make bad calls. They make bad calls partly because there are many falsehoods and myths out there.

                                                                                                                                                                                                                                     BSOD Trifecta
                                                                                                                                                                                                                                     But it's not like we're really trying to have nice things.

                                                                                                                                                                                                                                     I thought a little bit about where these bad ideas come from. Is there a social network pointing to sources of bad design? Can we mine the Facebook and Twitter graphs to identify such sources and banish them to an alternative network where every page has a BLINK tag? It's certainly true that some people are two sigma above the mean when it comes to generating bad ideas. For instance, the software architect who invented the treasure trove of ill-conceived design that was Object Linking and Embedding (OLE) (aka "failure model? what's that? network objects? not invented here, never heard of them!") also designed ActiveX (aka "security? what's that?"). And we all know about the petition to have Lennart Poettering to stop applying his reverse Midas touch to Linux (for those of you who missed it, Poettering had a hand in changing components, such as PulseAudio, HAL, DBUS, and the startup system in Linux, that didn't need changing and were broken for years as a result of said changes. This gave rise to various grassroots petitions for him to please stop).

                                                                                                                                                                                                                                     These exceptional people aside, the problems we face are by and large of a collective nature. Until recently, computer science was expanding so rapidly that it didn't pay to spend any time fixing up misconceptions or even errors -- if a bunch of people are doing something that's flawed, it's much easier, and certainly less confrontational, to just pick a different problem and avoid the shitshow altogether. But, over the course of a half-century, these misconceptions and errors accumulate, and distributed systems is chock full of them.

                                                                                                                                                                                                                                     And some misconceptions are actively propagated by people with very deep pockets. It occasionally crosses my mind that someone I love will end up at the mercy of a flight control or life support system designed by someone who read one too many blog posts about how to build scalable systems using eventual consistency. The blogosphere teaches programmers that because of CAP, one can't have in-flight audio available to everyone while telemetry is delivered consistently to the autopilot at the same time, and so one or the other has to go, and we know we can't let go of in-flight entertainment. Or something like that; it's hard to simulate the mental fog of someone who has been at the receiving end of industry pablum and the blogospheric echo chamber. You might think this is an exaggeration, but I've seen the developer conferences, I've watched the videos, I've read the code and I've seen things you people wouldn't believe.

                                                                                                                                                                                                                                     Hence, this blog. It was either this or start drinking. And I might yet choose the latter.

                                                                                                                                                                                                                                     Summary
                                                                                                                                                                                                                                     So, I plan to write about everything hacking related, especially about distributed systems, occasionally about designing hardware, and sometimes about the process of building things, but it will always be about building robust things that work. I'll write about exciting new developments in the fields of distributed systems, operating systems, networking and security. I certainly enjoy tackling technical myths and pointing out broken design, so there'll be quite a bit of that. I will try not to proselytize my own research too much, but I do plan to talk about other people's recent research that I find interesting. When I'm pressed for time, I tend to write longer, and I hate our newly emerging "tl;dr culture" on the Internet, so I will likely not be writing the usual 1-to-2-paragraph blog posts that are over before they've even begun. I hope some of the writings will challenge the way you think about building systems. I'll try to keep the writing accessible to a non-specialist audience, and I'll certainly have to take liberties with precision in the process. If you want rigor and precision, academic papers provide quite a bit of that and I have a CV full of those; my goal here is to stimulate that jump into relevant papers. And I do have a life, so if I can average about one semi-decent post per week, I'll be happy.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/01/17/intro/#sthash.sdfZdYSC.dpuf



                                                                                                                                                                                                                                     Ladislav Thon brought up a good point in response to my article on Mongo:

                                                                                                                                                                                                                                     You know, this all starts to make sense once you realize the original design goal of Mongo: a database for that kind of data where losing one record or two isn't a problem, but speed is crucial (i.e., web analytics)...

                                                                                                                                                                                                                                     Now Mongo is attacking the "system of the record" use-cases, and some of the original design tradeoffs are manifesting themselves.
                                                                                                                                                                                                                                     He's absolutely right: if all of your data has the property that you could lose some of it and still compute something meaningful, MongoDB is a harmless choice.

                                                                                                                                                                                                                                     But my endorsement has some subtle caveats that developers need to keep in mind:

                                                                                                                                                                                                                                     Mongo made its tradeoffs to gain speed. But it's not actually all that fast. There are faster systems out there that provide equal or better fault-tolerance guarantees. Wouldn't it make more sense to use one of those?

                                                                                                                                                                                                                                     There is no upper bound on how many records Mongo will lose at a time, so let's not be cavalier and claim that it'll only lose "one record or two." Your data may consist of discardable low value records, but clearly those records are valuable in aggregate, so you'll want some kind of a bound on how much data Mongo can drop for you. The precise amount of data Mongo can lose depends intimately on how you set up Mongo and how you wrote your application.

                                                                                                                                                                                                                                     So, how much data can be lost due to a single fault? I don't know (it's your code and set up, after all), but if you also don't know precisely what you can lose in one strike, it's probably time to look into alternative systems whose responses to failures are better characterized. Either that, or take the devops team out for a care-free night on the town, pagers off. For if you are running systems without bounds on data loss, it's not like there is a standard of excellence they're trying to meet.

                                                                                                                                                                                                                                     For Mongo to be an appropriate database, all your data has to be equally unimportant. Mongo is going to be indiscriminate about which records it loses. In web analytics, it could lose that high-value click on "mesothelioma" or "annuity." In warehousing, it could drop that high-value order from your biggest account. In applications where the data store keeps a mix of data, say, bitcoin transaction records for small purchases as well as wallets, it could lose the wallets just as easily as it could lose the transaction records.

                                                                                                                                                                                                                                     Pokemon
                                                                                                                                                                                                                                     You might be using that data store just to track the CEO's pokemon collection at the moment, but it can easily grow into the personnel database tomorrow. Could you live with yourself if you lost Pikachu?

                                                                                                                                                                                                                                     And it's not good engineering to pick a database that manages to meet an application's needs by the skin of its teeth. Civil and mechanical engineers design their structures with a safety factor. We know how software requirements change and systems evolve. So it would not be unwise, engineering-wise, to think about a substrate that can handle anticipated feature growth.

                                                                                                                                                                                                                                     So let us give onto Mongo what is clearly its: it's mature software with a large install base. If it loses data, it'll likely do so because of a deliberate design decision, rather than a bug. It's easy to find Mongo hosting and it's relatively easy to find people who are experienced with it. So if all your data is really of equal and low value, and you can afford to lose some of it, and your app's needs are unlikely to grow, then MongoDB can be a fine pick for your application.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/02/03/when-data-is-worthless/#sthash.fcehEh3I.dpuf


                                                                                                                                                                                                                                     We just released HyperDex 1.0RC3. There are major new features in this release:

                                                                                                                                                                                                                                     Ruby Bindings: Alright Rubyists, this is your moment. We now have officially-supported, well-integrated Ruby bindings for HyperDex. They are lean and mean.
                                                                                                                                                                                                                                     OSX Support: Everyone at your local cafe has a Mac these days, so simply owning a Mac just doesn't cut it for hipster-hacker cred. But a Mac running HyperDex, that's serious awesomeness. Now you can develop and test HyperDex on your Mac laptop.
                                                                                                                                                                                                                                     CentOS Support. We now have binary packages for CentOS 6, the OS that corporations love to designate as the official deployment platform.
                                                                                                                                                                                                                                     So, that's three fewer excuses to not adopt HyperDex. It works on your laptop, it works with most people's favorite languages (Ruby, Python, Java, .NET, C/C++), and it works on a ton of platforms (Macs, Ubuntu, Debian, Fedora, CentOS, Gentoo, plus source for all others). And the latest release gives you:

                                                                                                                                                                                                                                     ACID transactions. Move over Oracle, next-gen NoSQL has arrived.
                                                                                                                                                                                                                                     Fast retrieval by secondary attributes.
                                                                                                                                                                                                                                     Strong consistency guarantee: every GET sees the latest PUT.
                                                                                                                                                                                                                                     Fault-tolerance guarantee: no operation is complete until a specified number of copies are made.
                                                                                                                                                                                                                                     Plus, it has the clearest documentation in the NoSQL industry, with no need to learn arcane terminology or become intimate with internal implementation details to correctly configure a cluster, and with a tutorial that makes it easy to get started.

                                                                                                                                                                                                                                     It's ok to be skeptical of the many strong guarantees that the system provides -- it's certainly unprecedented. So, feel free to kick the tires and take it for a test ride. We're proud of what we built and we're sure it'll stand up to its claims.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/02/12/hyperdex1.0rc3/#sthash.jIRkBuUZ.dpuf

                                                                                                                                                                                                                                     What makes something a NoSQL data store? What are the quintessential elements that define the NoSQL movement? Will NoSQL forever remain a nebulous concept, in the same "I know it when I see it" category as pornography? [1]

                                                                                                                                                                                                                                     Little car with a big wing.
                                                                                                                                                                                                                                     It has both Bugatti and 747 features.

                                                                                                                                                                                                                                     These were the thoughts on my mind yesterday as MySQL 5.6 came out with a press release touting "NoSQL features." And what exactly were these much touted features? A memcached interface. For those of you who are not familiar with the memcached interface, here it is in all its 13-call glory. I can attach that to Excel, yet I will not end up with NoSQL.

                                                                                                                                                                                                                                     This whole marketing overreach is like boasting that your neon yellow Miata S2000, with the enormous after-market wing that you added with your buddies' help, has "Bugatti features." You just might feel a little rattle when you speed past 35. Funny how the real Bugatti owners never notice that at 150.

                                                                                                                                                                                                                                     Clarity through the marketing fog
                                                                                                                                                                                                                                     But we can't blame MySQL for trying to buy into the NoSQL hype by coopting a poorly defined term. If there is an overhyped area right now, it's clearly Big Data in general, and under that heading, NoSQL is the worst offender (leading by a nose over Hadoop).

                                                                                                                                                                                                                                     Now, there are lots of legitimate reasons for the excitement around this area. In fact, I would argue that the Big Data area is much more promising than what even the loudest voices in the press are proclaiming and forecasting. We really have lots of data, and we have an enormous shortage of tools that can keep up with the demand for functionality, scale and speed. It's an exciting space.

                                                                                                                                                                                                                                     But the excitement brings out the marketing engines, and they make so much noise that it gets difficult to tell apart traditional RDBMSs (think Oracle or IBM's System R from That 70's Show, the other one) from revolutionary NoSQL data stores.

                                                                                                                                                                                                                                     And we, the NoSQL folks ourselves, have been absolutely horrible at characterizing the area. If you look around with a critical eye, you hardly ever see anyone characterize NoSQL in positive, constructivist terms. The negativity is embedded in the name: No SQL. Well, my toaster has no SQL, so is it NoSQL? How can I tell? Not with that definition, I can't.

                                                                                                                                                                                                                                     It's not about BASE vs. ACID
                                                                                                                                                                                                                                     Beakers and graduated cylinders
                                                                                                                                                                                                                                     ACID vs. BASE? How about neither? All this foggy thinking and marketing speak has left me thirsty. Which beaker has the alcohol?

                                                                                                                                                                                                                                     Some people have come up with the acronym BASE to characterize the NoSQL movement, which contrasts nicely with the ACID guarantees of traditional RDBMSs. Catchy it may be, but it misses the mark by a mile:

                                                                                                                                                                                                                                     Basically Available: This term has no technical definition, but I think
                                                                                                                                                                                                                                     I know what it means. It's exactly like when I'm late to a meeting, and not just a little late, but late as in I-was-stepping-into-the-shower-around-the-time-when-you-guys-were-starting kind of late, and I call later from the car and say "On my way, I'm Basically There." Basically something is what we say when it's very much not that thing at all.
                                                                                                                                                                                                                                     Soft-state: Nice, a technical term whose definition
                                                                                                                                                                                                                                     we can all agree on. Let's just phrase it in layman's terms for Devops Borat: the data, it ain't safe in there. Better store it in something else if you want to see it again.
                                                                                                                                                                                                                                     Eventual Consistency: Ah, lest we forget, way to roll a specific
                                                                                                                                                                                                                                     and incredibly mediocre consistency model into the definition, because whoever came up with the acronym just cannot fathom a world where NoSQL stores actually provide strong guarantees.

                                                                                                                                                                                                                                     Face it: eventual consistency is no consistency at all. Data stores which never write to storage (e.g. by considering a write done when it's in the client's send buffer) are eventually consistent. Data stores that always read 42 are eventually consistent. And I do not mean either of these facetiously -- look up the technical definition if you don't believe me. When will you see your data again? Eeee-ven-tual-ly. I learn it from a book. A marketing book on BASE and NoSQL. Also, wikipedia.
                                                                                                                                                                                                                                     Wiki gets it wrong
                                                                                                                                                                                                                                     Jimmy Wales
                                                                                                                                                                                                                                     Jimmy Wales would like you to donate to Wikipedia, so we can have a clearer description of the most exciting thing to happen to data in 40 years.

                                                                                                                                                                                                                                     The definition of NoSQL in Wikipedia is pretty confused as well. Take a look at the first sentence:

                                                                                                                                                                                                                                     In computing, NoSQL (commonly interpreted as "not only SQL") is a broad class of database management systems identified by non-adherence to the widely used relational database management system model.
                                                                                                                                                                                                                                     For one, it's blatantly revisionist: NoSQL is not commonly interpreted as "not only SQL", not now and certainly at no time in the past. For another, it's a totally useless definition. A text file I open up in notepad also does not adhere to the RDBMS model, but it's not NoSQL.

                                                                                                                                                                                                                                     In Wiki's defense, someone has tagged the article for being out of compliance with Wikipedia's quality standards. Clearly, our community is way behind the curve -- even the hip-hop and punk rock pages have a better characterization of what their movement is all about.

                                                                                                                                                                                                                                     Towards a Proper Definition
                                                                                                                                                                                                                                     Let me, in a spirit of positivity on Valentine's Day, offer three useful observations that might help capture the NoSQLness of a piece of software:

                                                                                                                                                                                                                                     It's not the API: What sets aside NoSQL systems from RDBMSs is
                                                                                                                                                                                                                                     definitely not their interface. We're beginning to see NoSQL systems begin to offer SQL bridges, and we're beginning to see RDBMSs slap on NoSQL makeup on their APIs. So, whether a data store does SQL or not is just not the differentiating factor.
                                                                                                                                                                                                                                     It's not the features: What sets aside NoSQL systems from RDBMSs is
                                                                                                                                                                                                                                     definitely not the features or properties of the systems. Most are eventually consistent, but some second-generation NoSQL systems, like HyperDex, provide strong consistency guarantees. Some can lose data to failures, whereas others treat data gingerly even through disasters. Some are fast, and some have speedy-looking Cadillac wings but top out at 45 on the highway. So any attempt at finding a common set of features or properties, like BASE, is destined to fall short.
                                                                                                                                                                                                                                     It's the architecture: What sets aside every single NoSQL system
                                                                                                                                                                                                                                     I know from RDBMSs is their internal architecture. From 1970's until about 2000 or so, every database looked identical. It looked like a klunky centralized server, designed in a way where the server takes your data, forces you to declaratively specify what it is that you want, and it takes a "father knows best" attitude at giving it back to you. It does all kinds of things like computing "optimal" query plans to suck your data back off of the disks through the hose known as the disk head. NoSQL looks nothing like this. And that's why it's fast, that's why it scales, and that's why things that used to be easy for RDBMSs, like ACID transactions, are tricky on NoSQL.
                                                                                                                                                                                                                                     A Positive Definition
                                                                                                                                                                                                                                     My definition of NoSQL, which I tried to make as precise yet as inclusive as possible, is as follows: "NoSQL: a broad class of data management systems where the data is partitioned across a set of servers, where no server plays a privileged role."

                                                                                                                                                                                                                                     I think this definition accommodates every system I would characterize as NoSQL (e.g. HyperDex, Riak, Cassandra, MongoDB, etc), and it rules out foggy marketing speak. Feel free to pick it apart and to refine it, so that it may, one day, be better. Eventually. Or, ideally, within a time bound.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/02/14/whats-nosql/#sthash.NtMlDoIc.dpuf

                                                                                                                                                                                                                                     I've been reading a lot of NoSQL marketing material lately, and it's impossible to avoid noticing how many NoSQL data stores talk about their "partition tolerance." I want to bust three common myths about this buzzword:

                                                                                                                                                                                                                                     What the NoSQL industry is peddling under the guise of partition tolerance is not the ability to build applications that can survive partitions. They're repackaging and marketing a very specific, and odd, behavior known as partition obliviousness.
                                                                                                                                                                                                                                     Partition obliviousness is not a universally desirable property. It doesn't actually help you write partition tolerant applications. It often hinders the construction of robust applications. You probably don't want it.
                                                                                                                                                                                                                                     Partition obliviousness is not difficult to achieve, nor is it something to be proud of.
                                                                                                                                                                                                                                     Split brain
                                                                                                                                                                                                                                     There is a reason why lobotomies are not popular.

                                                                                                                                                                                                                                     Myth #1. In distributed systems literature, partition tolerance refers to a system's overall ability to live up to its specification in the presence of network partitions. So, if I'm implementing an online bank, and my datacenter network gets divided down the middle into two halves, my bank should continue to correctly debit your account and credit mine. If it's not doing this, then the bank suffers downtime, which is bad for business.

                                                                                                                                                                                                                                     Now, it's actually very difficult to build partition tolerant services. Think about what a bank could possibly do to facilitate a transfer if my record is on this side of the chasm and yours is on the other; namely, not all that much. In such cases, it is almost always better to build services that degrade gracefully under partitions. In this case, we would want failure detection and carry out those transfers where the accounts are both on the same side of the partition, while denying or deferring transfers that cross the chasm. If we do perform this failure detection and defer operations that cannot be satisfied because data is not available, sure, we lose some business, but at least we have a mostly working bank. We're not bankrupt, we didn't mess anything up, we're still a legitimate bank and we'll live to transact another day.

                                                                                                                                                                                                                                     But what most NoSQL systems offer is a peculiar behavior that is not partition tolerant, but partition oblivious instead. If I were to build a bank based on Dynamo, the granddaddy of all first-generation NoSQL data stores, it would silently split into two halves, like a lobotomized patient. Each of those two halves will then happily start serving data as if nothing happened, even though the worst possible kind of calamity just struck the datacenter. In this scenario, the hypothetical backend for Banko Dynamo would not only not provide any indication of failure, but allow a customer to create as many new accounts as there are partitions, one in each. You decide how great a feature this is and whether you want it. Perhaps you like manually reconciling diverged objects. Or perhaps you're too big to fail and Uncle Sam will make your customers whole.

                                                                                                                                                                                                                                     Headless chicken
                                                                                                                                                                                                                                     Miracle Mike, the partition oblivious chicken. Yes, it's alive. No, it's not a good role model for your applications.

                                                                                                                                                                                                                                     Myth #2. Partition obliviousness is a very odd and quirky property, one that most people would not want if it were branded properly. Imagine that we're doing a startup to topple the Great Satan known as Ticketmaster. We've built a buzzword-complete website with all the latest technologies, backed by a first generation NoSQL store that provides partition oblivious operation.

                                                                                                                                                                                                                                     Now assume that we have one of these partitions that the NoSQL salesmen are constantly talking about. The kind of partition that divides my datacenter down the middle and yet both halves are getting requests from the outside world. Do you really want to sell tickets from both halves of your system? By definition, there is no way you can guarantee uniqueness of those tickets. There will be customers holding identical tickets with identical seat numbers. If this is the kind of behavior you want, then great, because that's exactly what you'll get with partition obliviousness.

                                                                                                                                                                                                                                     NoSQL salesmen will tell you that they implement "healing." This refers to a simple process where, after the partition heals, they check to see if the objects diverged in the two halves. And if they did, the first-generation NoSQL stores usually take the ultimate punt by presenting all versions of the divergent objects to the application, and let the application resolve the mess. So, after the partition heals, and you figure out that you sold the same seat to Mr. Pink and Mrs. White, now what? They both printed their tickets long ago, donned their concert outfits and are at the coat check, moments away from a very unpleasant realization that they need to squeeze into the same seat. If instead of Ticketmaster, you were implementing an EBay clone, who won the auction and bought the last Furby? If you're running analytics, what happened to all your data from, say, the southern states, and how do you think the transparently missing data will torque your business purchasing decisions? You might be able to explain to your boss why your analytics ordered no grits this month, perhaps, but try explaining the quirky semantics for your brand of "partition tolerance" to crying kids who wanted that Furby. The data store may have tolerated that partition, but your application didn't; things got messed up.

                                                                                                                                                                                                                                     Of course, not every application resembles these e-commerce and analytics applications. If your NoSQL store only holds soft, inconsequential data, or the objects cannot possibly ever diverge, or the data store is write only, then it's absolutely not a problem to have multiple copies of the object evolve separately in each partition. If your service is embarrassingly parallel or embarrassingly replicable, first generation NoSQL stores are perfectly fine. But if your data is that soft and inconsequential, why not just use memcached? It's wicked fast, far faster than Mongo.

                                                                                                                                                                                                                                     Flintstone car
                                                                                                                                                                                                                                     The wheels don't turn and it doesn't really move, but that's why one can claim that the suspension offers a smooth and steady ride. It's not difficult to create a smooth ride for a non-moving car.

                                                                                                                                                                                                                                     Myth #3. A lot of NoSQL developers pretend that being partition oblivious is a difficult thing to implement. This is false. It's easy to make a program oblivious to a particular event; namely, you write no code to handle that event. If, by some mistake, you had an intern write some clever code for detecting partitions, you actively delete it. There, your system is now oblivious. Like every geek, I was oblivious to a lot of signals throughout high school; it came to me naturally, I had to undergo no special oblivion training, and, most importantly, and I cannot emphasize this enough, especially in hidsight about all the fun opportunities I missed: missing those signals wasn't a virtue.

                                                                                                                                                                                                                                     The thing that greatly helps first generation NoSQL data stores, the thing that enables them to package partition obliviousness as if it were equivalent to partition tolerance, is that they provide a very weak service guarantee in the first place. These systems cannot guarantee that, on a good day, your GET will return the latest PUT. In fact, eventual consistency means that a GET can return any previous value, including the Does Not Exist response from the very initial state of the system. Therefore, it's not a violation of their interface if, during and after a partition, they exhibit total amnesia.

                                                                                                                                                                                                                                     And that brings us back to the beginning, to the definition of partition tolerance. These systems are so weak that they can rebrand their oblivious behavior as partition tolerant -- it's not as if they are ever violating their own weak specification by returning bogus responses, for they gave you no guarantee in the first place. Most importantly, the data store itself may operate through partitions, but that does not mean it enables you to build applications on top that can do the same. If your application needs to uphold a strong service guarantee, these weak data stores will not necessarily help you provide that guarantee through a partition.

                                                                                                                                                                                                                                     Luckily, we know how to build stronger systems that provide better guarantees, and there are now second-generation NoSQL systems that do not suffer from these problems.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/03/07/partition-tolerance-myth/#sthash.c8Ih80NI.dpuf


                                                                                                                                                                                                                                     Alex Popescu disagrees with my post on the distinction between partition-obliviousness and partition-tolerance, and brings up an interesting discussion:

                                                                                                                                                                                                                                     The post presents a very dogmatic and radical perspective on what the requirements of both applications and distributed databases must be. I cannot agree with most of it if only for the reason its using the bank example.
                                                                                                                                                                                                                                     Clenched left fist
                                                                                                                                                                                                                                     Dogmatic radicals of the data management world, they might prevail! Or not! I wouldn't know because I'm not one of them.

                                                                                                                                                                                                                                     While I'll be the first to admit that I use colorful language and metaphors, my perspectives typically come straight out of Distributed Systems 101. Half the reason why I started this blog is that there is a big discrepancy between what people who have studied distributed systems know, and what is being said in the marketing materials of various vendors. And every time I say anything that is slightly different from the orthodox academic view, my colleagues jump on my case, like they did when I suggested that examples and tutorials could form part of a specification (Oh the heresy! I can see that I will never get a named chair at Cornell with that kind of talk. Keep on talking like this, and I'm destined for a lifetime of university committee work!). So call me anything, but a "radical" is just way off.

                                                                                                                                                                                                                                     Take the bank example. It is mentioned within the first 10 pages of every single database book I know of. It's part of our common DNA. To pretend that it does not exist, or that the NoSQL space is so different that we should forget what we know in terms of simple applications, seems odd.

                                                                                                                                                                                                                                     Now, Alex has a very good point, even though he's not as explicit about it as I would like; namely, many first-generation NoSQL stores are not meant for, and should never be deployed in any application that is trying to maintain a high-level service invariant in the presence of partitions. Why? Because their weak properties render them wholly unsuitable. Does that mean that NoSQL vendors are clearly admitting what Alex is saying? No, even Alex isn't explicit about the point he's conceding. Does it mean that Alex's message is well-understood in the developer community and people are avoiding such deployments? Sadly, not at all. Here's a whole lot of word soup, for instance, on how to use Couchbase to simulate "transactions". I'll leave it up to the readers to point out the different ways that the suggested pattern will fail, but the bottom line is that people are actively mis-deploying NoSQL solutions. The bank example is useful because everyone understands how it ought to work, and can see how Banko Dynamo will run into problems when it's partitioned.

                                                                                                                                                                                                                                     And to be clear, I provided some other applications as well. One was Ticketmaster. The other was EBay. Yet another was data analytics.

                                                                                                                                                                                                                                     Perhaps some view NoSQL as a niche solution that is forever limited to non-demanding applications. I disagree with this and believe that NoSQL has the potential to supplant RDBMSs and capture the bulk of the database market. The "daddy knows best" attitude that RDBMSs bring to data management, the way they strip all information that the developer had about her data and force her to write a declarative specification of what she wants, only to try to then come up with an efficient evaluation plan in the query optimizer, all reflect a klunky aesthetic to system design that the lean and mean NoSQL movement can and will supplant. NoSQL is to RDBMSs what Unix was to Multics. But if we are to see this happen in our lifetimes, we should not hesitate to think big. Why shouldn't it be possible for NoSQL to tackle the big boys and serve as the sole data store for Ticketmaster or EBay or Bank of America or Goldman? Sure, one cannot do this with Mongo, but there are alternatives.

                                                                                                                                                                                                                                     And even when we look at other, easier applications, we see the same problems; they just take a lot longer to set up and describe because they're not part of our DNA. For example, take a user discussion forum, like reddit. Processing a user post typically involves many lookups and multiple writes. Everyone realizes that there is one write to the subreddit and a corresponding update to the user's summary page. One might claim that, perhaps, it's quite ok for the user summary page to not be in perfect agreement with the discussion forum if the reddit cluster is partitioned. But what about duplicate URL detection? Post ID generation? Spam detection? The problem is that most people don't know how reddit checks for dupes or performs spam detection, or how it would do the same when it gets moved to NoSQL, so the examples get bloated. The bank example isolates the same issues without a complicated back story.

                                                                                                                                                                                                                                     It should be clear that almost every non-trivial application will have some high-level invariants to maintain through partitions. And a partition-oblivious data store, while it may itself operate through a partition, makes it difficult to build partition-tolerant applications on top.

                                                                                                                                                                                                                                     Michael Jackson's Doctor
                                                                                                                                                                                                                                     If you're a CTO and you use a partition-oblivious data store, you might need his assistance for a good night's sleep.

                                                                                                                                                                                                                                     And the real trouble with partition obliviousness is that it happens silently. If I'm running data analytics, and my NoSQL data store decides in the middle of an 8-hour run that some nodes are unreachable, do I even get to find out that I miscomputed my analysis? If I want to know when my computation is not computing the right results, am I a radical? By that logic, are you, the dear reader, not a radical?

                                                                                                                                                                                                                                     Let's take a benign scenario, one that most decidedly does not involve a bank or transactions: my team is currently working with a startup that has a very very very large data corpus. Their application issues a large number of queries and searches to this database. A partition-oblivious data store would return incomplete results, something that cannot be tolerated in this application. I can't imagine how their CTO could sleep well at night if a transient network failure during a long run could cause some GETs to fail silently.

                                                                                                                                                                                                                                     Alex says:

                                                                                                                                                                                                                                     NoSQL databases provide the knobs to tune the availability and consistency to the levels required by many applications. Applications can define more fine grained knobs on top of that.
                                                                                                                                                                                                                                     Lobotomy DIY
                                                                                                                                                                                                                                     One can always use various settings to improve a lobotomy.

                                                                                                                                                                                                                                     This is certainly true. In the limit, what you get from a first-gen NoSQL vendor is a data management library, and you can then use all sorts of settings to tweak its behavior. A system like Mongo has more settings than it has modules. This makes it difficult to pin down a discussion, and enables the salesmen to be even more mercurial than they normally would be. It's also true that computers are general-purpose Turing machines, and a careful engineer can take a NoSQL engine, no matter how scrappy, and tweak it (with the aid of settings, or in the limit, by patching it) until it more or less does what she needs. The problem is that these discussions tend to get fuzzy. Good engineers work with a well-understood substrate and they build guarantees on top; they do not handwave. Which is partly why I picked on Dynamo as the prototypical example; it has been published and documented, and therefore acts as a common, non-moving point of reference.

                                                                                                                                                                                                                                     As an aside, there are too many NoSQL systems, and too many settings for each, for me to be "fair" to each individual data store. Some Dynamo-clones, like Riak, are designed by careful engineers who thought deeply about their technological choices, and are sold by people who are careful to specify their limitations -- I suspect that they have happy users because their users are not surprised by sudden, catastrophic let downs of overblown expectations. And by no means is Riak alone, so if you're a good guy and I neglect to mention your system by name, sorry, the space is too crowded to do a thorough job. But the default in the industry seems to be to have documentation lying (pun intended) all around the web, with claims a mile high and the nudge-nudge wink-wink implications piled even higher. Yet the system implementations fall far short. Ask any developer if he understood that what is billed as a partition-tolerant data store is exactly what would keep him from building a partition-tolerant application, and we can then have a discussion over the responses.

                                                                                                                                                                                                                                     In my experience, when a system cannot definitively claim "we provide you with a guarantee on X," instead does a complex song and dance around the topic of X, and requires you to understand its internal implementation to correctly implement what you need, it usually fails at X altogether.

                                                                                                                                                                                                                                     In this case, X is partition management. And I think there's value in being first to point out the difference between the phrase used by salesmen to sell their system (partition tolerance), and the proper term for what the resulting applications end up experiencing (partition-obliviousness).

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/03/08/you-might-be-a-data-radical/#sthash.DPTrqNBu.dpuf


                                                                                                                                                                                                                                     My posts on the difference between partition-tolerance and partition-obliviousness (the original and the followup), seem to have scratched the surface of the great confusion that pervades most NoSQL-related discussions.

                                                                                                                                                                                                                                     Let me first do the constructive thing: since there seems to be a lot of confusion about Consistency and its various definitions, I wrote to clarify the C-word in a separate blog post. I hope this is useful to the community, especially developers new to NoSQL.

                                                                                                                                                                                                                                     Even though there was only one inappropriate response, let me now share the generic rules I use to detect when a technical discussion is headed in a fruitful direction, so we can avoid wasting time on dead-ends.

                                                                                                                                                                                                                                     Topic Drift
                                                                                                                                                                                                                                     Don Quixote
                                                                                                                                                                                                                                     Picking an appropriate target is a critical skill.

                                                                                                                                                                                                                                     Bad discussions drift away from the original topic towards well-trodden, hashed-out territory, until they reach the end predicted by Godwin's Law, or the bottom of this post.

                                                                                                                                                                                                                                     So if my posts on Partitions, with a capital P, evoke a response that is all about "eventual consistency," we know exactly where that discussion is going, namely, nowhere interesting. The C-word comes up only once in my first post, on a tangent when describing the backstory for how first-gen NoSQL stores can get away with what they do during partitions, and appears only in a quote from someone else in my second post.

                                                                                                                                                                                                                                     There have been countless religious battles between eventual consistency and stronger consistency models on developer forums, Quora, Reddit, HN, Wikipedia and just about everywhere else imaginable. (Though readers who revisit these discussions will probably be struck by how some of the loudest participants are also some of the least informed and bereft of even the most basic definitions). I have no desire to revisit these tired old arguments.

                                                                                                                                                                                                                                     We all realize that some people like to collect tired old arguments from religious battles and rehash them at the slightest opportunity. This is ok, but supremely boring. Anyone can re-live these discussions by reading the old ones. I will respond and link only to posts which materially advance the debate.

                                                                                                                                                                                                                                     Does Not Follow Fallacy
                                                                                                                                                                                                                                     No follow.
                                                                                                                                                                                                                                     Bad discussions create noise around a topic without making a logical case.

                                                                                                                                                                                                                                     My point was "X does not necessarily help Y, in fact it can hinder it" where X is a property of a database and Y is a property of an application that uses that database. Specifically, X here is "partition-tolerance," of the database. Surprisingly, Y is also "partition-tolerance," of the application.

                                                                                                                                                                                                                                     So it's confusing when I read a response that says "but, but, but X helps Z."

                                                                                                                                                                                                                                     Undoubtedly. There are lots of applications out there, and one should use a data store that serves the given application well. What is unexpected is that, even when X looks like a strong property on the surface, it may not be a good fit for the application, even when the property Y that the application needs sounds exactly like the property X that the database provides.

                                                                                                                                                                                                                                     Mixing of Colloquial and Technical Terms
                                                                                                                                                                                                                                     Bad discussions mix technical terms, such as the various Consistency incantations, with their colloquial definitions. Worse, they use terms with no definition.

                                                                                                                                                                                                                                     My Consistency Alphabet Soup post, as well as my NoSQL definition post, were made to avoid this kind of mixing and matching that causes people to talk past each other.

                                                                                                                                                                                                                                     Extreme Pedantry
                                                                                                                                                                                                                                     Bad discussions involve a participant who makes crazy, pedantic demands that serve no purpose.

                                                                                                                                                                                                                                     Insisting that I use the word "corpus callosotomy" instead of "lobotomy" makes the entire counter-argument unreadably cringe-worthy. Lobotomy stems from "split-brain operation," a technical term. But even if it did not, the pedantry involved, the need to correct something, anything, at any personal cost, would still induce a cringe.

                                                                                                                                                                                                                                     And no, I will not replace "crazy" in the topic sentence with an official DSM designation (301.7 with a dash of 312.34?).

                                                                                                                                                                                                                                     Emotional Reaction to Objective Reality
                                                                                                                                                                                                                                     Weak story bro.
                                                                                                                                                                                                                                     "Weak" has a technical meaning. Someone who does not know this meaning is not someone who should be debating its usage.

                                                                                                                                                                                                                                     Bad discussions involve a participant whose responses are driven not by logic fueled with passion, but instead by pure emotion.

                                                                                                                                                                                                                                     "Weak" is a technical term. Sometimes, all else being equal, it means desirable as in "our system makes weak assumptions." Sometimes, all else being equal, it means undesirable as in "eventual consistency provides weak guarantees compared to linearizability."

                                                                                                                                                                                                                                     Somehow, this technical term evokes an emotional reaction from people who do not understand that it is actually value-neutral. If a weak consistency model gets me performance, that would be great (turns out that it does not, but that's a separate issue).

                                                                                                                                                                                                                                     Luckily, most eventual consistency proponents are excellent engineers who understand the strengths and limitations of their consistency model. They will readily admit that EC can yield unexpected behavior. For instance, a developer who increments a value three times can have clients that observe, in order, "1, 2, 1, 2, 1, 2, Does-Not-Exist, 3, 2, 1, 3." EC is technically a weaker consistency model than, say, linearizability, which ensures "1, 2, 3," and nothing on earth can change this fact. The better companies in this space are those that are up-front about these properties.

                                                                                                                                                                                                                                     As a scientist and engineer, I am compelled to use the correct technical terms that correspond to reality. So if I call something "weak," it is not a value judgment. Non-scientists and overly emotional people will have to either learn to speak the technical language, or spend their life seething, needlessly.

                                                                                                                                                                                                                                     Ad Hominems
                                                                                                                                                                                                                                     Bad discussions quickly turn personal.

                                                                                                                                                                                                                                     Most people I know just walk away from any technical discussion involving ad hominems. The moment the discussion leaves the realm of the technical for the personal, whoever did that not only loses that particular argument, but loses credibility for ever after.

                                                                                                                                                                                                                                     Loaded Pigeonholes
                                                                                                                                                                                                                                     Baby girl in the air.
                                                                                                                                                                                                                                     You should see the spin I can put on a baby with my cleats.

                                                                                                                                                                                                                                     Bad discussions paint the other side into a corner that they do not occupy.

                                                                                                                                                                                                                                     This is typically a corner that involves beating wives, drop-kicking babies, or opposing modern version control systems such as git, hg and bzr.

                                                                                                                                                                                                                                     Modern version control tools have absolutely nothing to do with NoSQL. They are examples of applications which achieve partition-tolerance, but the way they do this, with the aid of a "C as in CAP" filesystem and without any help from a partition-tolerant database, is neither here nor there. Bringing them up on the NoSQL side undermines any point one might want to make about the partition-tolerance of first-generation NoSQL databases.

                                                                                                                                                                                                                                     But let's have the record reflect that I love motherhood, apple-pie and git as much as the next person. Perhaps some day I'll master rebase.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/03/22/rules-of-engagement/#sthash.kV6fdrO5.dpuf


                                                                                                                                                                                                                                     It seems that newcomers to distributed systems are often confused by the various definitions of the C-word. I want to go over them to draw a distinction that is often overlooked, without getting into the tired old arguments around eventual consistency. Specifically, I want to expand on the distinction between properties of a data store and the properties of applications built on top.

                                                                                                                                                                                                                                     The C in CAP, the C in ACID and the C People Care About Are Not the Same
                                                                                                                                                                                                                                     But first things first. I searched for clarification on the different definitions of the C-word. All I could find was a claim that the C in CAP equals AC in ACID (an incorrect claim), and a clipped Twitter exchange (in the right direction but too short). And Wikipedia is absolutely horrible when it comes to distributed systems. So, let's look at the different definitions together and discuss how they differ from the real consistency people care about.

                                                                                                                                                                                                                                     C as in CAP
                                                                                                                                                                                                                                     Ducks in a row
                                                                                                                                                                                                                                     C as in CAP is about getting one's ducks in a row, also known as linearizability.

                                                                                                                                                                                                                                     The "C as in CAP" is a narrow kind of consistency that pertains to how operations on a single item are ordered, performed and made visible by the database. Gilbert and Lynch, the theoreticians who formalized CAP, defined that capital C as "linearizability for a read/write register." A "read/write register" is an object in the datastore in this discussion, and what "linearizability" means, intuitively, is that all clients of a system would observe a uniform total order on operations on this object, with all operations taking place indivisably (atomically), and in accordance with an order that, loosely speaking, makes sense to the clients. Specifically, "makes sense" means that an operation that begins after another one has completed must appear later in the timeline. A linearizable data store behaves like a variable does in CS101; namely, a read will always return the value of the latest completed write (CS475, parallel systems, reveals that real memory systems are much more complicated than this, but also delves into mechanisms that cover up those complications to uphold this linearizability guarantee; these are beyond the scope of this discussion).

                                                                                                                                                                                                                                     A system that cannot guarantee linearizability is free to return values other than what a normal programmer might expect. For instance, reads may return values that were written quite some time ago, or the values might move forward and backwards in time, or they might disappear altogether. There are various names given to classes of such behaviors, called consistency models, where "eventual consistency" is one of the weakest.

                                                                                                                                                                                                                                     Needless to say, it's challenging to provide a linearizability guarantee in a distributed, replicated data store where messages can be delayed and failures can strike at any time. Many first-generation NoSQL stores explicitly forego consistency altogether, settling for eventual consistency. Such systems are, clearly, not "C as in CAP." In contrast, second-generation NoSQL systems, such as HyperDex, Google's Megastore and Spanner, provide linearizability. Every GET is guaranteed to return the results of the latest PUT.

                                                                                                                                                                                                                                     C as in ACID
                                                                                                                                                                                                                                     Acid trip.
                                                                                                                                                                                                                                     C as in ACID is rich, complex and can express anything related to one's current or past states.

                                                                                                                                                                                                                                     In contrast, the "C" in ACID refers to a broader kind of consistency that provides integrity guarantees that span the database itself. Whereas the "C as in CAP" relates solely to the ordering of operations on a single data item, the C in ACID refers to abstract properties that can involve any number of data items. "C as in ACID" is a strict superset of "C as in CAP," and it is a lot harder to achieve, as it requires coordinating reads, writes, updates and deletions involving arbitrarily large groups of objects.

                                                                                                                                                                                                                                     As an aside and a corollary to the definition above, a NoSQL system that claims to be "ACID-compliant on a per-item basis," or "ACID without transactions" makes no sense (and there are quite a few of these in claimed existence). A developer capable of building a trustworthy distributed data store would know better than to use that self-conflicting terminology. They really mean "C as in CAP plus D," but chances are that they are so confused about what they have implemented that it will come down to "I as in Inconsistent" and "DL as in Data Loss."

                                                                                                                                                                                                                                     Very few NoSQL data stores provide "C as in ACID" guarantees, as these guarantees necessarily involve a notion of transactions spanning multiple objects. Since NoSQL systems are based on a sharded architecture, transactions across multiple objects necessarily involve coordination across many nodes. Such coordination, if done via a centralized transaction manager, violates the main tenet of the NoSQL approach, which is to scale horizontally through sharding. If done via conventional distributed coordination protocols, such as Paxos consensus/2PC/3PC, the coordination overheads can be excessively high. But a few second-generation data stores use very interesting techniques to provide "C as in ACID" guarantees. For instance, Google's Megastore uses Paxos subgroups to achieve consensus on a total order. Calvin uses batching and dedicated ZooKeeper transaction managers for the same task. Google's Spanner uses mostly-synchronized clocks to determine when operations are safe to commit. And HyperDex Warp provides "C as in ACID" guarantees (in addition to "C as in CAP" guarantees) through a novel, high-performance distributed transaction protocol.

                                                                                                                                                                                                                                     The Difference Between the two Cs
                                                                                                                                                                                                                                     The difference between these C's is crucial. For instance, "C as in ACID" enables a CEO to say things like "the number of records in the personnel database should be N, the actual number of people I have hired." Or "number of cars produced should be less than or equal to the number of engines purchased." Or "at the end of the data analytics run, there should be a single summary object for each region, containing accurate statistics of sales figures." Or "every Bitcoin transfer that has been executed should be reflected in the corresponding users' wallets." In general, C as in ACID encompasses any kind of property that can be expressed over collections of records, where these properties are application-specific, user-defined, and checkable functions over total system state.

                                                                                                                                                                                                                                     A data store can be "C as in CAP" but fail to be "C as in ACID." A series of updates to the database that are individually "C as in CAP" can take the database through intermediate stages where they have been only partially applied to the data. Such intermediate stages may violate the "C as in ACID" requirements, even though they are applied individually to the data items in a "C as in CAP" manner. And these situations will not always resolve themselves. For instance, if one were to take a backup during one of those instances, resorting to the backup later might put the entire data store in a permanently bad state.

                                                                                                                                                                                                                                     Of course, if a data store is only eventually consistent, it is not C as in anything, it's EC. EC provides only a weak guarantee that, some unbounded amount of time after the system quiesces (assuming it does indeed quiesce), the system will converge on the state of the final writes.

                                                                                                                                                                                                                                     C as in what people Care about: Application Consistency
                                                                                                                                                                                                                                     So, the ever popular question: Should a data store provide C as in CAP, C as in ACID guarantees or just eventual consistency? The answer is the question cannot be answered as posed.

                                                                                                                                                                                                                                     The App and DB interfaces.
                                                                                                                                                                                                                                     Users, the only people who matter, perceive the invariants upheld at the blue boundary. The properties of the red boundary may limit that of the blue.

                                                                                                                                                                                                                                     The only thing that matters to a developer are the properties her application must possess, the invariants it must maintain at the application interface. For any real application, these invariants are typically very rich, very complex, and span a mix of both safety and liveness properties. They are certainly not limited to the constrained, and highly abused, alphabet one commonly sees in NoSQL-related blogs (e.g. Basically Available, Soft-state, Eventual Consistency, and Partition-Tolerance). Application-level invariants may span such issues as performance guarantees, bounds on failover time, reconfigurability, etc. They have a strictly richer vocabulary than what can be expressed through C as in CAP or C as in ACID.

                                                                                                                                                                                                                                     A CEO's application invariants may include such examples as "For every submitted video, there should either be a transcoded version suitable for mobile devices, or there should be a thread working on transcoding." Or "the clients never observe Bitcoin wallets that show old values, despite failures." Or "the data analytics run should complete within a few hours, tolerate faults, and always produce correct results." As these examples demonstrate, the invariants pertain to what the users see, and they span not only the state kept in the database, but the state and behavior of the application as well.

                                                                                                                                                                                                                                     Here's the main point: the properties of the database matter only to the extent that they are able to support application invariants, to the extent that the database can act as a suitable foundation for these rich application invariants on top.

                                                                                                                                                                                                                                     Of course, systems with weak properties typically cannot support applications that demand strong invariants. For example, if the red boundary does not provide the big P-word known as Performance, and the blue boundary needs to achieve high throughput, well, the app will then have to go get itself a different red layer that can provide the required performance.

                                                                                                                                                                                                                                     There have been countless debates on eventual consistency and what it's good for, whether it is desirable, and so forth. These discussions are tiresome and boring. And sadly, there are some people out there who have developed knee-jerk, previously-pickled and doled-out-at-the-slightest-stimulus kind of ready-made arguments for and against EC in the abstract. This kind of regurgitation misses the point: the action is at the blue boundary, the red stuff is involved solely by proxy.

                                                                                                                                                                                                                                     What is surprising to many people, based on the reaction to my posts on partition-tolerance and partition-obliviousness (original and followup), is that properties at the red boundary that sound incredibly strong do not necessarily translate into similarly strong properties at the blue layer. Something that sounds as fantastic as "partition-tolerance" in a data store can be the very impediment to achieving partition-tolerance in the application. Recall how almost every first-generation data store jettisoned a lot of desirable properties, purportedly in search of availability and partition tolerance (though I suspect they actually jettisoned the C as in CAP partly because it was hard and partly because they were seeking P as in Performance). Despite that tradeoff for partition-tolerance, the people who buy these systems to achieve partition-tolerance at the blue boundary by osmosis from the red might be in for a surprise.

                                                                                                                                                                                                                                     How To Pick A Data Store
                                                                                                                                                                                                                                     Accuracy and consistency.
                                                                                                                                                                                                                                     Until recently, the predominant attitude in the database community was that application developers cannot be trusted with data stores with weak properties. And the gold standard in data stores is ACID semantics. In all fairness to the database community, they collectively have many more decades of experience than any single company or developer. Their attitude makes sense because the DB community is driven by a desire to support as many applications as possible, and "C as in ACID" can express the widest range of application-level invariants on top. This is especially true for a tiered web application, where the front end code is transient and needs to use the back-end database for all its long-term storage needs. And further, the DB community has been around long enough to see that applications often have changing needs, where an application that starts out with a narrow mission with a small number of invariants balloons out over time to demand more. Solutions that are barely sufficient at the onset can turn out to be insufficient as software evolves.

                                                                                                                                                                                                                                     But I do not share this view, and have taken quite a bit of flak from my database colleagues as a result. I am not advocating that developers use a "C as in ACID" data store (even though my group built one), or a "C as in CAP" data store (even though my group built one as well -- HyperDex was first C as in CAP, now supports both C's). If someone can get by with an eventually consistent data store, power to them. A good developer will pick the tool that makes sense for her application. There are an infinite number of applications out there and some can operate well with no consistency guarantees. Some demand no fault-tolerance, as I discussed in the entry on when data is worthless. Some will never evolve.

                                                                                                                                                                                                                                     As I have said before, if someone has modest invariants to maintain at the application level, and they don't foresee their application changing much over time, a first-generation NoSQL store may be a perfect match.

                                                                                                                                                                                                                                     In practice, people often forego systems with strong properties in search of performance, the way they have been abandoning RDBMSs for NoSQL. We now have second-generation systems that not only provide stronger guarantees, but also outperform existing NoSQL systems.

                                                                                                                                                                                                                                     At the end of the day, everyone should pick whatever data store best supports the invariants their applications need to maintain. One could pick Riak for its Erlang or its management API or Cassandra for its durability. For all I care, people are even free to pick systems based on things like, say, how much noise they make in trade magazines, how pretty their logos are or how much schwag they give out (hey, even I have a Mongo t-shirt). But I will admit that I feel what the Germans call "fremdschmen" when I encounter cargo-cult engineering, and I will call out well-funded companies that encourage developers to make bad decisions. Part of my passion to write about NoSQL is to make sure that well-meaning engineers make their tech decisions for good reasons.

                                                                                                                                                                                                                                     Wrapping Up
                                                                                                                                                                                                                                     The letter C.
                                                                                                                                                                                                                                     To recap: the action is at the blue boundary. C as in ACID can support a wide range of application-level consistency properties that can be expressed over the current contents of the database. C as in CAP is useful when the application's invariants do not span more than one object. EC is the weakest possible property (where "weak" here is a value-neutral, technical term) that can cause time to move backwards where objects revert to an older value temporarily, or disappear and reappear; yet, even EC is a perfectly fine choice for applications whose invariants are not violated by such behavior. The main point is that, in the concrete here-and-now, what matters aren't the abstract properties of the red boundary, but what needs to be achieved at the blue boundary.

                                                                                                                                                                                                                                     There are some non-obvious effects, where seemingly good properties of the red boundary (say, partition-tolerance) can transfer themselves onto the blue one as a weak facsimile of themselves (e.g. partition-obliviousness). None of this is deep or even complicated for good engineers, but somehow the field has been characterized by religious arguments over murky definitions.

                                                                                                                                                                                                                                     In an ideal world, this would all be basic, well-established material we cover in undergraduate courses, but many of us have had to pick up this material from disparate sources, which use slightly different definitions for terms. And that has created a lot of confusion. The distinction between properties of the data store (red boundary) and properties of an application (blue boundary) seems to get short shrift, and causes even more confusion. I hope this overview is useful, and I look forward to discussions where all parties are using more or less the same terminology.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/03/23/consistency-alphabet-soup/#sthash.p38QCHgi.dpuf

                                                                                                                                                                                                                                     I've been away from blogging for a while, due mostly to a number of exciting technical developments here, and partly to non-technical things that kept me occupied for a bit. Let's see what happened in my absence:

                                                                                                                                                                                                                                     Bitcoin billionaire
                                                                                                                                                                                                                                     "Let's go to my pad where we can wade through my virtual cash."

                                                                                                                                                                                                                                     Bitcoin had an enormous rise and a partial fall, though it's holding its own at the moment. As the inventor of Karma, one of the first peer-to-peer currencies that preceded Bitcoin by half a decade, I have some thoughts about Bitcoin and how to value the Bitcoin economy. I guess this will have to wait for the next phase of excitement involving everyone's new favorite virtual currency. And no, unlike most academics, I do not believe that Bitcoins are worthless. But nor have I stocked my survival shelter to the gills with beans, ammo and offline bitcoins (By the way, here's my "CAP Theorem for survivalists": "brains, brawn, and booze: pick at most two." Guess which two I'm betting on. Bonus points for appreciating how precisely my theorem models the CAP theorem, where the "three-way tradeoff" is actually just a two way conflict between C and A. But I digress?). Anyhow, the thinking-person's analysis on Bitcoin needs to lie somewhere in between those two extremes and I look forward to discussing Bitcoin.

                                                                                                                                                                                                                                     There are a lot of exciting and major developments in distributed systems, both from my group and from my colleagues elsewhere. I plan to blog about some of the new results in distributed systems, as well as some cool hacks, in the coming weeks; some of them are ground-breaking.

                                                                                                                                                                                                                                     The Boston bombings had both me and the nation transfixed for a while. I have no analysis to offer on this front; not my domain.

                                                                                                                                                                                                                                     Some people just want to see the world burn
                                                                                                                                                                                                                                     Some people just want to see the world burn. Those same people want to see the world use inconsistent databases. These fine folks, some of whom are close friends, seem to have gone on a jihad against what they call the "bank example." Guys, by which I'm referring to people at a specific school, named after a famous philosopher, whose name starts with "B", and continues with "erkeley," I see that you're pretty upset about a particular example that is covered in every Databases 101 course at every school, including your own. Like every good example, it derives its power from being simple, stark, and stripped of all real-world accouterments in order to illustrate a point about transactions. Attacking the bank example because it's not realistic is like attacking Calvin and Hobbes because Calvin looks two dimensional and tigers can't speak. You've missed the point.

                                                                                                                                                                                                                                     Red dress at Gezi Park
                                                                                                                                                                                                                                     There was a people's uprising in yet another country. This one is squarely in my domain, not only because I was born there, or because it was engineered entirely by online systems and social media, or because it's possible to interpret social systems as distributed systems whose aggregate behavior can be predicted without being able to specifically predict the behavior of any single actor, but because it impinges on the question of "what happens when governments perform data mining en masse on their own people, press and companies?" Everything we're going through in the US has analogues elsewhere, and we can learn quite a bit from countries that are further along in the big-brotherification process.

                                                                                                                                                                                                                                     Out of the ballpark
                                                                                                                                                                                                                                     If she can stand for her rights, online and on the street, in a little black dress, without a gas mask, wearing open-toed shoes, and literally kick burning caustic gas canisters, any techie keyboard warrior can take a stance on political issues related to life online.

                                                                                                                                                                                                                                     There were significant revelations about the extent of eavesdropping and government surveillance. The debate about the tradeoff between perceived security and individual privacy is finally beginning to take place in the US. There is a lot to say on this topic, and everyone in tech should have an opinion. I realize that some people shun "taking a political stance." Being apolitical is regarded as a virtue among a certain class of technocrats. Yet, not having an opinion and refusing to take a stance on such a crucial topic is in turn a political stance, and quite a regressive one at that. Technologists are in a particularly gifted position, as they can see what would happen if certain capabilities were to be scaled up and taken to their logical conclusions. So we need to play a much needed public role in this emerging national debate.

                                                                                                                                                                                                                                     The kid is back
                                                                                                                                                                                                                                     The cult classic Buffalo-66 had many iconic lines. I particularly like and use "the kid is back!" My friends prefer "you know why they call you Goon?"

                                                                                                                                                                                                                                     I'm looking forward to catching up.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/06/08/back-to-blogging/#sthash.0gGXOCCx.dpuf



                                                                                                                                                                                                                                     LevelDB is a very popular data store that has been adopted in many domains far beyond its original use in Google Chrome. It's no surprise that many startups and large companies are using LevelDB: LevelDB internally stores data in contiguous chunks, leverages the performance of sequential disk I/O, and takes advantage of high performance buffer management in modern operating systems. This organization plays well with modern memory hierarchies and avoids conflicting with operating system decisions to yield high performance.

                                                                                                                                                                                                                                     While stock LevelDB is an excellent foundation, our experience with HyperDex identified several opportunities for further performance improvements. This article describes the changes we've made to LevelDB meet HyperDex clients' demanding needs.

                                                                                                                                                                                                                                     Specifically, these changes improve LevelDB in two ways:

                                                                                                                                                                                                                                     Increased parallelism through careful lock management
                                                                                                                                                                                                                                     Higher throughput through an improved compaction process
                                                                                                                                                                                                                                     This article describes these changes in detail to give an intuition behind why the resulting fork, called HyperLevelDB, is so much faster.

                                                                                                                                                                                                                                     Improved Parallelism
                                                                                                                                                                                                                                     Stock LevelDB internally protects all state with a single mutex, ensuring that multiple threads may safely write to the database simultaneously. Analogous to the global interpreter lock in Python, this mutex simplifies single-threaded execution at the expense of performance in a concurrent environment. On a modern processor with multiple cores, achieving high performance requires taking advantage of all cores. To this end, our first change was to crack this lock.

                                                                                                                                                                                                                                     Of course, this has to be done carefully to avoid potential data races. Let's briefly examine the concurrency requirements of stock LevelDB before describing the new organization under HyperLevelDB.

                                                                                                                                                                                                                                     Stock LevelDB provides internal synchronization to allow multiple threads to access the data store in parallel. Internally, writes proceed in FIFO order: the writer at the head of the FIFO is the only thread that may write into the database, and all others that are enqueued must wait to write. Every write inserts data into a disk-backed log and memory-backed "memtable". The log is an append-only file that provides persistence for crash recovery, while the memtable is a sorted skip list that enables reads to efficiently lookup data stored in the log. Writers must write data to both the log and the memtable before signalling the next writer in the FIFO.

                                                                                                                                                                                                                                     To overcome the queuing effect, stock LevelDB batches writes that encompass a prefix of the FIFO, writing the data for many writers in one batch. This batching improves the performance, especially for synchronous writes, but does not fundamentally alter the sequential nature of writes. It also incurs a cost, as the thread doing the batching must necessarily read, and possibly copy, the data being written by the other threads. Only the head of the FIFO is doing any real work, and the other writers are blocked by the kernel waiting for a signal from the head writer.

                                                                                                                                                                                                                                     HyperLevelDB Parallelism

                                                                                                                                                                                                                                     HyperLevelDB improves parallelism between writers by allowing all writer threads to independently insert their own writes into the log and memtable, using synchronization solely for maintaining the order of writes. To understand how HyperLevelDB can do this, it's important to understand the way reads and writes interact within LevelDB.

                                                                                                                                                                                                                                     Internally, every write within LevelDB is assigned a unique sequence number, generated by an ever-increasing counter. These sequence numbers enable readers to select data from a specific point in time, denoted by a sequence number for that point in time, and ignore any writes that happened logically after the sequence number. A LevelDB Snapshot captures the latest sequence number at the time the snapshot was created, enabling the snapshot to iterate or read the data without seeing any subsequent writes. Any read not performed on a snapshot will implicitly use the latest sequence number. These sequence numbers are the key to HyperLevelDB's parallel optimization, because they provide a way to control the data that is visible to readers.

                                                                                                                                                                                                                                     HyperLevelDB maintains the invariant that the sequence number chosen for a read will never rely upon writes that have yet to complete. The writers update the log/memtable in any order, without regard for their respective sequence numbers. After writing, each writer uses synchronization to update the global sequence number such that the writes appear to have been performed in FIFO order, which maintains the required ordering invariant. Each writer thread is responsible for independently updating the log and memtable -- which constitutes the bulk of the work -- and only uses synchronization to increment the sequence number used for future reads.

                                                                                                                                                                                                                                     To make the most of this new-found parallelism, HyperLevelDB modifies LevelDB's log and memtable implementation to safely permit concurrent insertions.

                                                                                                                                                                                                                                     Thread-safe memtable: The memtable is implemented as a concurrent skip list. Google LevelDB's skip list allows lock-free reads, and uses external synchronization for writes. Reads and writes share the same first step: traverse the skiplist to find the desired key in the list. Our thread-safe implementation does this traversal without holding any locks, and then validates the traversal, possibly traversing further, after acquiring the lock. This moves the most time-consuming part of inserting into the skip list outside of the lock, which allows all threads to perform it in parallel without sacrificing correctness. In our measurement using four writer threads, our improved skip list has a 99th percentile insert latency of 1.7us, while the default skip list has a 99th percentile insert latency of 2.6us.

                                                                                                                                                                                                                                     Thread-safe log: The append-only log is a file with a user-space buffer. Google's implementation always appends to the end of the file, maintaining a sliding window with the user-space buffer, using mmap and munmap on the file as necessary. HyperLevelDB's implementation also maintains the user-space buffer with mmap and munmap, but allows concurrent threads to atomically append to the file. The log synchronizes access to the underlying buffers, and assigns non-overlapping regions to different writers which allows the writers to copy their data in parallel.

                                                                                                                                                                                                                                     Improved Compaction
                                                                                                                                                                                                                                     LevelDB takes its name from the tiered storage structure it employs. Data is stored within levels, where each level contains more data than those below it. Within each level, data is sorted and stored in files called a sorted-string tables, or SSTs, where every SST is sorted, and SSTs are non-overlapping, so each key will reside in at most one SST. Writes are merged into the bottom of the tree, and a process called compaction moves data upwards through the levels, maintaining the sorting invariants within each level.

                                                                                                                                                                                                                                     The incremental compaction used within LevelDB represents a trade-off between two extremes. Without any compaction, data written to the system remains unsorted, requiring linear access to all data to perform a key lookup. Alternatively, compaction could be taken to the extreme wherein it sorts everything, generating ever-larger SSTs, and rewriting the entire data on disk with every compaction. Compaction in LevelDB ensures that data remains sorted, increasing read efficiency, while performing incremental maintenance to limit the amount of data that a compaction will write.

                                                                                                                                                                                                                                     The downside to any form of compaction is that it inherently suffers from a form of write amplification where the amount of disk I/O is a multiple of the amount of data written to LevelDB because data will be rewritten at each level of the tree. The exponential difference in size between levels all but ensures that an SST at Level-X will overlap with multiple SSTs at Level-X+1. In our experience, the default LevelDB implementation will rewrite three bytes from Level-X+1 for every byte written from Level-X. Only one quarter of the I/O generated by LevelDB is doing productive work; the rest is wasted I/O and a target for optimization.

                                                                                                                                                                                                                                     The many approaches one could take to reducing this wasted I/O include:

                                                                                                                                                                                                                                     Tuning the compaction-related constants using empirical analysis to reduce write amplification.
                                                                                                                                                                                                                                     Leave data unsorted in lower levels to avoid linear merge costs at the expense of read latency.
                                                                                                                                                                                                                                     Pick compaction targets (SSTs) that explicitly minimize write amplification.
                                                                                                                                                                                                                                     Of these three approaches, only one directly tackles the problem of write amplification. The first two techniques require manual, hands-on tuning to find a performance sweet spot for a given workload. We found such tuning to be fragile and unsatisfying because it was sensitive to the workload. HyperLevelDB follows the third approach and provides a principled redesign of the compaction process.

                                                                                                                                                                                                                                     Write-Deamplified Compactor

                                                                                                                                                                                                                                     HyperLevelDB's altered compaction algorithm explicitly chooses a set of SSTs which result in minimal write amplification between two levels. A background compaction thread monitors the size of each level, and compacts using the optimal set of SSTs for compaction. A second background thread, which triggers when the first thread becomes overwhelmed, performs global optimization to select compaction targets with low write amplification, without regard for the size of the individual levels. Typically, this second thread will move files from the lower levels without any write amplification. For the upper levels, the second thread will preemptively compact data often enough to keep them under the size-limit heuristics used by the first thread, ensuring that the first background thread will not be tied up in compaction at the upper levels.

                                                                                                                                                                                                                                     Summary
                                                                                                                                                                                                                                     HyperLevelDB improves LevelDB's performance through two significant redesigns of its architecture. The resulting fork is a factor of four faster than stock LevelDB.

                                                                                                                                                                                                                                     If you're using LevelDB in production, feel free to check out HyperLevelDB (Get the code!). It's licensed under the same terms as LevelDB, maintains API compatibility, and offers improved performance.

                                                                                                                                                                                                                                     If you've rolled a custom distributed system on top of LevelDB, you' should check out HyperDex, which provides the benefits of LevelDB in a consistent, fault-tolerant distributed data store.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/06/17/hyperleveldb/#sthash.8w2R5Ktk.dpufi

                                                                                                                                                                                                                                     The intelligence community has been harping on the word "metadata" to try to underscore that the information they collected is not quite "data", is not subject to the same limits, and is not quite as bad. I want to put an end to this charade, by way of an analogy.

                                                                                                                                                                                                                                     An Analogy
                                                                                                                                                                                                                                     Card catalog
                                                                                                                                                                                                                                     Suppose I want to understand everything there is to know about the lightning bugs in my back yard. They are pesky, ugly little insects, and their numbers fluctuate wildly. I want to understand what they are doing and what makes them tick so I can eradicate them.

                                                                                                                                                                                                                                     If I were a molecular biologist, my starting point would be that the data I want is a bug's DNA. Surely, the DNA describes the behavior of an individual completely, so it makes sense that I would want to acquire it and figure out everything a single sequenced individual will do. If the bug DNA is "data," then all other information about how many bugs there are in my backyard, their gender, their swarm sizes, their flashing frequencies and so forth would simply be "metadata." It's nice to know that stuff, a molecular biologist would say, but clearly it's not as critical as the DNA sequence, and therefore shouldn't be subject to the same care and attention as DNA samples.

                                                                                                                                                                                                                                     But if I were an old-school entomologist, like my real-life neighbor who is a retired professor, I would claim that the DNA data is really of no consequence. Humankind had thousands of years of bug eradication long before DNA was discovered let alone sequenced, and can do so again. All an entomologist would need is time-series data on population counts, gender, swarm sizes, flashing frequencies and so forth -- what was metadata for the molecular biologist is data for an entomologist. He would use this data to figure out and manipulate their behavioral patterns so as to drive down their numbers. To an entomologist, the "metadata" would involve the circumstances around his real data of interest. The location of my backyard, the types of other plant and insect species on premises, the weather patterns and so forth would constitute metadata for this individual.

                                                                                                                                                                                                                                     And if I were a dyed-in-the-wool ecologist who had to suffer through grad school along with a lot of traditional bio-geeks who've never encountered an animal too disgusting to dissect, I would say that all this attention is being wasted on individual organisms. Give me a species interaction graph and a census of all the species in the yard so we can figure out how the bug population will behave over time. We'd introduce competing species and drive down the numbers. The entomologists' metadata would be my data.

                                                                                                                                                                                                                                     Meta Depends on Your Point Of View
                                                                                                                                                                                                                                     Card catalog
                                                                                                                                                                                                                                     One could go on. Clearly, what constitutes data versus metadata is determined not by any intrinsic property of the data itself, but by the questions that that data is meant to answer.

                                                                                                                                                                                                                                     Let's examine what it is that the intelligence community wants to do with phone call records and online activity logs to see if it fits any kind of meta designation.

                                                                                                                                                                                                                                     The contents of phone conversations are clearly important. If our goal is to stop an immediate attack, a voice that says "attack at dawn" is what we want to catch. And this is the imaginary scenario that the intelligence community will play up. But if our goal is to investigate a network, to find out who is related to whom by what degree, and what their usual communication activities are, then the call log "metadata" is very much the actual data we seek. It is not one-step removed; it is the very thing and the only thing we want. If we're doing anomaly detection or community discovery or determining some kind of a simplistic color-coded terror alert level, we'd be able to do our analyses solely with metadata.

                                                                                                                                                                                                                                     The "meta" designation is really an attempt to denigrate the value of the data at stake, to insinuate that this data is one step removed from that which we want, and to subtly insist that it should therefore be subject to less scrutiny.

                                                                                                                                                                                                                                     Meta is Often More Valuable
                                                                                                                                                                                                                                     Yet metadata is often far more valuable than so-called data itself.

                                                                                                                                                                                                                                     Take, for instance, the NSA's current predicament following Snowden's leaks. What Snowden leaked was information about the information that the NSA collected. Since NSA calls the latter "metadata," this makes Snowden's leaks meta-metadata. I don't need to belabor how damaging the leak was for the NSA, even though it's supposedly twice removed from "data."

                                                                                                                                                                                                                                     And going further, here's the NSA's response to a FOIA request, explaining why revealing the presence or absence of some metadata (which would be metametadata) would cause grave harm to the United States, because it would reveal information about the capabilities of the NSA. We're veering off to cubic-meta territory here.

                                                                                                                                                                                                                                     Card catalog
                                                                                                                                                                                                                                     There have been narrow legalistic arguments between legal scholars about the privacy guarantees over call records. While it's futile to try to keep lawyers from discussing arcane legalistic definitions, these discussions all miss the point. Simply put, the public finds it creepy for the government to track their lives, their interactions and their overall behavior at that scale and in that fashion. Jane Average can turn a blind eye towards evil, unwarranted or even illegal activities on occasion, especially if they take place overseas, but a domestic creeper is a hard sell to families.

                                                                                                                                                                                                                                     So the intelligence community, which never met-a-data that it didn't want to collect, should drop the whole metadata charade. The discussion should not be about legalistic definitions. It should be whether or not collecting this particular information, for the particular purpose of massively cross-linking and analyzing it, at this massive scale, is at odds with our values.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/08/02/metadata/#sthash.K7mDSlMx.dpuf


                                                                                                                                                                                                                                     There is a new, careful, non-partisan study from the ITIF out today that has focused specifically on the question of how much the computer industry would lose due to privacy concerns. And their estimate turns out to be $35 billion over 3 years. That comes out to about $12B per year on average.

                                                                                                                                                                                                                                     This matches well with my recent estimate. When I outlined a framework for the forces that affect Internet policy, I did a quick back of the envelope calculation for the strength of each force vector. My guesstimate was that the amount the computer industry stood to lose was about the same as what the NSA was spending on broad surveillance, and that was a number in the "very low double digit billions."

                                                                                                                                                                                                                                     So, to the extent that this new estimate is accurate, it corroborates my previous guesstimate. Of course, such numbers are fundamentally unknowable, and the cascade effects of our cloud companies losing their worldwide dominance could well incur long-term costs that far exceed double-digit billions in the long run.

                                                                                                                                                                                                                                     One twist is that the costs are not uniformly distributed over the three year time span. Instead, they are back-loaded, as there is quite a bit of inertia for cloud customers to switch providers. The losses in the first year are only a few billion, while the losses in the third year are on the order of $20B. So, expect the computer industry to take an increasingly pro-privacy stance as its bottom line takes an ever greater hit.

                                                                                                                                                                                                                                     Overall, the good news is that the numbers, no matter how one looks at them, are sufficiently high that the computer industry is motivated to intervene. And they have already held one secret meeting with Obama to voice their concerns. That's the force vector in action, pulling the Internet towards a freer, better place. Let's hope that this evenly matched tug of war yields some tangible results soon.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/08/09/back-of-the-envelope/#sthash.j1eIqod3.dpuf


                                                                                                                                                                                                                                     The NYTimes asks the question Is Big Data an Economic Big Dud and gets the entire discussion wrong in three critical ways.

                                                                                                                                                                                                                                     Terrible Definition for Big Data
                                                                                                                                                                                                                                     Not everything that happens online or in the cloud is related to Big Data. In particular, the article seems to equate YouTube videos with Big Data. The word "data" has essentially become a meaningless epithet applied to all things digital. This is a horrible practice perpetrated by people who lack a tech background. So here's a handy guide for all the journalists who shunned techie courses in favor of the fun times spent taking humanities courses.

                                                                                                                                                                                                                                     The way to decide if something constitutes "Big Data" is to ask if it's data, and if so, how big it is. Just because something is digital, or because it is the input to a computer program does not make it "data" in the "Big Data" sense. The question is: What is the information content here, and how big is it?

                                                                                                                                                                                                                                     Quantifying information content is actually a complex and deep subject, the domain of an intriguing field called Information Theory, but I'll skip the complexities of Info Theory, not mention Shannon, avoid defining entropy, and describe a simpler technique that anyone can use: simply summarize in English what information is encoded in the data.

                                                                                                                                                                                                                                     For instance, imagine the universe, so rich and full of information that it defies a summary. The universe, or even smallish fractions of it that hold some of its inner workings that we cannot currently summarize with our knowledge base, would be the subject of Big Data. Imagine the data collected by the Square Kilometer Array, petabytes collected per day through thousands of radio receivers sprawled over an enormous chunk of land; that's Big Data, the information collected by these radiotelescopes might even contain traces of other civilizations. Imagine all the biomolecular information collected at various laboratories around the world; again, Big Data, holding secrets to countless drugs. Imagine the information encoded in all books published since the 1600's; Big Data. Imagine the information embodied in the movements of humans, and how it encodes all sorts of complex phenomena; Big Data.

                                                                                                                                                                                                                                     Now imagine a dumb cat video on YouTube. It can be summarized in under 14 bytes as a "dumb cat video." It's not Big Data, no matter how many times it is downloaded.

                                                                                                                                                                                                                                     Terrible Metric for Impact
                                                                                                                                                                                                                                     The metric that people use for economic impact is often GDP. Indeed, the article measures Big Data's impact by how much it grows the economy. Yet even economists will admit freely that the GDP is at best a misleading metric for progress.

                                                                                                                                                                                                                                     The textbook case that illustrates the failings of GDP as a progress metric involves the "broken windows" example. If someone were to go around breaking perfectly fine windows, the GDP of a country would increase as everyone has to buy replacement glass, but almost no one, save for a few glass vendors, would be any happier or better off.

                                                                                                                                                                                                                                     The GDP is an especially misleading metric for Big Data, as Big Data is often used to improve business efficiency, which is more likely to shrink the economy than to grow it in the short term. Imagine that Target studies the buying patterns of its customers so well that it only ships precisely as many items as will be sold, precisely on time -- the net effect will be a reduction in GDP!

                                                                                                                                                                                                                                     Terrible Product Placement
                                                                                                                                                                                                                                     There is a "submarine" below the fold, where there is a reference to a small player in the Big Data space. Even though the company is actually all about improving efficiency, the article doesn't pick up on this fact, and goes on to talk about GDP growth, leaving the reader wondering why this reference was dropped in the first place.

                                                                                                                                                                                                                                     There are countless companies in the Big Data space. Surely, the NYT can afford a few phone calls.

                                                                                                                                                                                                                                     Terrible Framework
                                                                                                                                                                                                                                     Overall, the discussion falls far short of the mark. Big Data, properly defined, is clearly not a dud -- it has the potential to improve our lives in immeasurable ways, through drug discovery, individualized medicine, more efficient business practices, and many others in all aspects of science, every branch of engineering and even, with the resurgence of quantitative methods in the humanities, in liberal arts. But if we let the word get misappropriated, if we blithely apply it to everything digital such that the term loses its meaning, kind of like IBM's erstwhile "autonomic computing," then it is guaranteed to become a meaningless bandwagon that denotes whatever the author feels like that day, and it is guaranteed to fall short of expectations.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/08/18/big-dud-on-big-data/#sthash.WUlQ2sYA.dpuf


                                                                                                                                                                                                                                     For some time today, the NYTimes web site was being directed to malicious servers that were serving malware.

                                                                                                                                                                                                                                     At the moment, nytimes.com does not resolve at all for me. The NYT is off the web.

                                                                                                                                                                                                                                     This is a collosal screwup.

                                                                                                                                                                                                                                     This email sent to the dns-ops mailing list provides some insight into what may have gone wrong and illustrates some of the many things wrong with our current name resolution infrastructure.

                                                                                                                                                                                                                                     from: david@from525.com

                                                                                                                                                                                                                                     to: dns-operations@mail.dns-oarc.net

                                                                                                                                                                                                                                     date: Tue, Aug 27, 2013 at 5:55 PM

                                                                                                                                                                                                                                     subject: [dns-operations] Request To Clear Cache: NYTimes.com

                                                                                                                                                                                                                                     All,

                                                                                                                                                                                                                                     I am a DNS Administrator at NYTimes.com. Earlier today we had issues with our registrar updating our NS records on the root servers to a malicious site. The registrar has since locked our domain with the registry on our proper Name Servers. We have had reports that the malicious site that our domain was redirected to was infecting users with malware. It would be a great service to the internet if everyone could please clear their cache for NYTimes.com. I understand that several other large websites/domains are experience the same thing. I would not be surprised if several request like this come in over the list today.

                                                                                                                                                                                                                                     Thanks, David Porsche Systems Administrator The New York Times
                                                                                                                                                                                                                                     Some immediate observations and questions, taking the claims in the email at face value:

                                                                                                                                                                                                                                     Why is the legitimate owner of a domain name unable to
                                                                                                                                                                                                                                     exert control over name resolution? Why is any registrar in a position to wreck this kind of damage in the first place?
                                                                                                                                                                                                                                     How or why did the registrar decide to redirect the NYTimes
                                                                                                                                                                                                                                     nameservers to a malicious host? Was it the victim of social engineering, the result of a compromise, or an insider attack? If it was a compromise, did it take place on the registrar side or the client side?
                                                                                                                                                                                                                                     Would DNSSEC have helped?

                                                                                                                                                                                                                                     What operational procedures, if any, would allow a registrar
                                                                                                                                                                                                                                     to avoid this kind of an error? If the answer is "nothing at all," is it possible to devise alternative name resolution protocols that can limit the damage registrars can do?
                                                                                                                                                                                                                                     Why is a desperate plea from the domain owner necessary to fix the
                                                                                                                                                                                                                                     problem? Why should a one-time, short-duration attack get amplified into a long-standing vulnerability, and require human intervention around the globe to fix?
                                                                                                                                                                                                                                     Most of these questions are rhetorical, and everyone knows the rather depressing answers.

                                                                                                                                                                                                                                     Edit
                                                                                                                                                                                                                                     False flag.
                                                                                                                                                                                                                                     The flag, it is false.

                                                                                                                                                                                                                                     There are claims now that the problem stemmed from a malicious external attack by hackers working for the Syrian Electronic Army. They supposedly took advantage of vulnerabilities at MelbourneIT. It's funny how we have such few details on what precisely happened, yet we already know who to blame. And it's a politically convenient group.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/08/27/dns-nytimes/#sthash.oqIGVH6f.dpuf


                                                                                                                                                                                                                                     NPR's All Things Considered mocks NSA for using their considerable listening and tracking apparatus to spy on love interests. This practice has spawned some NSA pickup lines on Twitter. Some choice ones:

                                                                                                                                                                                                                                     Do you believe in love at first sight, or should we switch on your webcam again?

                                                                                                                                                                                                                                     Girl, you must have fallen from heaven because there is no tracking data to indicate how you arrived at this location.

                                                                                                                                                                                                                                     I know exactly where you have been all my life.

                                                                                                                                                                                                                                     Roses are red, violets are blue, your pin number is 6852.

                                                                                                                                                                                                                                     Surveillance cameras
                                                                                                                                                                                                                                     Smile!

                                                                                                                                                                                                                                     It's clear that the public finds NSA's practices to be creepy. And from a PR perspective, "creepy" is absolutely the worst adjective to try to fight. Even pure evil, like detaining people indefinitely in concentration camps, torturing prisoners and making naked human pyramids, is easier to contain for a PR engine -- it's just a matter of depersonalizing the "Others."

                                                                                                                                                                                                                                     As an aside, this notion of the "Other," the enemy, the evil, was once predominant in French literature. We had a resurgence of the same concepts here in the US recently as the US was occupying Afghanistan and Iraq. Just as an example, the TV series Lost spent countless hours demonizing a set of people with strange, unknowable customs, with values completely opposed to our own, with concerns and motivations completely unlike us and indecipherable by us. And once there is a ready-made concept of the "Other", the "less than humans," then all sorts of evil activities can be justified, because after all, who knows what those people will want to do next. You can shoot them first before they draw, waste them before they waste your time explaining why they are innocent and you can even kill their kids before they grow up to be Others.

                                                                                                                                                                                                                                     But these rhetorical tricks do not work to justify "creepy" activities targeted at us. Once soccer dads and moms decide that they don't want their children's activities tracked by creepy men or their spouses harassed by creepy ex'es, the PR battle has been lost. It'll be fun watching the intelligence community try to come up with creative narratives to justify their practices.

                                                                                                                                                                                                                                     Related
                                                                                                                                                                                                                                     My analysis of how the Snowden saga will play itself out.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/08/27/nsa-pickup/#sthash.r1RRR9k2.dpuf


                                                                                                                                                                                                                                     Every passing day reveals further revelations about the extent of NSA's extreme eavesdropping on Americans. There is hardly any aspect of our online lives that the NSA seems to view as sacrosanct as it trawls the network for information on our activities. This attitude is best demonstrated by the NSA chief, who seems to think that Big Data equates to Big Carte Blanche, as if Moore's Law somehow gave him authority to override constitutional guarantees just because it made it possible to do so.

                                                                                                                                                                                                                                     But what I want to focus on is the computer industry's reaction to this ongoing train wreck. Specifically, I want to point out just how tepid and wrong-headed the industrial reaction has been so far.

                                                                                                                                                                                                                                     Transparent but not healthy.
                                                                                                                                                                                                                                     It's good for him because it's transparent.

                                                                                                                                                                                                                                     For the industrial reaction consists mostly of a call for transparency. What Facebook, Yahoo and Google seem to want to do is to be "more transparent" about the extent of their overt cooperation with the NSA. They seem to believe that by revealing the number of official NSA requests for information they have received, the public will go "oh, looks like there were only 8,000 FISA-related information requests this quarter. Awesome, the chances that I am one of those are pretty low!"

                                                                                                                                                                                                                                     I guess the thinking here is that the public just loves the word "transparency." And it's true, Joe Average Sixpack seems to have an infatuation with transparency these days. He thinks there is nothing on earth that would not improve with an extra added dose of transparency. It's like these people get their political philosophy from the pages of Victoria's Secret catalogue, where the only virtue is to be more revealing.

                                                                                                                                                                                                                                     Interestingly, universities are the most transparently-run organizations we have, and ask anyone who has worked at one: just because a process is transparent does not mean it is right or good. We've all watched monkeys through the transparent glass of a zoo enclosure; it didn't make the monkeys any more sentient or better behaved.

                                                                                                                                                                                                                                     Designing laws, or political systems, is a lot like designing distributed systems: the forces (a.k.a. checks and balances) have to line up so that there is no single unopposed force taking the system towards a messy state, one possibly involving a right arm in the air, combovers, a cute little moustache, and guys carrying big torches on a crystal night. Transparency is not a necessary or sufficient ingredient for a balanced, reasonable outcome; what is necessary is a force that can push back on the intelligence apparatus, one that can say "no, you may not collect this data, the reasoning you have for collecting it is specious."

                                                                                                                                                                                                                                     Transparent but not healthy.
                                                                                                                                                                                                                                     She may seem like a creepy stalker, but no worries, the wrap is fully transparent!

                                                                                                                                                                                                                                     In fact, transparency can be a detractor that serves to hide fundamental flaws. We know that the NSA has the ability to capture and eavesdrop on encrypted communications. We know that they have acquired, either by breaking in or by carefully placed operatives, encryption keys from major service providers. We know that the telcos are deeply in bed with the NSA's data collection efforts. At this point, the number of FISA-related information requests are just a cover story, a sideshow compared to the large scale eavesdropping taking place in the network.

                                                                                                                                                                                                                                     So, enough of this transparency charade. Facebook, Google and Yahoo need to abandon this dumb attempt to appease and mislead the masses, and actually work on making our infrastructure safe against NSA-style eavesdropping.

                                                                                                                                                                                                                                     - See more at: http://hackingdistributed.com/2013/09/24/transparency-isnt-what-its-cracked-up-to-be/#sthash.bEV98ngK.dpuf


                                                                                                                                                                                                                                     Altruism Among Capitalists?
                                                                                                                                                                                                                                     Tuesday November 05, 2013 at 03:10 PMIttay Eyal, and Emin Gn Sirer
                                                                                                                                                                                                                                     Gordon Gekko.
                                                                                                                                                                                                                                     He'll totally leave Bitcoin alone, because he's passionate about the long-term future of cryptocurrencies. Right.

                                                                                                                                                                                                                                     The reaction to the selfish mining attack we revealed yesterday has been phenomenal. We've been in contact with the maintainers of the reference Bitcoin implementation; they seem to understand the attack and its implications. But then, for every few such experts, there are dozens of self-styled "experts" who seem to be making all kinds of claims.

                                                                                                                                                                                                                                     Everyone Is In It For The Long Term?
                                                                                                                                                                                                                                     Vitalik Buterin, technical editor of Bitcoin Magazine, was quoted as saying this:

                                                                                                                                                                                                                                      "No honest (or semi-honest) miner would want to join a selfish pool," he suggested. "Even if they do have a small incentive to [join], they have an even greater incentive to not break the Bitcoin network to preserve the value of their own Bitcoins and mining hardware."
                                                                                                                                                                                                                                      This is patently false. Take your regular, run-of-the-mill counterfeiter. Every time he manufactures a banknote, he undermines the very currency he is illegally replicating. Yet that doesn't stop counterfeiters from creating bogus banknotes. Buterin's logic would have us believe that no one would ever be motivated to counterfeit, yet we have people printing fake money, even at the risk of incredibly long prison sentences. And if the cops totally stopped enforcing the law, kind of the way Bitcoin provides no pushback against selfish miners, how many people would turn their color printers into home-made mints? No one can know the exact number, but it's guaranteed to be higher than 0, because it already is, even under heavy law enforcement.

                                                                                                                                                                                                                                      There are lots of people out there who spent a lot of money on their mining rigs. They can very well engage in selfish-mining in order to make money in the short term, at the risk of long-term stability of the currency.

                                                                                                                                                                                                                                      No One Will Misbehave?
                                                                                                                                                                                                                                       Buterin said the attack was "highly theoretical" because no software currently existed that could turn an honest mining group into a selfish one.
                                                                                                                                                                                                                                       How does he know? Does he have access to everyone's code repositories?

                                                                                                                                                                                                                                       Overall, the Bitcoin community is not served well by these transparent attempts to pretend that all is well. All is not well. The sooner the miners acknowledge this and take steps to mitigate the risks, the stronger the community will become.

                                                                                                                                                                                                                                       And perhaps it's not quite clear, but our goal is to make the currency better, to characterize it more accurately, and to protect it from future attacks.

                                                                                                                                                                                                                                       The Bitcoin community is actually quite impressive: last spring, the entire decentralized community quickly dealt with a hard fork in the global ledger that records all transactions, and patched the software to reconcile the books kept differently by different versions of the software. This is not a time to bury one's head in the sand and claim that "no one would do that" where "that" is "take steps to make more money." The community deserves a more robust currency.

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/05/altruistic-capitalists/#sthash.g7RRBqFY.dpuf


                                                                                                                                                                                                                                       There seems to be a collection of programmers and Bitcoin enthusiasts out there who avoid math at all costs. This makes sane conversations very difficult.

                                                                                                                                                                                                                                       Regardless, here are some questions we have been asked, and our best attempt to provide some intuitive answers, without delving into math:

                                                                                                                                                                                                                                       Isn't a selfish miner taking a risk? What if the honest miners discover a block and nullify his work?

                                                                                                                                                                                                                                       Absolutely. There are no rewards to be had without risk. By giving up on a sure 1-unit win right now, the attacker positions himself to take advantage of a large number of outcomes where he makes more revenue later.

                                                                                                                                                                                                                                       But the honest miners command a majority! The attacker would always be behind.

                                                                                                                                                                                                                                       No, that's not right. If it helps pedagogically, think about a 49% attacker. He is essentially neck-and-neck with the honest crowd, and, at times, can develop a substantial lead. If he manages to get ahead by two or more blocks, he can reveal those two blocks at any time he feels threatened by the public closing in on his lead, and consolidate his winnings. When he is behind, he just follows as usual, no worse off. The same situation is true for a 48% attacker, with a lower likelihood of establishing that lead. And so on, all the way down, for even a 1% attacker.

                                                                                                                                                                                                                                       At this point, we really have to break out the math, and start computing probabilities. Economists have a saying called "model please?" If someone does not have a mathematical model, they're wasting everyone's time and just add to the noise component of the discourse. Our math is in the paper, it stands analytical scrutiny as well as numerical simulation. And it shows that the attacker will earn revenues in excess of his resource contributions.

                                                                                                                                                                                                                                       We realize that our findings are counterintuitive. That's why any reasonable counterargument has to invoke mathematics. If there is no math, if it fits in a tweet, it is just noise, no matter how much it might appeal to one's gut feelings.

                                                                                                                                                                                                                                       I thought about a 49% attacker, and I see your point. But only for an attacker near 50%. There is no way that the attack would work at lower thresholds.

                                                                                                                                                                                                                                       You didn't think about it hard enough.

                                                                                                                                                                                                                                       Does an attacker have to establish a comfortable 2-or-more block lead to profit?

                                                                                                                                                                                                                                       No. Even if an attacker is ahead by just a single block, he can gain excess revenues, depending on what happens after he reveals his block. With some (small) likelihood, he will be able to mine on his own block for a 2-unit win. With some likelihood, he will be able to influence an honest node, who then extends his revealed block, for a 1-unit win for both. And with some other likelihood, the honest nodes will build on their own block, and the attacker will gain nothing. This is all explained in the Revenue discussion of our paper.

                                                                                                                                                                                                                                       Once again, we have to break out the math right at this point, go through these likelihoods and compute when they pay off. We did this, and the results are in our paper.

                                                                                                                                                                                                                                       This attack requires an attacker have good network position to get its blocks accepted.

                                                                                                                                                                                                                                       This attack relies on a Sybil attack for its success.

                                                                                                                                                                                                                                       Not at all. The most pessimal assumption is to imagine that the attacker's proposed blocks are never adopted by any honest node when there are alternatives. Every single time he proposes a block, the honest nodes drop the block he proposed. This corresponds to a gamma value of 0 in our analysis, a scenario we fully modeled.

                                                                                                                                                                                                                                       Even under this pessimal assumption, selfish mining pays dividends when the attacker commands more than 33% of the network. The wins have everything to do with controlling the timing of information release and getting the honest nodes to work on blocks that are ultimately discarded. Our attack does not rely on network position or well-connectedness. It does not require Sybils. It does not require a fast connection to other miners. Anyone who claims otherwise does not understand the attack.

                                                                                                                                                                                                                                       Are you trying to take down Bitcoin?

                                                                                                                                                                                                                                       No. We're Bitcoin supporters and are working to make the currency stronger against a broader set of possible misbehaviors than what has been considered so far.

                                                                                                                                                                                                                                       Why didn't you launch this attack?

                                                                                                                                                                                                                                       Because we're nice people. We want Bitcoin to succeed. And we have day jobs that pay well enough (also, see answer to last question).

                                                                                                                                                                                                                                       I actually didn't read the paper, and will never read anything that is longer than a tweet, but I think your attack can't possibly work.

                                                                                                                                                                                                                                       Sigh.

                                                                                                                                                                                                                                       You are part of an "academic conspiracy" and/or a "beaurocratic establishment" aiming to take down Bitcoin and prop up a worthless fiat currency.

                                                                                                                                                                                                                                       We have no words. All we have are our secret Illuminati handshakes. And each night, we sing this song:

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/05/faq-selfish-mining/#sthash.c898HF2U.dpuf


                                                                                                                                                                                                                                       This is a family-friendly blog. While I reserve the right to delete any comment at any time for any reason, I do not normally exercise this right. But I will delete every single comment that contains a profanity.

                                                                                                                                                                                                                                       Today, I deleted two comments that had profanities in them.

                                                                                                                                                                                                                                       And there is already a nutcase who is claiming that "the researchers just did a massive delete of critical comments from their blog posts." He goes on to make other disparaging remarks, all for deleting two comments, both of which had non-substantive verbiage, laced with profanities.

                                                                                                                                                                                                                                       And there were some other interesting comments from the Bitcoin fringe. One commenter on Twitter asked us how much money we received from the NSA. Another accused us of being a member of an academic conspiracy to bring down Bitcoin and prop up fiat currency. There are people who claim, in the same breath, that they don't believe that the selfish mining attack works at all (though they cannot find fault with the reasoning or the math), and that we were irresponsible for publishing it (hey, if it doesn't work, why are you so upset?). One group with whom we had never corresponded before demanded to be listed as coauthors on our paper, and when we told them that this was not possible, let's just say that the conversation turned sour. I used to think that money brought out the worst in people, until I saw Bitcoin.

                                                                                                                                                                                                                                       I'm not sure what my policy is going to be on exceedingly stupid comments. I made a large number of life choices and sacrifices solely to be surrounded by smart people, and consequently, I don't see a compelling reason why stupidity should be welcome on my blog. Yet, to date, I have not deleted any of it.

                                                                                                                                                                                                                                       But this blog does have readers in classrooms, and I will not tolerate profanities. Please behave accordingly.

                                                                                                                                                                                                                                       Addendum, Nov 10, 2013
                                                                                                                                                                                                                                       I now suspect that the person loudly complaining about "massive" comment deletion is actually confused because Discus only shows the top-rated comments. The down-voted, mostly inane comments are not deleted. They just require an extra click or two to unveil. And in any case, this is all done automatically by Discus, a third-party service.

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/06/comment-policy/#sthash.4PE1qfR0.dpuf


                                                                                                                                                                                                                                       Our colleague Ed Felten proposes a hybrid strategy called fairweather mining, where the core idea is that a miner will maintain a presence on both the selfish mining team and the honest team. When the selfish mining team has no lead, the miner switches to the honest team to reap rewards on that side. When the selfish mining team is ahead, he switches back to selfish mining. Felten conjectures that fairweather mining would lead to an unstable situation, where all the selfish miners dissipate as soon as they are behind, and uses this to argue that selfish mining pools would be very short lived.

                                                                                                                                                                                                                                       The Flaw in Felten's Analysis
                                                                                                                                                                                                                                       But Felten's analysis does not account for the proofs of work that miners have to present to collect their amortized payoffs. He seems to be assuming that the miners are compensated at the time they discover a block, and that those earnings are blindly distributed to the members of the pool that were present at the time the block is discovered. This is not how pools operate.

                                                                                                                                                                                                                                       Earnings need to be distributed to pool participants in proportion to the effort they contributed, such that someone with a big FPGA rig that spent a lot of time in the pool will make more money than someone with a puny CPU who spent a tiny amount of time. The pool manager cannot tell who has what kind of equipment at home or wherever else they get their cheap electricity and cooling. And she can't very well ask the participants to self-declare because they would all lie and claim to have huge mining rigs. Instead, pool operators ask participants to furnish proofs of work, that is, solutions to simple cryptopuzzles that they discover by accident on their way to solving the hard cryptopuzzle, kind of like the easy sudokus one might solve on a lazy Sunday while trying to work on a hard one. The number of such puzzles one solves indicates how much time they spent on the sudoku, even if they never get to solve the hard one.

                                                                                                                                                                                                                                       Once the proofs of work are taken into account, we believe that the fairweather strategy falls apart.

                                                                                                                                                                                                                                       The Insight Behind Fairweather Mining
                                                                                                                                                                                                                                       Strip mine.
                                                                                                                                                                                                                                       The Soviets: for who else would fish with dynamite and mine with nuclear bombs?

                                                                                                                                                                                                                                       To be sure, Felten brings up a critical point. While we have shown that selfish mining dominates is better than honest mining when everybody else follows the honest Bitcoin protocol, we have not shown that it is the dominant strategy for the Bitcoin game. There may be other strategies that actually outperform ours, but fairweather mining is not one of them, at least, not based on the reasoning Felten has provided so far.

                                                                                                                                                                                                                                       On a completely unrelated note, all this talk about mining and dominant strategies bring to mind Program #7 . Back in the day, the Soviets developed techniques for, shall we say, "efficient" mining: namely, they used atom bombs to open up enormous strip mines. That, if anything, is a dominant mining strategy.

                                                                                                                                                                                                                                       Edit, November 15, 2013
                                                                                                                                                                                                                                       We are grateful to Ed Felten for pointing out the need for a clarification involving the use of the word dominates.

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/08/fairweather-mining/#sthash.3d9SbsUi.dpuf


                                                                                                                                                                                                                                       Before you read the actual blog entry, I want you to imagine the future and think about an idea I have for human transport. Imagine how awesome it would be if humans didn't have to walk everywhere using their own feet, like peasants, but could be transported efficiently by some other means. Now, it's hard to outdo bipedal locomotion. Moving walkways, like at the airport, are fine, but they are a jarring experience, especially for older people. And if you want to go anywhere fast, you need lots of them side by side, operating at progressively faster speeds. But what if there was a liquid substance that flowed like water horizontally, but was incompressible vertically? I call this Flo. You could stand on it without sinking, and it'd flow like water. I imagine that it'd be a deep orange in color, or perhaps purple. We would replace urban sidewalks with rivers of Flo. Like a meandering brook, the edges of Flo would be moving slow, so even old people could step onto them easily, and watch the storefronts at slow speeds. And anyone who wants to travel fast would smoothly move towards the middle where the flow is fastest, accelerating gradually.

                                                                                                                                                                                                                                       Got the image? Flo is a kind of a cool invention, right? Let's keep it in mind, it'll be useful later.

                                                                                                                                                                                                                                       By the way, I imagine that Flo would be made of some material that organizes itself into sheets at the atomic level, kind of like graphite. And using my amazing background in atomic physics (for the record, I have none), I hereby declare that Flo is not going to be possible. Sorry to rain down on the parade there. Now, let's get back to Bitcoin.

                                                                                                                                                                                                                                       Noise and Novelty
                                                                                                                                                                                                                                       So, some people are claiming that our work is "nothing new" and claim that the idea of selfish mining had been discovered by the Bitcoin community long before we came along and outlined in a forum discussion in December 2010. Oddly, these are typically the very same people who claim that this idea is "patently wrong," but are unable to furnish proof or counterargument. And they are also the same people who will one day have to concede the point to us, as math has a way of cutting through sophistry, but they sure seem intent on making a lot of noise until that point.

                                                                                                                                                                                                                                       I want to point out how flawed this argument is.

                                                                                                                                                                                                                                       Not the Same Attack
                                                                                                                                                                                                                                       First of all, that forum post does not describe our attack.

                                                                                                                                                                                                                                       While that discussion seems to have started out with a similar insight as our attack, it did not explore the full attack, did not perform an analysis, and did not actually discover the implications. My reading of the forum discussion is that they discussed an attack that is a subset of ours and decided that their idea wouldn't work.

                                                                                                                                                                                                                                       In contrast, our work does a few things not covered in that discussion:

                                                                                                                                                                                                                                       it shows a more extensive attack than the one described there, one that works,
                                                                                                                                                                                                                                       it performs a full analysis of the revenue to be obtained from that attack, and characterizes that revenue as a function of attacking pool size and attacking pool's ability to control information flow in the network,
                                                                                                                                                                                                                                       it shows that Bitcoin is not incentive-compatible,
                                                                                                                                                                                                                                       it shows that, even under the best of circumstances (i.e. the attacker has terrible network connectivity, no Sybils, no control over information propagation and loses to the honest miners every single time), defending against the attacker requires at least 2/3rds of the network to be honest.
                                                                                                                                                                                                                                       Business Week is somehow parroting the line that this attack had been discussed before by the Bitcoin community and that our results were known. This is just sloppy, one-sided reporting.

                                                                                                                                                                                                                                       Here's a simple demonstration of how badly the Bitcoin community dropped the ball on this one: conventional wisdom was that Bitcoin was safe from attack as long as the majority of miners were honest. It was us who showed that this is false.

                                                                                                                                                                                                                                       That should place our work in perspective. The claim that the Bitcoin community "already knew about this attack" shows signs of a group desperately trying to manage the PR around a gold-laying hen, laying down a PR smokescreen, pretending that the hen's doing just fine even though there seem to be some real issues with the egg-laying process. We don't think the hen is destined to die, in fact, we hope not, but the fact that it is sick is a scientifically observable fact, an undeniable piece of objective reality backed by math. PR and sophistry might sow confusion through the news cycle, but it cannot cure the hen.

                                                                                                                                                                                                                                       What Does it Take To Develop a Thought
                                                                                                                                                                                                                                       This brings us to the whole issue of "what does it take to develop an idea?" This is part and parcel of everyone's curriculum in graduate school, no matter what topic they may be studying; namely, what's the difference between a brainfart, an insight, an idea, a result and a contribution?

                                                                                                                                                                                                                                       Half-baked ideas.
                                                                                                                                                                                                                                       She is full of creative ideas. But the execution leaves much to be desired.

                                                                                                                                                                                                                                       Those of you who have been in the presence of a truly creative person know just how awe-inspiring it can be to see the seeds of ideas spout from an energetic, creative source. It's truly a sight to see someone generate a stream of interesting proto-ideas. But half-baked ideas, even in the rare cases where they contain a genuine insight, require much more additional work to become actionable. Creative graduate students drop out of graduate school all the time because they lack the wherewithal or the tools to develop their half-baked, muddled thoughts into distilled, developed contributions. In our field of distributed systems, this development process typically requires mathematical analysis, sound reasoning, code development and sometimes system deployment. It takes, shall we say, substantial additional effort.

                                                                                                                                                                                                                                       Take my Flo example above. I outlined a sort-of-an-okay-idea for urban transportation. But I didn't do jack to actually develop that idea. I'm not some kind of an urban transportation genius. To make the analogy to the Bitcoin fringe complete, I even falsely dismissed the Flo idea, using no science and no math! Imagine what would happen if some day, someone goes through the laborious process of developing a workable Flo molecule, and we do change our cities to replace sidewalks with a richly colored liquid that carries people. If I were to get up to claim that their work was not novel and I was the original inventor of Flo, I'd look silly. In fact, the creative guy who originally wrote the Bitcoin talk forum post did NOT claim anything remotely similar -- it's the PR flaks who are making this claim. And when a reporter lends credence to these claims, they're just being sloppy.

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/09/no-you-dint/#sthash.wUb6DrTS.dpuf


                                                                                                                                                                                                                                       Anyone who has had to explain how Bitcoin works to non-techie people will agree that this seemingly straightforward distributed system is actually quite complex and nuanced. Here's the best description we've seen so far. And if you think the parody is ridiculous, then check out the non-parody below it.


                                                                                                                                                                                                                                       "Bitcoin derives its monetary value from a series of encrypted Candy Crush"

                                                                                                                                                                                                                                       "What is a Bitcoin? A Bitcoin isn't really anything"
                                                                                                                                                                                                                                       Edit
                                                                                                                                                                                                                                       Here is a truly clear and crisp description of how Bitcoin operates

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/10/clearest-description-of-bitcoin/#sthash.37fVmXUd.dpuf


                                                                                                                                                                                                                                       Anyone who has had to explain how Bitcoin works to non-techie people will agree that this seemingly straightforward distributed system is actually quite complex and nuanced. Here's the best description we've seen so far. And if you think the parody is ridiculous, then check out the non-parody below it.


                                                                                                                                                                                                                                       "Bitcoin derives its monetary value from a series of encrypted Candy Crush"

                                                                                                                                                                                                                                       "What is a Bitcoin? A Bitcoin isn't really anything"
                                                                                                                                                                                                                                       Edit
                                                                                                                                                                                                                                       Here is a truly clear and crisp description of how Bitcoin operates

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/10/clearest-description-of-bitcoin/#sthash.37fVmXUd.dpuf


                                                                                                                                                                                                                                       Selfish-Mine
                                                                                                                                                                                                                                       Simulated Bitcoin network.
                                                                                                                                                                                                                                       There is now a publicly available visual simulator for the selfish mining attack we outlined. To see the attack in action, set the "Node 0 hashing power each" to something like 40%, then click "Go" and then click "faster" multiple times like you have adult ADD, then check the percentage reported on the right. If it reports anything above the percentage that you selected, the attack is working. We get around 60% of the revenue when we let it run for a little while, which shows that the attack is real and successful.

                                                                                                                                                                                                                                       The code was developed by ebfull, and funded by the Bitcoin community. I can't vouch for the overall correctness of the implementation -- javascript is beyond me -- but the results for high alpha seem in line with our simulations. Note that gamma is not controllable in this simulation, but is, I think, an emergent property of the underlying topology, propagation speeds and propagation tactics baked into the simulation (or maybe it's 0; I cannot tell). So, pick a high enough alpha for whatever gamma might be.

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/17/selfish-mining-simulator/#sthash.LXnuaSp6.dpuf


                                                                                                                                                                                                                                       The Senate just held hearings on Bitcoin yesterday, and what do you know, it turns out that the US Government has a secret crush on everyone's favorite anti-establishment, stick-it-to-the-man, we're-so-cool-we-reject-your-dirty-fiat-money cryptocurrency. Partly in response to this newfound love, Bitcoin prices hit an all time high at $900, up from $12 a year ago, and $240 just two weeks ago.

                                                                                                                                                                                                                                       I, too, would like to join the nearest Bitcoin-hits-900 party right about now, and in fact, do plan to go out with a bunch of CS geeks this evening, some of whom undoubtedly made real cash yesterday. But before we do exactly what is expected of us and fulfill the role The Man has cut out for our demographic (and if you're saying "hey, wait a minute, I'm nobody's demographic, I'm a free person with his/her independent opinion on Bitcoin, with some libertarian leanings and a penchant for TED talks", well, I don't need to finish the sentence), let's look a little bit into why The Man has such an unrequited love affair with our beloved cryptocurrency.

                                                                                                                                                                                                                                       For if we don't sort this out, we run the risk of reading about it a decade from now, when another Snowden elopes to Russia and reveals that the powers that be ran a series of clandestine projects, probably code-named "Operation Libertarian Neckbeard" for the one on Bitcoin, and "Operation I-Know-What-You-Did-Every-Summer" on Tor. Oh wait, Snowden the First already revealed the project on Tor, though the name NSA picked for it was a lot more prosaic.

                                                                                                                                                                                                                                       Untraceability versus Anonymity
                                                                                                                                                                                                                                       Tracing paper
                                                                                                                                                                                                                                       Senate Commission: "Is Al Gore Satoshi Nakamoto?" "Haha, that's funny because Satoshi is probably a three letter agency that Al Gore funded."

                                                                                                                                                                                                                                       The Man loves Bitcoin because it's a digital accounting system that must necessarily keep track of all transactions. Unlike currency we're used to, there aren't any coin/greenback counterparts exchanging virtual hands, ever, anywhere in Bitcoin. Instead, there is an accounting book that keeps track of all transactions between people going back to the beginning of Bitcoin time. I have previously described how Bitcoin works and fails to work (and there's always this video), so I won't belabor the point, except to say that an angel dies every time some press article refers to Bitcoin as "untraceable". It is anything but. The whole currency is about traceability; in fact, transactions are individually traced independently by every node in the system to ensure that nobody is spending more than they earned.

                                                                                                                                                                                                                                       What the press really means to say is that Bitcoin users can create new addresses at will, thereby allowing someone to create a bank-account-equivalent without having to show ID. This makes it possible for a user to be anonymous, as long as she prescribes to a set of practices that would protect her identity, even though her transactions are fully traceable.

                                                                                                                                                                                                                                       The Feds love this system, because once you know the accounts belonging to Escobar-Jr, you can trace exactly from which other accounts he received his cash flows. Sure, pinning these accounts back to real people is not trivial because of the potential anonymity, but here is where the Feds have some special techniques. For one, they know that in real life, most people will go through exchanges that require some form of ID. The more commercialized and consolidated the Bitcoin space is, the easier it is for them to get at this data first-hand. And if some users avoid the exchanges and roll their own, the Feds know that real people have difficulty keeping up with all the requisite anonymity practices. Even the Dread Pirate Roberts who ran the Silk Road drug marketplace was easily traced back, and while the cover story says that he was caught because he used the same nickname on different forums, he could also have been caught by deanonymizing his address when he transferred Bitcoins. For Bitcoin users reveal their IP addresses when they submit transactions, and all our ISPs have our real names on file, and we know from Snowden that the ISPs are fully compromised.

                                                                                                                                                                                                                                       What to Do
                                                                                                                                                                                                                                       Tracing paper
                                                                                                                                                                                                                                       Shhhh, don't say a word. The suits think they are ahead of the techie crowd.

                                                                                                                                                                                                                                       We should not get too exuberant about the positive Senate testimony. It should give us pause that the people at the hearing yesterday were the same people who, only 20 years ago, were trying to ban the export of t-shirts with crypto algorithms on them. It's difficult to believe that they suddenly saw the promise of crypto-currencies. The same way your parents didn't suddenly, or ever, go from being total buzzkills to being totally cool with sleepovers and teenage sex, the Feds cannot suddenly turn all cool and progressive. They are just as short-sighted as they always were, except this time, they think they can take advantage of Bitcoin's traceability for their own purposes in the short term.

                                                                                                                                                                                                                                       So, the Feds' newfound love for cryptography and cryptocurrencies should not be surprising.

                                                                                                                                                                                                                                       But nor should it cause alarm.

                                                                                                                                                                                                                                       Bitcoin still has lots of exciting uses, and the Feds' love affair with Bitcoin will pave the path for other cryptocurrencies, including modified/improved versions of Bitcoin. These currencies may well end up offering stronger guarantees than the current Bitcoin system. For instance, ZeroCoin offers improved anonymity, even though I cannot tell how one makes change in ZeroCoin despite having read their paper carefully. But I remain convinced that this is a minor, solvable problem, and I am sure there will be other, yet-to-be-invented currencies with even better features in the future. Little does The Man realize that once we all commit to a new way of doing business, it'll be difficult to unwind his policies when systems he cannot manipulate or navigate come along.

                                                                                                                                                                                                                                       So, I'll join my prescribed demographic in celebration tonight, partly to genuinely celebrate and partly so as not to alert the Feds that we're onto them being onto us, and we're one step ahead.

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/19/why-da-man-loves-bitcoin/#sthash.IMBl7pKM.dpuf


                                                                                                                                                                                                                                       We are proud to announce HyperDex 1.0.rc5, the next generation NoSQL data store that provides ACID transactions, fault-tolerance, and high-performance. This new release has a number of exciting features:

                                                                                                                                                                                                                                       Improved cluster management. The cluster will automatically grow as new nodes are added.
                                                                                                                                                                                                                                       Backup support. Take backups of the coordinator and daemons in a consistent state and be able to restore the cluster to the point when the backup was taken.
                                                                                                                                                                                                                                       An admin library which exposes performance counters for tracking cluster-wide statistics relating to HyperDex
                                                                                                                                                                                                                                       Support for HyperLevelDB. This is the first HyperDex release to use HyperLevelDB, which brings higher performance than Google's LevelDB.
                                                                                                                                                                                                                                       Secondary indices. Secondary indices improve the speed of search without the overhead of creating a subspace for the indexed attributes.
                                                                                                                                                                                                                                       New atomic operations. Most key-based operations now have conditional atomic equivalents.
                                                                                                                                                                                                                                       Improved coordinator stability. This release introduces an improved coordinator that fixes a few stability problems reported by users.
                                                                                                                                                                                                                                       Binary packages for Debian 7, Ubuntu 12.04-13.10, Fedora 18-19, and CentOS 6 are available on the HyperDex Download page, as well as source tarballs for other Linux platforms.

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/20/hyperdex-1.0rc5/#sthash.0ZVOjvEZ.dpuf

                                                                                                                                                                                                                                       In my previous post, I explained why the Feds had such a sudden crush on Bitcoin, due to its traceability. In this post, I want to tell you how to coopt Bitcoin's traceability to bring about societal change. There is an exciting new idea, called a cash-boycott, that was relayed to me by two friends. I have not heard it discussed elsewhere, so I want to tell you about this idea and what makes it such a potentially powerful force.

                                                                                                                                                                                                                                       I'll tell the story from my own perspective. Feel free to duck out if you can't relate -- social change isn't for everyone -- but if you can stick through, this discussion will bring up how Bitcoin-style cryptocurrencies provide a new tool with which ordinary citizens can put pressure on corporations to behave better.

                                                                                                                                                                                                                                       The Context
                                                                                                                                                                                                                                       Boycott.
                                                                                                                                                                                                                                       What do we want? Blah. When do we want it? It doesn't matter, boycotts hardly ever work.

                                                                                                                                                                                                                                       Back in 2003, I was having dinner with some friends in a fantastic dive restaurant where there is no menu; you just sit down, order a bottle and start discussing serious stuff, like what's wrong with the world, while the chef just sends you whatever foods he thinks you'd like at that moment. At the end of the night, you get a check with just a single number that corresponds to how much fun the chef thinks you had, and you most definitely don't even think about asking for an itemized bill. It's a great place: there is no server named Joe or Jill who reads off a long list of completely banal specials of the day that always include a balsamic reduction and a bed of kale, the lack of control over the food keeps out all your attention-seeking friends with annoying food restrictions, and the billing practices guarantee that there will be no tourists even though it's in a trendy location.

                                                                                                                                                                                                                                       That night, we were getting somewhat worked up about how broken the justice systems tend to be everywhere. As you'll recall, those were grim years. Corporations pretty much had free rein and were running the show. Just like now, except back then, we were enraged about it. Microsoft had received a slap on the wrist for its decades long practice of playing hardball with every competitor. There were guys on late night TV telling you how you should be flipping houses. The finance sector was busy creating arcane instruments that were about to implode, impacting everything, including the coffee machine in the Cornell CS department that would unceremoniously have to be hauled away someday, which would leave everyone puzzled about how the coffee machine could have gotten itself tangled up in credit default swaps, or why it should take the fall, given that no one on Wall Street was going to go to jail.

                                                                                                                                                                                                                                       And Enron's antics had actually caused rolling blackouts throughout California. bin Ladin, who was very much alive at the time, must have been incredibly jealous. The Enron boys literally terrorized a whole state for a summer. Who would have known that if your goons shouted "Greed is Good" instead of "Allahu Aqbar," they'd instantly get a get-out-of-jail card? Ok, perhaps they wouldn't all get off scot-free; perhaps someone's kid would have to swap their 49-footer motor-yacht with a slightly more fuel-efficient 39-footer, but you knew they'd still drive it drunk and mow down swimmers.

                                                                                                                                                                                                                                       In any case, it was evident then that the justice system was too feeble when it came to actors who were too big. That's when my friends Giray Pultar (founder of Parkyeri) and Selcuk Pultar suggested a very cool idea: why not outsource the enforcement of societal verdicts out of the justice system into the hands of ordinary people, using the currency?

                                                                                                                                                                                                                                       Product Boycotts Don't Work
                                                                                                                                                                                                                                       Boycott.
                                                                                                                                                                                                                                       End Galactic Corporate Greed.

                                                                                                                                                                                                                                       Now, everyone has heard of a regular boycott -- vote with your pocketbook, hit them where it hurts -- and we know that consumer boycotts just don't work. Koch brothers are behind all but a few of the paper products at the supermarket, even though they seem to be manufactured by different labels. There is no way I can remember all the brand names for all the TP I am supposed to boycott. All those expensive eyeglasses are manufactured by the same cartel regardless of their brand; boycott them and you're down to hunting for monocles at the local thrift shop. Even when there are multiple players, competition typically has forced them to adopt the same business practices, in a dive to the bottom. "Do you prefer Pepsi or Coke, Nike or Adidas?" The battle is already lost.

                                                                                                                                                                                                                                       And boycotts can't work unless a significant fraction of the consumers are involved. Let's face it, demand is mostly inflexible -- when your kid is screaming for that Tickle-Me-Elmo, you'll buy it, even if it means that someone else's kid in Bengladesh is slightly more likely to die in a warehouse fire as a result. Even when BP opened a gaping hole in the earth's mantle that was gushing a thick brown liquid of death into the ocean, approximately nobody changed their driving habits. Consumers mostly do not care about social causes, as evidenced by the sad number of signatures on change.org for even the worthiest petitions. And consumption is a terminal activity that starts and ends at the point of sale. There is no leverage; the advertising engines are too good, too strong, and the consumer sheep are already going baaa louder than you can say 'hey let's do things differently.'

                                                                                                                                                                                                                                       What we need is a network effect, one by which a small group of cognoscenti can leverage their power and exert a force on an even larger populace to do the right thing.

                                                                                                                                                                                                                                       A Cash-Boycott
                                                                                                                                                                                                                                       Bar scene from The Shining.
                                                                                                                                                                                                                                       "Your money's no good here." Not because someone else footed the bill, but because you're a terrible human being.

                                                                                                                                                                                                                                       So, the Pultars had a crazy idea: why not, instead of boycotting a company's products, boycott their cash flow? For this, all we need is a currency that records where the money has been. The idea is that, if someone hands you a dollar bill that he got from the Koch brothers, you could say, with righteousness dripping from your voice, "your money's no good here." That will certainly make people think twice about working for companies that their first-hop social network does not approve. Sure, you could work for Bechtel or the Carlisle group or Halliburton, and you could even get rich doing so, but you'd have to do all your shopping at Walmart. The quaint cafes would be inaccessible to you; you'd have to run a lemonade stand on the weekend if you wanted to get anything from that place with that cute and flirty barista with the tongue piercing. Churches, which offer salvation to everyone and accept all donations, even the banknotes with white powdery residue, might have to be more discerning, and if they aren't, then you can be more discerning about which churches you want to attend. And perhaps you could be super rich off of blood diamonds, but your kids would not be able to go to a progressive school.

                                                                                                                                                                                                                                       And this approach allows one to go beyond the first-hop neighborhood in the social graph, put leverage on third-parties and enlist them to your cause. Imagine what you'd do if you knew that the super-trendy restaurant isn't just organic, but it's also refusing to accept cash that has been through a factory farm. That'd certainly make you think twice about touching tainted cash yourself, and suddenly, you're a recruit to their cause.

                                                                                                                                                                                                                                       This, in a very real sense, is the opposite of a regular boycott, one directed not at products but at cash. Whereas regular boycotts don't work because they're negative, product oriented, devoid of network effects, and, most importantly, go counter to people's overwhelming urge to buy things, a cash-boycott harnesses people's desire to buy things into a desire to act for the social good.

                                                                                                                                                                                                                                       Social good, of course, is defined separately for each individual, by the behavior of people from whom they procure goods and services. If these people are jerks, you might find yourself having to, say, avoid Planned Parenthood. All good knives cut both ways, and tools that can bring about societal change can be used by awful communities just as well. History seems to show, however, that all cutting edge tools are used for progressive causes for decades before they are coopted, and by that time there are even better tools available to progressives.

                                                                                                                                                                                                                                       So, perhaps with the naivete of being 10 years younger and having consumed a substantial portion of a bottle of hard liquor, we were convinced that this was a stellar idea. It could put a bunch of geeks in a position to enact societal change.

                                                                                                                                                                                                                                       The only problem was that there was no electronic currency back then that could keep track of such a history of transactions.

                                                                                                                                                                                                                                       Along Comes Bitcoin
                                                                                                                                                                                                                                       Pile of Bitcoin.
                                                                                                                                                                                                                                       Every Bitcoin records everywhere it has been, every single person who has touched it.

                                                                                                                                                                                                                                       Now that Bitcoin is here, this idea is completely feasible. Every Bitcoin already records every single transaction it has been in, every single wallet it has passed through. All it takes to go from Bitcoin as it is today to a cash-boycott is a registry that maps Bitcoin wallets to their owners in the real world.

                                                                                                                                                                                                                                       Such a list is not hard to compile: many addresses are already public. For instance, the Bitcoin donation address for virtual-notary.org is right on the web page itself. If you're one of the people who hates me for whatever reason (and there are some. Ever since we showed that the Bitcoin protocol is broken, there have been some unhinged commenters intent on killing the messenger, who somehow have not caught onto the fact that we also offered the best known fix and have been working to make the currency more robust), you could put that address on your personal blacklist. Similarly, every corporation that sells something will typically have a public address that can be harvested. From there, it's mostly a game of keeping a blacklist containing addresses and associated timestamps (encoded as positions in the blockchain), such that all coins that pass through that address after that position in the blockchain are considered null and void. You'd ask the vendor to issue you a new one, just as you would if he were to hand you a tattered, damaged US banknote. As we see more Bitcoin adoption, I can imagine companies emerging that provide Bitcoin reputation scores for different Bitcoin addresses, though I would much rather have this collection and classification be performed organically, by users.

                                                                                                                                                                                                                                       And thinking about which companies ought to be on one's blacklist is a fun exercise. Yes, AT&T, I'm looking at you. I have not forgotten your practice of jacking my long-distance calling plan every six months. "Oh I'm sorry sir, the Friends-And-Family plan has had a tenfold rate hike; our new competitive plan is called Family-And-Friends." Nor have I forgotten how you kept charging me exorbitant fees for a phone line that did not belong to me, and kept sending me to a phone-menu hell that would inevitably hang up on me because I was not calling from the phone line that did not belong to me. You knew it would do that, but you forced me to explore every twisted turn in that phone menu anyway. I guess you could play these games and shake down people who are otherwise too busy to deal with your antics, as well as the infirm and the elderly. I guess they teach in MBA school that, when a manager cannot raise revenue through productivity increases, it's completely ok to tear at the social contract that holds us together. Well, with a cash-boycott, these people would have difficulty hiring the phone staff they need for their perfectly-legal-but-completely-unethical scams. And let's not get started with the "you need to be home between 9am and 3am every workday for all months that start with a consonant" cable company. The cable already runs into the house and all the equipment is already here; all you have to do is press a button. Anyhow, the list of people I'd line up against the wall is kind of long, but you nice people out there should think about whose money you would not accept because of your principles, and what the consequences would be for the world if you could start a snowball effect.

                                                                                                                                                                                                                                       Tumblers and Laundries
                                                                                                                                                                                                                                       Money laundering.
                                                                                                                                                                                                                                       It's one thing to have money, it's another thing to have clean hands.

                                                                                                                                                                                                                                       It is certainly possible to whitewash cash by putting it through a third-party service. There may be pairwise cash swaps that allow the parties to get their hands on a clean pile of cash: for instance, someone who earns but is unable to spend fracking money in Ithaca could swap it with someone who earns their salary at a hippy commune in Appalachia. This kind of pooling and redistribution would be performed by what is known as a tumbler or mixer, but such behaviors would tend to get noticed, and tumblers would then end up on everyone's blacklists. Besides, on topics where there is broad societal support (e.g. "I see that you got a taxpayer-funded Wall Street bonus while young people were unable to find jobs. Why don't you have a seat over there with your tainted cash while we all re-think those bonuses? And by the way, we're still trying to figure out where our coffee machine went during the crisis."), it'd be dificult to find someone with whom to execute a successful swap.

                                                                                                                                                                                                                                       Of course, Escobar-Jr could give his Bitcoins to, say the winner of a Bitcoin lottery, and swap his tainted dirty money for their clean lottery winnings. That's perfectly legitimate, except he'd pay about 10-15% overhead for this. Don't ask me how I know the premium paid for such laundry operations, but let's just say that this already does happen with regular cash. We don't have a solution for this, but not everything in life gets a solution, and luckily, there are only so many lottery winners who'd want to swap and end up with Escobar-Jr's problem in their hands.

                                                                                                                                                                                                                                       So, mixers, tumblers and regular laundering operations could be used to get around a cash-boycott, but not without cost and not for long without being detected. The opportunity to put pressure on people opens up a novel opportunity for enacting social change.

                                                                                                                                                                                                                                       Summary
                                                                                                                                                                                                                                       We'll leave you with this thought: if you were unhappy with the government shutdown, you could refuse to touch the money that has been paid to the people who shut it down. They can pay themselves all they like, but they would have to take those coins to their graves. It sounds almost like poetic justice.

                                                                                                                                                                                                                                       It's an exciting world ahead with Bitcoin, and we haven't even begun to touch the new kinds of applications enabled by a fully-traceable transaction graph.

                                                                                                                                                                                                                                       - See more at: http://hackingdistributed.com/2013/11/21/bitcoin-cash-boycott/#sthash.T6S0lcRy.dpuf


